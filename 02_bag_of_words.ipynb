{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words \n",
    "\n",
    "\n",
    "Data pre-processsing for text analysis and pairwise word overlap\n",
    "\n",
    "* 1) Load libraries and define functions\n",
    "* 2) Import data: parsed email data and path length data\n",
    "* 3) Process and store each user's vocabulary \n",
    "* 4) Calcualte pairwise % overlaps \n",
    "* 5) Create pairwise lists of  non-overlapping words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading libraries and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "from subprocess import check_output\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.util import ngrams \n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_clean(text):\n",
    "    text = re.sub(r'\\n--.*?\\n', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'enron.com', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'Forwarded by.*?Subject:', '', text, flags=re.DOTALL) \n",
    "    text = re.sub(r'Fwd:.*?Subject:', '', text, flags=re.DOTALL) \n",
    "    text = re.sub(r'Fw:.*?Subject:', '', text, flags=re.DOTALL)     \n",
    "    text = re.sub(r'FW:.*?Subject:', '', text, flags=re.DOTALL)         \n",
    "    text = re.sub(r'Forwarded:.*?Subject:', '', text, flags=re.DOTALL)         \n",
    "    text = re.sub(r'From:.*?Subject:', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'PM', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'AM', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\",\n",
    "                 \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \n",
    "                 \"enron america corp\", \"enron\", \"etc\", \"na\", firstname, lastname))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    porter= PorterStemmer()\n",
    "    \n",
    "    text=text.rstrip()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()))])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    #stem = \" \".join(porter.stem(token) for token in normalized.split())\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing data \n",
    " \n",
    "I will use parsed email data; path length data; user-address key data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_email_data = 'C:/Users/Margeum/Dropbox/DS projects/05. Email data/emails_in_csv'\n",
    "os.chdir(path_to_email_data)\n",
    "\n",
    "emails_df = pd.read_csv('./emails_parsed.csv')\n",
    "pw_path_length_df = pd.read_csv('./pw_path_length_df.csv')\n",
    "address_user_df = pd.read_csv('./address_user_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pairs:18090\n"
     ]
    }
   ],
   "source": [
    "pw_paths_df = pw_path_length_df[pw_path_length_df['s_path_length'] > 0]\n",
    "print (\"Total number of pairs:\" + str(len(pw_paths_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text for each user\n",
    "\n",
    "Clean text and store text frequency as a list of lists \n",
    "This will be used in TF-IDF as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allen-p\n",
      "arnold-j\n",
      "arora-h\n",
      "badeer-r\n",
      "bailey-s\n",
      "bass-e\n",
      "baughman-d\n",
      "beck-s\n",
      "blair-l\n",
      "brawner-s\n",
      "buy-r\n",
      "campbell-l\n",
      "carson-m\n",
      "cash-m\n",
      "causholli-m\n",
      "corman-s\n",
      "cuilla-m\n",
      "dasovich-j\n",
      "davis-d\n",
      "dean-c\n",
      "delainey-d\n",
      "derrick-j\n",
      "dickson-s\n",
      "donoho-l\n",
      "donohoe-t\n",
      "dorland-c\n",
      "ermis-f\n",
      "farmer-d\n",
      "fischer-m\n",
      "forney-j\n",
      "fossum-d\n",
      "gang-l\n",
      "gay-r\n",
      "geaccone-t\n",
      "germany-c\n",
      "giron-d\n",
      "griffith-j\n",
      "grigsby-m\n",
      "guzman-m\n",
      "haedicke-m\n",
      "hain-m\n",
      "harris-s\n",
      "hayslett-r\n",
      "heard-m\n",
      "hendrickson-s\n",
      "hernandez-j\n",
      "hodge-j\n",
      "holst-k\n",
      "horton-s\n",
      "hyatt-k\n",
      "hyvl-d\n",
      "jones-t\n",
      "kaminski-v\n",
      "kean-s\n",
      "keavey-p\n",
      "keiser-k\n",
      "king-j\n",
      "kitchen-l\n",
      "kuykendall-t\n",
      "lavorato-j\n",
      "lay-k\n",
      "lenhart-m\n",
      "lewis-a\n",
      "lokay-m\n",
      "lokey-t\n",
      "love-p\n",
      "lucci-p\n",
      "maggi-m\n",
      "mann-k\n",
      "martin-t\n",
      "may-l\n",
      "mccarty-d\n",
      "mcconnell-m\n",
      "mckay-b\n",
      "mckay-j\n",
      "mclaughlin-e\n",
      "meyers-a\n",
      "motley-m\n",
      "neal-s\n",
      "nemec-g\n",
      "panus-s\n",
      "parks-j\n",
      "pereira-s\n",
      "perlingiere-d\n",
      "pimenov-v\n",
      "platter-p\n",
      "presto-k\n",
      "quenet-j\n",
      "quigley-d\n",
      "rapp-b\n",
      "reitmeyer-j\n",
      "richey-c\n",
      "ring-a\n",
      "ring-r\n",
      "rogers-b\n",
      "ruscitti-k\n",
      "sager-e\n",
      "saibi-e\n",
      "salisbury-h\n",
      "sanchez-m\n",
      "sanders-r\n",
      "scholtes-d\n",
      "schoolcraft-d\n",
      "schwieger-j\n",
      "scott-s\n",
      "semperger-c\n",
      "shackleton-s\n",
      "shankman-j\n",
      "shapiro-r\n",
      "shively-h\n",
      "skilling-j\n",
      "slinger-r\n",
      "smith-m\n",
      "solberg-g\n",
      "staab-t\n",
      "steffes-j\n",
      "stepenovitch-j\n",
      "stokley-c\n",
      "storey-g\n",
      "sturm-f\n",
      "swerzbin-m\n",
      "symes-k\n",
      "taylor-m\n",
      "tholt-j\n",
      "thomas-p\n",
      "townsend-j\n",
      "tycholiz-b\n",
      "ward-k\n",
      "watson-k\n",
      "weldon-c\n",
      "whalley-g\n",
      "white-s\n",
      "whitt-m\n",
      "williams-j\n",
      "wolfe-j\n",
      "zipper-a\n",
      "zufferli-j\n"
     ]
    }
   ],
   "source": [
    "# On a CPU, it takes about 30 minutes\n",
    "\n",
    "tf_list = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for index, row in address_user_df.iterrows():\n",
    "    \n",
    "    text_cleaned_i = []\n",
    "\n",
    "    user_i = row['user']\n",
    "    print user_i\n",
    "    \n",
    "    lastname = row['address'].split('@')[0].split('.')[-1]\n",
    "    firstname = str(row['address'].split('.')[0])\n",
    "\n",
    "    text_to_clean_df_i = emails_df[emails_df[\"user\"] == user_i][[\"content\", \"user\"]].reset_index()\n",
    "        \n",
    "    for text in text_to_clean_df_i['content']:\n",
    "        text_cleaned_i.append(clean(email_clean(text)).split())\n",
    "\n",
    "    unlisted_text_cleaned_i = [item for sublist in text_cleaned_i for item in sublist]\n",
    "    freqdist_user_i = nltk.FreqDist(ngrams(unlisted_text_cleaned_i, 1))\n",
    "\n",
    "    tf_list.append(freqdist_user_i)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allen-p\n",
      "arnold-j\n",
      "arora-h\n",
      "badeer-r\n",
      "bailey-s\n",
      "bass-e\n",
      "baughman-d\n",
      "beck-s\n",
      "blair-l\n",
      "brawner-s\n",
      "buy-r\n",
      "campbell-l\n",
      "carson-m\n",
      "cash-m\n",
      "causholli-m\n",
      "corman-s\n",
      "cuilla-m\n",
      "dasovich-j\n",
      "davis-d\n",
      "dean-c\n",
      "delainey-d\n",
      "derrick-j\n",
      "dickson-s\n",
      "donoho-l\n",
      "donohoe-t\n",
      "dorland-c\n",
      "ermis-f\n",
      "farmer-d\n",
      "fischer-m\n",
      "forney-j\n",
      "fossum-d\n",
      "gang-l\n",
      "gay-r\n",
      "geaccone-t\n",
      "germany-c\n",
      "giron-d\n",
      "griffith-j\n",
      "grigsby-m\n",
      "guzman-m\n",
      "haedicke-m\n",
      "hain-m\n",
      "harris-s\n",
      "hayslett-r\n",
      "heard-m\n",
      "hendrickson-s\n",
      "hernandez-j\n",
      "hodge-j\n",
      "holst-k\n",
      "horton-s\n",
      "hyatt-k\n",
      "hyvl-d\n",
      "jones-t\n",
      "kaminski-v\n",
      "kean-s\n",
      "keavey-p\n",
      "keiser-k\n",
      "king-j\n",
      "kitchen-l\n",
      "kuykendall-t\n",
      "lavorato-j\n",
      "lay-k\n",
      "lenhart-m\n",
      "lewis-a\n",
      "lokay-m\n",
      "lokey-t\n",
      "love-p\n",
      "lucci-p\n",
      "maggi-m\n",
      "mann-k\n",
      "martin-t\n",
      "may-l\n",
      "mccarty-d\n",
      "mcconnell-m\n",
      "mckay-b\n",
      "mckay-j\n",
      "mclaughlin-e\n",
      "meyers-a\n",
      "motley-m\n",
      "neal-s\n",
      "nemec-g\n",
      "panus-s\n",
      "parks-j\n",
      "pereira-s\n",
      "perlingiere-d\n",
      "pimenov-v\n",
      "platter-p\n",
      "presto-k\n",
      "quenet-j\n",
      "quigley-d\n",
      "rapp-b\n",
      "reitmeyer-j\n",
      "richey-c\n",
      "ring-a\n",
      "ring-r\n",
      "rogers-b\n",
      "ruscitti-k\n",
      "sager-e\n",
      "saibi-e\n",
      "salisbury-h\n",
      "sanchez-m\n",
      "sanders-r\n",
      "scholtes-d\n",
      "schoolcraft-d\n",
      "schwieger-j\n",
      "scott-s\n",
      "semperger-c\n",
      "shackleton-s\n",
      "shankman-j\n",
      "shapiro-r\n",
      "shively-h\n",
      "skilling-j\n",
      "slinger-r\n",
      "smith-m\n",
      "solberg-g\n",
      "staab-t\n",
      "steffes-j\n",
      "stepenovitch-j\n",
      "stokley-c\n",
      "storey-g\n",
      "sturm-f\n",
      "swerzbin-m\n",
      "symes-k\n",
      "taylor-m\n",
      "tholt-j\n",
      "thomas-p\n",
      "townsend-j\n",
      "tycholiz-b\n",
      "ward-k\n",
      "watson-k\n",
      "weldon-c\n",
      "whalley-g\n",
      "white-s\n",
      "whitt-m\n",
      "williams-j\n",
      "wolfe-j\n",
      "zipper-a\n",
      "zufferli-j\n"
     ]
    }
   ],
   "source": [
    "bow_list = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for index, row in address_user_df.iterrows():\n",
    " # Convert frequency into indicator (from frequency dict to bag of words)\n",
    "    bow_i = []\n",
    "    user_i = row['user']\n",
    "    print user_i\n",
    "    \n",
    "    freqdist_user_i = tf_list[index]\n",
    "    \n",
    "    for k in range(len(freqdist_user_i)):\n",
    "        bow_i.append(str(list(freqdist_user_i)[k][0]))\n",
    "    bow_list.append(bow_i)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create overlap scores and lists of non-overlapping words (i.e., pair-wise jargon lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pw_paths_df, run 1) overlap; union; intersect  --> a list\n",
    "# For pw_paths_df, run 2) i_minus_j (we have both ways --> so, I only take care about i here)\n",
    "\n",
    "node_i_list = []\n",
    "node_j_list = []\n",
    "pw_pl_list = []\n",
    "user_i_list = []\n",
    "user_j_list = []\n",
    "bow_count_i_list = []\n",
    "bow_count_j_list = []\n",
    "count_overlap_list = []\n",
    "count_union_list = []\n",
    "count_intersect_list = []\n",
    "non_overlap_list = []\n",
    "unused_ij = []\n",
    "\n",
    "for index, row in pw_paths_df.iterrows():\n",
    "    \n",
    "    ## Need to check this step \n",
    "    try: \n",
    "        index_i = address_user_df.index[address_user_df[\"address\"]==row[\"node_i\"]].tolist()[0]\n",
    "        try:\n",
    "            index_j = address_user_df.index[address_user_df[\"address\"]==row[\"node_j\"]].tolist()[0]\n",
    "\n",
    "            user_i_list.append(address_user_df.loc[address_user_df[\"address\"]==row[\"node_i\"], 'user'].item()) \n",
    "            user_j_list.append(address_user_df.loc[address_user_df[\"address\"]==row[\"node_j\"], 'user'].item())\n",
    "            node_i_list.append(row[\"node_i\"])\n",
    "            node_j_list.append(row[\"node_j\"])\n",
    "            pw_pl_list.append(row[\"s_path_length\"])\n",
    "\n",
    "\n",
    "            bow_i = bow_list[index_i]   # Bag of words for user i\n",
    "            bow_j = bow_list[index_j]    # Bag of words for user j\n",
    "\n",
    "            union = list(set().union(bow_i, bow_j))\n",
    "            intersect = list(set(bow_i) & set(bow_j))\n",
    "\n",
    "            bow_count_i_list.append(float(len(bow_i)))\n",
    "            bow_count_j_list.append(float(len(bow_j)))\n",
    "            count_overlap_list.append((float(len(intersect)))/(float(len(union))))\n",
    "            count_union_list.append(float(len(union)))\n",
    "            count_intersect_list.append(float(len(intersect)))\n",
    "\n",
    "            if row['s_path_length' > 2]:\n",
    "                bow_i_df = pd.DataFrame(bow_i, columns = [\"word\"])\n",
    "                bow_i_df[\"intersect\"] = pd.DataFrame((bow_i_df[\"word\"].isin(bow_j)))  # if intersect = False, then, complement\n",
    "                non_overlap = list(bow_i_df[bow_i_df[\"intersect\"]==False][\"word\"])\n",
    "                non_overlap_list.append(non_overlap)\n",
    "            else: \n",
    "                non_overlap_list.append('Short path')\n",
    "\n",
    "        except: \n",
    "            unused_ij.append(node_j)\n",
    "#            print (\"At \" + str(index) + \" , node:\" + str(node_j))\n",
    "            pass\n",
    "    except: \n",
    "        unused_ij.append(node_i)\n",
    "#        print (\"At \" + str(index) + \" , node:\" + str(node_i))\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_pl_df = pd.DataFrame(node_i_list, columns =['node_i'])\n",
    "pw_pl_df['node_j'] = node_j_list\n",
    "pw_pl_df['stranger_score'] = pw_pl_list\n",
    "pw_pl_df['user_i'] = user_i_list \n",
    "pw_pl_df['user_j'] = user_j_list \n",
    "pw_pl_df['bow_count_i'] = bow_count_i_list \n",
    "pw_pl_df['bow_count_j'] = bow_count_j_list \n",
    "pw_pl_df['count_overlap'] = count_overlap_list \n",
    "pw_pl_df['count_union'] = count_union_list \n",
    "pw_pl_df['count_intersect'] = count_intersect_list \n",
    "#print len(non_overlap_list)\n",
    "#len(pw_pl_df)\n",
    "pw_pl_df['non_overlap'] = non_overlap_list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_pl_df.to_csv('pairwise_nw_bow_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(address_user_df)):\n",
    "    user_bow_df = pw_pl_df[pw_pl_df['node_i']==address_user_df.iloc[i][\"address\"]]\n",
    "    user_bow_df = user_bow_df[user_bow_df['stranger_score']>3]\n",
    "    user_bow_df = user_bow_df[['node_j', 'non_overlap']]\n",
    "    user_file_name = str(address_user_df.iloc[i][\"user\"]) + '_nonoverlap.pkl'\n",
    "    user_file_name\n",
    "    user_bow_df.to_pickle(user_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
