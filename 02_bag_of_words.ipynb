{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words \n",
    "\n",
    "\n",
    "Data pre-processsing for text analysis and pairwise word overlap\n",
    "\n",
    "* 1) Load libraries and define functions\n",
    "* 2) Import data: parsed email data and path length data\n",
    "* 3) Process and store each user's vocabulary \n",
    "* 4) Calcualte pairwise % overlaps \n",
    "* 5) Create pairwise lists of  non-overlapping words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading libraries and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "from subprocess import check_output\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.util import ngrams \n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_clean(text):\n",
    "    text = re.sub(r'\\n--.*?\\n', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'enron.com', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'Forwarded by.*?Subject:', '', text, flags=re.DOTALL) \n",
    "    text = re.sub(r'Fwd:.*?Subject:', '', text, flags=re.DOTALL) \n",
    "    text = re.sub(r'Fw:.*?Subject:', '', text, flags=re.DOTALL)     \n",
    "    text = re.sub(r'FW:.*?Subject:', '', text, flags=re.DOTALL)         \n",
    "    text = re.sub(r'Forwarded:.*?Subject:', '', text, flags=re.DOTALL)         \n",
    "    text = re.sub(r'From:.*?Subject:', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'PM', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'AM', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\",\n",
    "                 \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \n",
    "                 \"enron america corp\", \"enron\", \"etc\", \"na\", firstname, lastname))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    porter= PorterStemmer()\n",
    "    \n",
    "    text=text.rstrip()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()))])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    #stem = \" \".join(porter.stem(token) for token in normalized.split())\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing data \n",
    " \n",
    "I will use parsed email data; path length data; user-address key data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_email_data = 'C:/Users/Margeum/Dropbox/DS projects/05. Email data/emails_in_csv'\n",
    "os.chdir(path_to_email_data)\n",
    "\n",
    "emails_df = pd.read_csv('./emails_parsed.csv')\n",
    "pw_path_length_df = pd.read_csv('./pw_path_length_df.csv')\n",
    "address_user_df = pd.read_csv('./address_user_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pairs:18090\n"
     ]
    }
   ],
   "source": [
    "pw_paths_df = pw_path_length_df[pw_path_length_df['s_path_length'] > 0]\n",
    "print (\"Total number of pairs:\" + str(len(pw_paths_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text for each user\n",
    "\n",
    "Clean text and store text frequency as a list of lists \n",
    "This will be used in TF-IDF as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On a CPU, it takes about 30 minutes\n",
    "\n",
    "tf_list = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for index, row in address_user_df.iterrows():\n",
    "    \n",
    "    text_cleaned_i = []\n",
    "\n",
    "    user_i = row['user']\n",
    "    print user_i\n",
    "    \n",
    "    lastname = row['address'].split('@')[0].split('.')[-1]\n",
    "    firstname = str(row['address'].split('.')[0])\n",
    "\n",
    "    text_to_clean_df_i = emails_df[emails_df[\"user\"] == user_i][[\"content\", \"user\"]].reset_index()\n",
    "        \n",
    "    for text in text_to_clean_df_i['content']:\n",
    "        text_cleaned_i.append(clean(email_clean(text)).split())\n",
    "\n",
    "    unlisted_text_cleaned_i = [item for sublist in text_cleaned_i for item in sublist]\n",
    "    freqdist_user_i = nltk.FreqDist(ngrams(unlisted_text_cleaned_i, 1))\n",
    "\n",
    "    tf_list.append(freqdist_user_i)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_list = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for index, row in address_user_df.iterrows():\n",
    " # Convert frequency into indicator (from frequency dict to bag of words)\n",
    "    bow_i = []\n",
    "    user_i = row['user']\n",
    "    print user_i\n",
    "    \n",
    "    freqdist_user_i = tf_list[index]\n",
    "    \n",
    "    for k in range(len(freqdist_user_i)):\n",
    "        bow_i.append(str(list(freqdist_user_i)[k][0]))\n",
    "    bow_list.append(bow_i)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create overlap scores and lists of non-overlapping words (i.e., pair-wise jargon lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pw_paths_df, run 1) overlap; union; intersect  --> a list\n",
    "# For pw_paths_df, run 2) i_minus_j (we have both ways --> so, I only take care about i here)\n",
    "\n",
    "node_i_list = []\n",
    "node_j_list = []\n",
    "pw_pl_list = []\n",
    "user_i_list = []\n",
    "user_j_list = []\n",
    "bow_count_i_list = []\n",
    "bow_count_j_list = []\n",
    "count_overlap_list = []\n",
    "count_union_list = []\n",
    "count_intersect_list = []\n",
    "non_overlap_list = []\n",
    "unused_ij = []\n",
    "\n",
    "for index, row in pw_paths_df.iterrows():\n",
    "    \n",
    "    ## Need to check this step \n",
    "    try: \n",
    "        index_i = address_user_df.index[address_user_df[\"address\"]==row[\"node_i\"]].tolist()[0]\n",
    "        try:\n",
    "            index_j = address_user_df.index[address_user_df[\"address\"]==row[\"node_j\"]].tolist()[0]\n",
    "\n",
    "            user_i_list.append(address_user_df.loc[address_user_df[\"address\"]==row[\"node_i\"], 'user'].item()) \n",
    "            user_j_list.append(address_user_df.loc[address_user_df[\"address\"]==row[\"node_j\"], 'user'].item())\n",
    "            node_i_list.append(row[\"node_i\"])\n",
    "            node_j_list.append(row[\"node_j\"])\n",
    "            pw_pl_list.append(row[\"s_path_length\"])\n",
    "\n",
    "\n",
    "            bow_i = bow_list[index_i]   # Bag of words for user i\n",
    "            bow_j = bow_list[index_j]    # Bag of words for user j\n",
    "\n",
    "            union = list(set().union(bow_i, bow_j))\n",
    "            intersect = list(set(bow_i) & set(bow_j))\n",
    "\n",
    "            bow_count_i_list.append(float(len(bow_i)))\n",
    "            bow_count_j_list.append(float(len(bow_j)))\n",
    "            count_overlap_list.append((float(len(intersect)))/(float(len(union))))\n",
    "            count_union_list.append(float(len(union)))\n",
    "            count_intersect_list.append(float(len(intersect)))\n",
    "\n",
    "            if row['s_path_length' > 2]:\n",
    "                bow_i_df = pd.DataFrame(bow_i, columns = [\"word\"])\n",
    "                bow_i_df[\"intersect\"] = pd.DataFrame((bow_i_df[\"word\"].isin(bow_j)))  # if intersect = False, then, complement\n",
    "                non_overlap = list(bow_i_df[bow_i_df[\"intersect\"]==False][\"word\"])\n",
    "                non_overlap_list.append(non_overlap)\n",
    "            else: \n",
    "                non_overlap_list.append('Short path')\n",
    "\n",
    "        except: \n",
    "            unused_ij.append(node_j)\n",
    "#            print (\"At \" + str(index) + \" , node:\" + str(node_j))\n",
    "            pass\n",
    "    except: \n",
    "        unused_ij.append(node_i)\n",
    "#        print (\"At \" + str(index) + \" , node:\" + str(node_i))\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_pl_df = pd.DataFrame(node_i_list, columns =['node_i'])\n",
    "pw_pl_df['node_j'] = node_j_list\n",
    "pw_pl_df['stranger_score'] = pw_pl_list\n",
    "pw_pl_df['user_i'] = user_i_list \n",
    "pw_pl_df['user_j'] = user_j_list \n",
    "pw_pl_df['bow_count_i'] = bow_count_i_list \n",
    "pw_pl_df['bow_count_j'] = bow_count_j_list \n",
    "pw_pl_df['count_overlap'] = count_overlap_list \n",
    "pw_pl_df['count_union'] = count_union_list \n",
    "pw_pl_df['count_intersect'] = count_intersect_list \n",
    "#print len(non_overlap_list)\n",
    "#len(pw_pl_df)\n",
    "pw_pl_df['non_overlap'] = non_overlap_list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_pl_df.to_csv('pairwise_nw_bow_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(address_user_df)):\n",
    "    user_bow_df = pw_pl_df[pw_pl_df['node_i']==address_user_df.iloc[i][\"address\"]]\n",
    "    user_bow_df = user_bow_df[user_bow_df['stranger_score']>2]\n",
    "    user_bow_df = user_bow_df[['node_j', 'non_overlap']]\n",
    "    user_file_name = str(address_user_df.iloc[i][\"user\"]) + '_nonoverlap.pkl'\n",
    "    user_file_name\n",
    "    user_bow_df.to_pickle(user_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
