{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding most-widely used, multi-contextual words\n",
    "\n",
    "\n",
    "To build the third function, \"Context Detector\", I will extract the word-sense associatoin from Bert model. \n",
    "But I certainly do not want to serach for contextual meaning for every possible word that a user can possibly use. Therefore, in this notebook, I will come up with a dictionary of words that Enron employees commonly use and potentially in different contexts. To do so, I will use email topic labels, which were hand-coded by CMU students (available from https://data.world/brianray/enron-email-dataset)\n",
    "\n",
    "\n",
    "* 1) Load libraries and define functions\n",
    "* 2) Import data: email data and labeld data \n",
    "* 3) Calculate TF-IDF scores for each words in the company-wide email corpus by selecting words that occur frequently, across many people's \n",
    "* 4) Using the topic labels data, calculate topic-level term frequency\n",
    "    * Join the labeled data with TF-IDF results\n",
    "* 5) Build and store a dictionary of words that widely-used (according to TF-IDF scores) and multi-contextual words (that appear across all categories, quiet frequently).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading libraries and defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from subprocess import check_output\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.util import ngrams \n",
    "from nltk.probability import FreqDist\n",
    "import os, re, nltk, string\n",
    "\n",
    "def email_clean(text):\n",
    "    text = re.sub(r'\\n--.*?\\n', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'enron.com', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'Forwarded by.*?Subject:', '', text, flags=re.DOTALL) \n",
    "    text = re.sub(r'Fwd:.*?Subject:', '', text, flags=re.DOTALL) \n",
    "    text = re.sub(r'Fw:.*?Subject:', '', text, flags=re.DOTALL)     \n",
    "    text = re.sub(r'FW:.*?Subject:', '', text, flags=re.DOTALL)         \n",
    "    text = re.sub(r'Forwarded:.*?Subject:', '', text, flags=re.DOTALL)         \n",
    "    text = re.sub(r'From:.*?Subject:', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'PM', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'AM', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\",\n",
    "                 \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \n",
    "                 \"enron america corp\", \"enron\", \"etc\", \"na\"))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    porter= PorterStemmer()\n",
    "    \n",
    "    text=text.rstrip()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()))])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    #stem = \" \".join(porter.stem(token) for token in normalized.split())\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Margeum\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2714: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "path_to_email_data = 'C:/Users/Margeum/Dropbox/DS projects/05. Email data/emails_in_csv'\n",
    "os.chdir(path_to_email_data)\n",
    "emails_df = pd.read_csv('emails_parsed.csv')\n",
    "labeled_emails_df = pd.read_csv(\"enron_05_17_2015_with_labels_v2.csv\") # Labeled data \n",
    "address_user_df = pd.read_csv('./address_user_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculating TF-IDF scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate word frequencies for the email corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allen-p\n",
      "arnold-j\n",
      "arora-h\n",
      "badeer-r\n",
      "bailey-s\n",
      "bass-e\n",
      "baughman-d\n",
      "beck-s\n",
      "blair-l\n",
      "brawner-s\n",
      "buy-r\n",
      "campbell-l\n",
      "carson-m\n",
      "cash-m\n",
      "causholli-m\n",
      "corman-s\n",
      "cuilla-m\n",
      "dasovich-j\n",
      "davis-d\n",
      "dean-c\n",
      "delainey-d\n",
      "derrick-j\n",
      "dickson-s\n",
      "donoho-l\n",
      "donohoe-t\n",
      "dorland-c\n",
      "ermis-f\n",
      "farmer-d\n",
      "fischer-m\n",
      "forney-j\n",
      "fossum-d\n",
      "gang-l\n",
      "gay-r\n",
      "geaccone-t\n",
      "germany-c\n",
      "giron-d\n",
      "griffith-j\n",
      "grigsby-m\n",
      "guzman-m\n",
      "haedicke-m\n",
      "hain-m\n",
      "harris-s\n",
      "hayslett-r\n",
      "heard-m\n",
      "hendrickson-s\n",
      "hernandez-j\n",
      "hodge-j\n",
      "holst-k\n",
      "horton-s\n",
      "hyatt-k\n",
      "hyvl-d\n",
      "jones-t\n",
      "kaminski-v\n",
      "kean-s\n",
      "keavey-p\n",
      "keiser-k\n",
      "king-j\n",
      "kitchen-l\n",
      "kuykendall-t\n",
      "lavorato-j\n",
      "lay-k\n",
      "lenhart-m\n",
      "lewis-a\n",
      "lokay-m\n",
      "lokey-t\n",
      "love-p\n",
      "lucci-p\n",
      "maggi-m\n",
      "mann-k\n",
      "martin-t\n",
      "may-l\n",
      "mccarty-d\n",
      "mcconnell-m\n",
      "mckay-b\n",
      "mckay-j\n",
      "mclaughlin-e\n",
      "meyers-a\n",
      "motley-m\n",
      "neal-s\n",
      "nemec-g\n",
      "panus-s\n",
      "parks-j\n",
      "pereira-s\n",
      "perlingiere-d\n",
      "pimenov-v\n",
      "platter-p\n",
      "presto-k\n",
      "quenet-j\n",
      "quigley-d\n",
      "rapp-b\n",
      "reitmeyer-j\n",
      "richey-c\n",
      "ring-a\n",
      "ring-r\n",
      "rogers-b\n",
      "ruscitti-k\n",
      "sager-e\n",
      "saibi-e\n",
      "salisbury-h\n",
      "sanchez-m\n",
      "sanders-r\n",
      "scholtes-d\n",
      "schoolcraft-d\n",
      "schwieger-j\n",
      "scott-s\n",
      "semperger-c\n",
      "shackleton-s\n",
      "shankman-j\n",
      "shapiro-r\n",
      "shively-h\n",
      "skilling-j\n",
      "slinger-r\n",
      "smith-m\n",
      "solberg-g\n",
      "staab-t\n",
      "steffes-j\n",
      "stepenovitch-j\n",
      "stokley-c\n",
      "storey-g\n",
      "sturm-f\n",
      "swerzbin-m\n",
      "symes-k\n",
      "taylor-m\n",
      "tholt-j\n",
      "thomas-p\n",
      "townsend-j\n",
      "tycholiz-b\n",
      "ward-k\n",
      "watson-k\n",
      "weldon-c\n",
      "whalley-g\n",
      "white-s\n",
      "whitt-m\n",
      "williams-j\n",
      "wolfe-j\n",
      "zipper-a\n",
      "zufferli-j\n"
     ]
    }
   ],
   "source": [
    "tf_list = []\n",
    "\n",
    "for index, row in address_user_df.iterrows():\n",
    "    \n",
    "    text_cleaned_i = []\n",
    "\n",
    "    user_i = row['user']\n",
    "    print user_i\n",
    "    \n",
    "    lastname = row['address'].split('@')[0].split('.')[-1]\n",
    "    firstname = str(row['address'].split('.')[0])\n",
    "\n",
    "    text_to_clean_df_i = emails_df[emails_df[\"user\"] == user_i][[\"content\", \"user\"]].reset_index()\n",
    "        \n",
    "    for text in text_to_clean_df_i['content']:\n",
    "        text_cleaned_i.append(clean(email_clean(text)).split())\n",
    "\n",
    "    unlisted_text_cleaned_i = [item for sublist in text_cleaned_i for item in sublist]\n",
    "    freqdist_user_i = nltk.FreqDist(ngrams(unlisted_text_cleaned_i, 1))\n",
    "\n",
    "    tf_list.append(freqdist_user_i)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating word frequencies at the user-account level (Documet-level frequencies where all emails in a user's account are treated as a document) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(jurek,)</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(wefc,)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(caqigapifbgbfagecbgdzbqyaxwibagya,)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(qaaadfa,)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(rjizidi,)</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Frequency\n",
       "Term                                           \n",
       "(jurek,)                                      2\n",
       "(wefc,)                                       1\n",
       "(caqigapifbgbfagecbgdzbqyaxwibagya,)          1\n",
       "(qaaadfa,)                                    1\n",
       "(rjizidi,)                                    2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_text_cleaned_i = []\n",
    "\n",
    "for text in emails_df[\"content\"]:\n",
    "    idf_text_cleaned_i.append(clean(email_clean(text)).split())\n",
    "\n",
    "idf_unlisted_text_cleaned_i = [item for sublist in idf_text_cleaned_i for item in sublist]\n",
    "idf_list = nltk.FreqDist(ngrams(idf_unlisted_text_cleaned_i, 1))\n",
    "\n",
    "df_idf = pd.DataFrame.from_dict(idf_list, orient='index')\n",
    "df_idf.columns = ['Frequency']\n",
    "df_idf.index.name = 'Term'\n",
    "df_idf.sort_values(by = \"Frequency\", ascending = False)\n",
    "df_idf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge the corpus-level frequencies and account-level frequencies by using word as key. Then, calculate the TF-IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>tf_idf_0</th>\n",
       "      <th>tf_idf_1</th>\n",
       "      <th>tf_idf_2</th>\n",
       "      <th>tf_idf_3</th>\n",
       "      <th>tf_idf_4</th>\n",
       "      <th>tf_idf_5</th>\n",
       "      <th>tf_idf_6</th>\n",
       "      <th>tf_idf_7</th>\n",
       "      <th>tf_idf_8</th>\n",
       "      <th>...</th>\n",
       "      <th>tf_idf_130</th>\n",
       "      <th>tf_idf_131</th>\n",
       "      <th>tf_idf_132</th>\n",
       "      <th>tf_idf_133</th>\n",
       "      <th>tf_idf_134</th>\n",
       "      <th>tf_idf_135</th>\n",
       "      <th>tf_idf_136</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "      <th>count_notnull</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(com,)</th>\n",
       "      <td>664513</td>\n",
       "      <td>0.064602</td>\n",
       "      <td>0.136463</td>\n",
       "      <td>0.035238</td>\n",
       "      <td>0.040803</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.536490</td>\n",
       "      <td>0.230609</td>\n",
       "      <td>0.045009</td>\n",
       "      <td>0.041880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064295</td>\n",
       "      <td>0.027775</td>\n",
       "      <td>0.041008</td>\n",
       "      <td>0.075271</td>\n",
       "      <td>0.031134</td>\n",
       "      <td>0.030083</td>\n",
       "      <td>0.006822</td>\n",
       "      <td>0.121391</td>\n",
       "      <td>0.054427</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(ect,)</th>\n",
       "      <td>595113</td>\n",
       "      <td>0.051099</td>\n",
       "      <td>0.114941</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.283041</td>\n",
       "      <td>0.022942</td>\n",
       "      <td>0.842460</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013989</td>\n",
       "      <td>0.049141</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.007758</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109993</td>\n",
       "      <td>0.067947</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(please,)</th>\n",
       "      <td>379986</td>\n",
       "      <td>0.042960</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>0.014132</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.009904</td>\n",
       "      <td>0.093490</td>\n",
       "      <td>0.057190</td>\n",
       "      <td>0.243459</td>\n",
       "      <td>0.049375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021529</td>\n",
       "      <td>0.045836</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.029836</td>\n",
       "      <td>0.028927</td>\n",
       "      <td>0.022144</td>\n",
       "      <td>0.009462</td>\n",
       "      <td>0.065944</td>\n",
       "      <td>0.010998</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(e,)</th>\n",
       "      <td>378364</td>\n",
       "      <td>0.030563</td>\n",
       "      <td>0.067662</td>\n",
       "      <td>0.011867</td>\n",
       "      <td>0.027738</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.136676</td>\n",
       "      <td>0.055550</td>\n",
       "      <td>0.097857</td>\n",
       "      <td>0.021375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029532</td>\n",
       "      <td>0.018377</td>\n",
       "      <td>0.008624</td>\n",
       "      <td>0.027591</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>0.012677</td>\n",
       "      <td>0.006756</td>\n",
       "      <td>0.065836</td>\n",
       "      <td>0.022772</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(would,)</th>\n",
       "      <td>325382</td>\n",
       "      <td>0.035473</td>\n",
       "      <td>0.071772</td>\n",
       "      <td>0.008231</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.080464</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.194751</td>\n",
       "      <td>0.042077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030666</td>\n",
       "      <td>0.018186</td>\n",
       "      <td>0.010902</td>\n",
       "      <td>0.008887</td>\n",
       "      <td>0.012868</td>\n",
       "      <td>0.019618</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.056304</td>\n",
       "      <td>0.018686</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(power,)</th>\n",
       "      <td>309093</td>\n",
       "      <td>0.019926</td>\n",
       "      <td>0.049984</td>\n",
       "      <td>0.005755</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>0.010519</td>\n",
       "      <td>0.041472</td>\n",
       "      <td>0.068072</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025996</td>\n",
       "      <td>0.043382</td>\n",
       "      <td>0.009769</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>0.009262</td>\n",
       "      <td>0.015549</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>0.053380</td>\n",
       "      <td>0.047698</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(hou,)</th>\n",
       "      <td>273226</td>\n",
       "      <td>0.024497</td>\n",
       "      <td>0.050358</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.157157</td>\n",
       "      <td>0.010776</td>\n",
       "      <td>0.302318</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007567</td>\n",
       "      <td>0.020162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004837</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.052184</td>\n",
       "      <td>0.012847</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(energy,)</th>\n",
       "      <td>298781</td>\n",
       "      <td>0.010515</td>\n",
       "      <td>0.073026</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.021464</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.022019</td>\n",
       "      <td>0.054794</td>\n",
       "      <td>0.020162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031497</td>\n",
       "      <td>0.007524</td>\n",
       "      <td>0.005740</td>\n",
       "      <td>0.016279</td>\n",
       "      <td>0.011624</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.051578</td>\n",
       "      <td>0.030421</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(company,)</th>\n",
       "      <td>288719</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>0.122422</td>\n",
       "      <td>0.005604</td>\n",
       "      <td>0.010342</td>\n",
       "      <td>0.005147</td>\n",
       "      <td>0.015994</td>\n",
       "      <td>0.022055</td>\n",
       "      <td>0.072010</td>\n",
       "      <td>0.030377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084517</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.041657</td>\n",
       "      <td>0.007793</td>\n",
       "      <td>0.007312</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.049604</td>\n",
       "      <td>0.025251</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(venturewire,)</th>\n",
       "      <td>8506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.048956</td>\n",
       "      <td>0.007185</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Frequency  tf_idf_0  tf_idf_1  tf_idf_2  tf_idf_3  tf_idf_4  \\\n",
       "Term                                                                          \n",
       "(com,)             664513  0.064602  0.136463  0.035238  0.040803  0.002539   \n",
       "(ect,)             595113  0.051099  0.114941  0.002162  0.004349  0.000076   \n",
       "(please,)          379986  0.042960  0.069430  0.014132  0.014648  0.009904   \n",
       "(e,)               378364  0.030563  0.067662  0.011867  0.027738  0.002580   \n",
       "(would,)           325382  0.035473  0.071772  0.008231  0.011096  0.003156   \n",
       "(power,)           309093  0.019926  0.049984  0.005755  0.024375  0.002442   \n",
       "(hou,)             273226  0.024497  0.050358  0.001461  0.002490       NaN   \n",
       "(energy,)          298781  0.010515  0.073026  0.002870  0.021464  0.006077   \n",
       "(company,)         288719  0.010054  0.122422  0.005604  0.010342  0.005147   \n",
       "(venturewire,)       8506       NaN       NaN  0.000017       NaN       NaN   \n",
       "\n",
       "                tf_idf_5  tf_idf_6  tf_idf_7  tf_idf_8  ...  tf_idf_130  \\\n",
       "Term                                                    ...               \n",
       "(com,)          0.536490  0.230609  0.045009  0.041880  ...    0.064295   \n",
       "(ect,)          0.283041  0.022942  0.842460  0.000153  ...    0.013989   \n",
       "(please,)       0.093490  0.057190  0.243459  0.049375  ...    0.021529   \n",
       "(e,)            0.136676  0.055550  0.097857  0.021375  ...    0.029532   \n",
       "(would,)        0.080464  0.023600  0.194751  0.042077  ...    0.030666   \n",
       "(power,)        0.010519  0.041472  0.068072  0.011486  ...    0.025996   \n",
       "(hou,)          0.157157  0.010776  0.302318  0.000623  ...    0.007567   \n",
       "(energy,)       0.010877  0.022019  0.054794  0.020162  ...    0.031497   \n",
       "(company,)      0.015994  0.022055  0.072010  0.030377  ...    0.084517   \n",
       "(venturewire,)       NaN       NaN       NaN       NaN  ...         NaN   \n",
       "\n",
       "                tf_idf_131  tf_idf_132  tf_idf_133  tf_idf_134  tf_idf_135  \\\n",
       "Term                                                                         \n",
       "(com,)            0.027775    0.041008    0.075271    0.031134    0.030083   \n",
       "(ect,)            0.049141    0.000127    0.011700    0.007758    0.003001   \n",
       "(please,)         0.045836    0.014574    0.029836    0.028927    0.022144   \n",
       "(e,)              0.018377    0.008624    0.027591    0.021989    0.012677   \n",
       "(would,)          0.018186    0.010902    0.008887    0.012868    0.019618   \n",
       "(power,)          0.043382    0.009769    0.006045    0.009262    0.015549   \n",
       "(hou,)            0.020162         NaN    0.004837    0.003712    0.000479   \n",
       "(energy,)         0.007524    0.005740    0.016279    0.011624    0.009888   \n",
       "(company,)        0.005772    0.005243    0.041657    0.007793    0.007312   \n",
       "(venturewire,)         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "                tf_idf_136      mean  variance  count_notnull  \n",
       "Term                                                           \n",
       "(com,)            0.006822  0.121391  0.054427            137  \n",
       "(ect,)                 NaN  0.109993  0.067947            134  \n",
       "(please,)         0.009462  0.065944  0.010998            137  \n",
       "(e,)              0.006756  0.065836  0.022772            137  \n",
       "(would,)          0.005609  0.056304  0.018686            137  \n",
       "(power,)          0.005634  0.053380  0.047698            137  \n",
       "(hou,)                 NaN  0.052184  0.012847            122  \n",
       "(energy,)         0.003087  0.051578  0.030421            137  \n",
       "(company,)        0.001756  0.049604  0.025251            137  \n",
       "(venturewire,)         NaN  0.048956  0.007185              3  \n",
       "\n",
       "[10 rows x 141 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tf_idf_stacked = df_idf\n",
    "\n",
    "for i in range(len(address_user_df)):\n",
    "    my_td = pd.DataFrame.from_dict(tf_list[i], orient='index')\n",
    "    my_td.columns = ['Frequency']\n",
    "    my_td.index.name = 'Term'\n",
    "\n",
    "    tf_idf_i = my_td.reset_index().set_index('Term').join(df_idf.reset_index().set_index('Term'), on= 'Term', how='left',\n",
    "                                              lsuffix='_left', rsuffix='_right')\n",
    "\n",
    "    tf_idf_i[\"idf\"] =  np.log(tf_idf_i['Frequency_right'])/(len(df_idf)+1)\n",
    "    tf_idf_i[\"tf_idf\"] = tf_idf_i['Frequency_left']*tf_idf_i['idf']\n",
    "    tf_idf_i.sort_values(by = \"tf_idf\", ascending = False)\n",
    "\n",
    "    tf_idf_i = pd.DataFrame(tf_idf_i[\"tf_idf\"])\n",
    "    tf_idf_i.columns = tf_idf_i.columns + \"_\" + str(i)\n",
    "\n",
    "    tf_idf_stacked = tf_idf_stacked.join(tf_idf_i, on = 'Term', how ='left', lsuffix='_left', rsuffix  ='right')\n",
    "\n",
    "\n",
    "col_list= list(tf_idf_stacked)\n",
    "col_list.remove('Frequency')\n",
    "\n",
    "\n",
    "tf_idf_stacked['mean'] = tf_idf_stacked[col_list].mean(axis=1)\n",
    "tf_idf_stacked['variance'] = tf_idf_stacked[col_list].var(axis=1)\n",
    "tf_idf_stacked['count_notnull'] = tf_idf_stacked[col_list].count(axis=1)\n",
    "tf_idf_stacked.sort_values(by = \"mean\", ascending = False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522762 unique words are in the email corpus\n"
     ]
    }
   ],
   "source": [
    "print (str(len(tf_idf_stacked[\"mean\"])) + ' unique words are in the email corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to cut the words from this complete list. First, we will keep only top 1 percent words in terms of mean TF-IDF score. In other words, we will keep words with high importance in terms of their frequency across different accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5156 unique words are in this list of top 1 percent words)\n",
      "Top 1 percent words account for 0.798830791283 unique words are in this list of top 1 percent words)% in terms of frequency\n"
     ]
    }
   ],
   "source": [
    "mean_val_cut_99 = tf_idf_stacked[\"mean\"].quantile(.99)\n",
    "df_mean_val_cut_99 = tf_idf_stacked[tf_idf_stacked['mean'] > mean_val_cut_99]\n",
    "\n",
    "print (str(len(df_mean_val_cut_99)) + ' unique words are in this list of top 1 percent words)')\n",
    "print ('Top 1 percent words account for ' + \n",
    "       str(float(df_mean_val_cut_99[\"Frequency\"].sum())/float(tf_idf_stacked[\"Frequency\"].sum())) +\n",
    "       ' unique words are in this list of top 1 percent words)' + '% in terms of frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>tf_idf_0</th>\n",
       "      <th>tf_idf_1</th>\n",
       "      <th>tf_idf_2</th>\n",
       "      <th>tf_idf_3</th>\n",
       "      <th>tf_idf_4</th>\n",
       "      <th>tf_idf_5</th>\n",
       "      <th>tf_idf_6</th>\n",
       "      <th>tf_idf_7</th>\n",
       "      <th>tf_idf_8</th>\n",
       "      <th>...</th>\n",
       "      <th>tf_idf_130</th>\n",
       "      <th>tf_idf_131</th>\n",
       "      <th>tf_idf_132</th>\n",
       "      <th>tf_idf_133</th>\n",
       "      <th>tf_idf_134</th>\n",
       "      <th>tf_idf_135</th>\n",
       "      <th>tf_idf_136</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "      <th>count_notnull</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(com,)</th>\n",
       "      <td>664513</td>\n",
       "      <td>0.064602</td>\n",
       "      <td>0.136463</td>\n",
       "      <td>0.035238</td>\n",
       "      <td>0.040803</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.536490</td>\n",
       "      <td>0.230609</td>\n",
       "      <td>0.045009</td>\n",
       "      <td>0.041880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064295</td>\n",
       "      <td>0.027775</td>\n",
       "      <td>0.041008</td>\n",
       "      <td>0.075271</td>\n",
       "      <td>0.031134</td>\n",
       "      <td>0.030083</td>\n",
       "      <td>0.006822</td>\n",
       "      <td>0.121391</td>\n",
       "      <td>0.054427</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(ect,)</th>\n",
       "      <td>595113</td>\n",
       "      <td>0.051099</td>\n",
       "      <td>0.114941</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.283041</td>\n",
       "      <td>0.022942</td>\n",
       "      <td>0.842460</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013989</td>\n",
       "      <td>0.049141</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.007758</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109993</td>\n",
       "      <td>0.067947</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(please,)</th>\n",
       "      <td>379986</td>\n",
       "      <td>0.042960</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>0.014132</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.009904</td>\n",
       "      <td>0.093490</td>\n",
       "      <td>0.057190</td>\n",
       "      <td>0.243459</td>\n",
       "      <td>0.049375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021529</td>\n",
       "      <td>0.045836</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.029836</td>\n",
       "      <td>0.028927</td>\n",
       "      <td>0.022144</td>\n",
       "      <td>0.009462</td>\n",
       "      <td>0.065944</td>\n",
       "      <td>0.010998</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(e,)</th>\n",
       "      <td>378364</td>\n",
       "      <td>0.030563</td>\n",
       "      <td>0.067662</td>\n",
       "      <td>0.011867</td>\n",
       "      <td>0.027738</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.136676</td>\n",
       "      <td>0.055550</td>\n",
       "      <td>0.097857</td>\n",
       "      <td>0.021375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029532</td>\n",
       "      <td>0.018377</td>\n",
       "      <td>0.008624</td>\n",
       "      <td>0.027591</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>0.012677</td>\n",
       "      <td>0.006756</td>\n",
       "      <td>0.065836</td>\n",
       "      <td>0.022772</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(would,)</th>\n",
       "      <td>325382</td>\n",
       "      <td>0.035473</td>\n",
       "      <td>0.071772</td>\n",
       "      <td>0.008231</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.080464</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.194751</td>\n",
       "      <td>0.042077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030666</td>\n",
       "      <td>0.018186</td>\n",
       "      <td>0.010902</td>\n",
       "      <td>0.008887</td>\n",
       "      <td>0.012868</td>\n",
       "      <td>0.019618</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.056304</td>\n",
       "      <td>0.018686</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(power,)</th>\n",
       "      <td>309093</td>\n",
       "      <td>0.019926</td>\n",
       "      <td>0.049984</td>\n",
       "      <td>0.005755</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>0.010519</td>\n",
       "      <td>0.041472</td>\n",
       "      <td>0.068072</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025996</td>\n",
       "      <td>0.043382</td>\n",
       "      <td>0.009769</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>0.009262</td>\n",
       "      <td>0.015549</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>0.053380</td>\n",
       "      <td>0.047698</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(hou,)</th>\n",
       "      <td>273226</td>\n",
       "      <td>0.024497</td>\n",
       "      <td>0.050358</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.157157</td>\n",
       "      <td>0.010776</td>\n",
       "      <td>0.302318</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007567</td>\n",
       "      <td>0.020162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004837</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.052184</td>\n",
       "      <td>0.012847</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(energy,)</th>\n",
       "      <td>298781</td>\n",
       "      <td>0.010515</td>\n",
       "      <td>0.073026</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.021464</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.022019</td>\n",
       "      <td>0.054794</td>\n",
       "      <td>0.020162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031497</td>\n",
       "      <td>0.007524</td>\n",
       "      <td>0.005740</td>\n",
       "      <td>0.016279</td>\n",
       "      <td>0.011624</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.051578</td>\n",
       "      <td>0.030421</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(company,)</th>\n",
       "      <td>288719</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>0.122422</td>\n",
       "      <td>0.005604</td>\n",
       "      <td>0.010342</td>\n",
       "      <td>0.005147</td>\n",
       "      <td>0.015994</td>\n",
       "      <td>0.022055</td>\n",
       "      <td>0.072010</td>\n",
       "      <td>0.030377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084517</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.041657</td>\n",
       "      <td>0.007793</td>\n",
       "      <td>0.007312</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.049604</td>\n",
       "      <td>0.025251</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(venturewire,)</th>\n",
       "      <td>8506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.048956</td>\n",
       "      <td>0.007185</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Frequency  tf_idf_0  tf_idf_1  tf_idf_2  tf_idf_3  tf_idf_4  \\\n",
       "Term                                                                          \n",
       "(com,)             664513  0.064602  0.136463  0.035238  0.040803  0.002539   \n",
       "(ect,)             595113  0.051099  0.114941  0.002162  0.004349  0.000076   \n",
       "(please,)          379986  0.042960  0.069430  0.014132  0.014648  0.009904   \n",
       "(e,)               378364  0.030563  0.067662  0.011867  0.027738  0.002580   \n",
       "(would,)           325382  0.035473  0.071772  0.008231  0.011096  0.003156   \n",
       "(power,)           309093  0.019926  0.049984  0.005755  0.024375  0.002442   \n",
       "(hou,)             273226  0.024497  0.050358  0.001461  0.002490       NaN   \n",
       "(energy,)          298781  0.010515  0.073026  0.002870  0.021464  0.006077   \n",
       "(company,)         288719  0.010054  0.122422  0.005604  0.010342  0.005147   \n",
       "(venturewire,)       8506       NaN       NaN  0.000017       NaN       NaN   \n",
       "\n",
       "                tf_idf_5  tf_idf_6  tf_idf_7  tf_idf_8  ...  tf_idf_130  \\\n",
       "Term                                                    ...               \n",
       "(com,)          0.536490  0.230609  0.045009  0.041880  ...    0.064295   \n",
       "(ect,)          0.283041  0.022942  0.842460  0.000153  ...    0.013989   \n",
       "(please,)       0.093490  0.057190  0.243459  0.049375  ...    0.021529   \n",
       "(e,)            0.136676  0.055550  0.097857  0.021375  ...    0.029532   \n",
       "(would,)        0.080464  0.023600  0.194751  0.042077  ...    0.030666   \n",
       "(power,)        0.010519  0.041472  0.068072  0.011486  ...    0.025996   \n",
       "(hou,)          0.157157  0.010776  0.302318  0.000623  ...    0.007567   \n",
       "(energy,)       0.010877  0.022019  0.054794  0.020162  ...    0.031497   \n",
       "(company,)      0.015994  0.022055  0.072010  0.030377  ...    0.084517   \n",
       "(venturewire,)       NaN       NaN       NaN       NaN  ...         NaN   \n",
       "\n",
       "                tf_idf_131  tf_idf_132  tf_idf_133  tf_idf_134  tf_idf_135  \\\n",
       "Term                                                                         \n",
       "(com,)            0.027775    0.041008    0.075271    0.031134    0.030083   \n",
       "(ect,)            0.049141    0.000127    0.011700    0.007758    0.003001   \n",
       "(please,)         0.045836    0.014574    0.029836    0.028927    0.022144   \n",
       "(e,)              0.018377    0.008624    0.027591    0.021989    0.012677   \n",
       "(would,)          0.018186    0.010902    0.008887    0.012868    0.019618   \n",
       "(power,)          0.043382    0.009769    0.006045    0.009262    0.015549   \n",
       "(hou,)            0.020162         NaN    0.004837    0.003712    0.000479   \n",
       "(energy,)         0.007524    0.005740    0.016279    0.011624    0.009888   \n",
       "(company,)        0.005772    0.005243    0.041657    0.007793    0.007312   \n",
       "(venturewire,)         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "                tf_idf_136      mean  variance  count_notnull  \n",
       "Term                                                           \n",
       "(com,)            0.006822  0.121391  0.054427            137  \n",
       "(ect,)                 NaN  0.109993  0.067947            134  \n",
       "(please,)         0.009462  0.065944  0.010998            137  \n",
       "(e,)              0.006756  0.065836  0.022772            137  \n",
       "(would,)          0.005609  0.056304  0.018686            137  \n",
       "(power,)          0.005634  0.053380  0.047698            137  \n",
       "(hou,)                 NaN  0.052184  0.012847            122  \n",
       "(energy,)         0.003087  0.051578  0.030421            137  \n",
       "(company,)        0.001756  0.049604  0.025251            137  \n",
       "(venturewire,)         NaN  0.048956  0.007185              3  \n",
       "\n",
       "[10 rows x 141 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_val_cut_99.sort_values(by = \"mean\", ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column in the df_mean_val_cut_99 dataframe, It shows that some of these top-1-percent words appear only in some of the accounts. These words might be important for those user who use them, but less likely to be a multi-context words. Therefore, I will remove them from the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0% of words have more than 105.0 accounts\n"
     ]
    }
   ],
   "source": [
    "count_null_cut = 0.5\n",
    "print (str(count_null_cut*100) + '% of words have more than ' + str(df_mean_val_cut_99[\"count_notnull\"].quantile(count_null_cut)) + ' accounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although 50% is an arbitrary cut, we will use that as our cut point for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 2571 words in the list\n"
     ]
    }
   ],
   "source": [
    "keywords_dictionary = df_mean_val_cut_99[df_mean_val_cut_99[\"count_notnull\"]> df_mean_val_cut_99[\"count_notnull\"].quantile(count_null_cut)]\n",
    "print ('We now have ' + str(len(keywords_dictionary)) + ' words in the list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(keywords_dictionary)\n",
    "#keywords_dictionary.to_csv('keywords_dictionary.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Count topic-level word frequencies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I want to figure out whether the key words in our dictionary appears across differen topic areas. The labeled_emails_df has email topic labels, but only for a subset of emails. Using message IDs, I will merge the labeles to our keywords dictionary so that we can further trim down the dictionary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset provides topic labels for 1702 emails\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Message-ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Subject</th>\n",
       "      <th>X-From</th>\n",
       "      <th>X-To</th>\n",
       "      <th>X-cc</th>\n",
       "      <th>X-bcc</th>\n",
       "      <th>...</th>\n",
       "      <th>Cat_10_level_1</th>\n",
       "      <th>Cat_10_level_2</th>\n",
       "      <th>Cat_10_weight</th>\n",
       "      <th>Cat_11_level_1</th>\n",
       "      <th>Cat_11_level_2</th>\n",
       "      <th>Cat_11_weight</th>\n",
       "      <th>Cat_12_level_1</th>\n",
       "      <th>Cat_12_level_2</th>\n",
       "      <th>Cat_12_weight</th>\n",
       "      <th>labeled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>379</td>\n",
       "      <td>&lt;9831685.1075855725804.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>2001-03-15 14:45:00</td>\n",
       "      <td>frozenset({'phillip.allen@enron.com'})</td>\n",
       "      <td>frozenset({'todd.burke@enron.com'})</td>\n",
       "      <td>Re: Confidential Employee Information/Lenhart</td>\n",
       "      <td>Phillip K Allen</td>\n",
       "      <td>Todd Burke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>381</td>\n",
       "      <td>&lt;21041312.1075855725847.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>2001-03-15 14:11:00</td>\n",
       "      <td>frozenset({'phillip.allen@enron.com'})</td>\n",
       "      <td>frozenset({'kim.bolton@enron.com'})</td>\n",
       "      <td>RE: PERSONAL AND CONFIDENTIAL COMPENSATION INF...</td>\n",
       "      <td>Phillip K Allen</td>\n",
       "      <td>Kim Bolton</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>2139</td>\n",
       "      <td>&lt;5907100.1075858639941.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>2001-06-20 17:04:51</td>\n",
       "      <td>frozenset({'k..allen@enron.com'})</td>\n",
       "      <td>frozenset({'matt.smith@enron.com', 'matthew.le...</td>\n",
       "      <td>FW: Western Wholesale Activities - Gas &amp; Power...</td>\n",
       "      <td>Allen, Phillip K. &lt;/O=ENRON/OU=NA/CN=RECIPIENT...</td>\n",
       "      <td>Lenhart, Matthew &lt;/O=ENRON/OU=NA/CN=RECIPIENTS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2140</th>\n",
       "      <td>2140</td>\n",
       "      <td>&lt;26625142.1075858639964.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>2001-06-20 17:09:00</td>\n",
       "      <td>frozenset({'k..allen@enron.com'})</td>\n",
       "      <td>frozenset({'matt.smith@enron.com', 'matthew.le...</td>\n",
       "      <td>FW: Western Wholesale Activities - Gas &amp; Power...</td>\n",
       "      <td>Allen, Phillip K. &lt;/O=ENRON/OU=NA/CN=RECIPIENT...</td>\n",
       "      <td>Lenhart, Matthew &lt;/O=ENRON/OU=NA/CN=RECIPIENTS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>2232</td>\n",
       "      <td>&lt;19730598.1075858642129.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>2001-08-09 12:30:58</td>\n",
       "      <td>frozenset({'k..allen@enron.com'})</td>\n",
       "      <td>frozenset({'matt.smith@enron.com', 'm..tholt@e...</td>\n",
       "      <td>FW: Western Wholesale Activities - Gas &amp; Power...</td>\n",
       "      <td>Allen, Phillip K. &lt;/O=ENRON/OU=NA/CN=RECIPIENT...</td>\n",
       "      <td>Smith, Matt &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                     Message-ID  \\\n",
       "379          379   <9831685.1075855725804.JavaMail.evans@thyme>   \n",
       "381          381  <21041312.1075855725847.JavaMail.evans@thyme>   \n",
       "2139        2139   <5907100.1075858639941.JavaMail.evans@thyme>   \n",
       "2140        2140  <26625142.1075858639964.JavaMail.evans@thyme>   \n",
       "2232        2232  <19730598.1075858642129.JavaMail.evans@thyme>   \n",
       "\n",
       "                     Date                                    From  \\\n",
       "379   2001-03-15 14:45:00  frozenset({'phillip.allen@enron.com'})   \n",
       "381   2001-03-15 14:11:00  frozenset({'phillip.allen@enron.com'})   \n",
       "2139  2001-06-20 17:04:51       frozenset({'k..allen@enron.com'})   \n",
       "2140  2001-06-20 17:09:00       frozenset({'k..allen@enron.com'})   \n",
       "2232  2001-08-09 12:30:58       frozenset({'k..allen@enron.com'})   \n",
       "\n",
       "                                                     To  \\\n",
       "379                 frozenset({'todd.burke@enron.com'})   \n",
       "381                 frozenset({'kim.bolton@enron.com'})   \n",
       "2139  frozenset({'matt.smith@enron.com', 'matthew.le...   \n",
       "2140  frozenset({'matt.smith@enron.com', 'matthew.le...   \n",
       "2232  frozenset({'matt.smith@enron.com', 'm..tholt@e...   \n",
       "\n",
       "                                                Subject  \\\n",
       "379       Re: Confidential Employee Information/Lenhart   \n",
       "381   RE: PERSONAL AND CONFIDENTIAL COMPENSATION INF...   \n",
       "2139  FW: Western Wholesale Activities - Gas & Power...   \n",
       "2140  FW: Western Wholesale Activities - Gas & Power...   \n",
       "2232  FW: Western Wholesale Activities - Gas & Power...   \n",
       "\n",
       "                                                 X-From  \\\n",
       "379                                     Phillip K Allen   \n",
       "381                                     Phillip K Allen   \n",
       "2139  Allen, Phillip K. </O=ENRON/OU=NA/CN=RECIPIENT...   \n",
       "2140  Allen, Phillip K. </O=ENRON/OU=NA/CN=RECIPIENT...   \n",
       "2232  Allen, Phillip K. </O=ENRON/OU=NA/CN=RECIPIENT...   \n",
       "\n",
       "                                                   X-To X-cc X-bcc  ...  \\\n",
       "379                                          Todd Burke  NaN   NaN  ...   \n",
       "381                                          Kim Bolton  NaN   NaN  ...   \n",
       "2139  Lenhart, Matthew </O=ENRON/OU=NA/CN=RECIPIENTS...  NaN   NaN  ...   \n",
       "2140  Lenhart, Matthew </O=ENRON/OU=NA/CN=RECIPIENTS...  NaN   NaN  ...   \n",
       "2232  Smith, Matt </O=ENRON/OU=NA/CN=RECIPIENTS/CN=M...  NaN   NaN  ...   \n",
       "\n",
       "     Cat_10_level_1 Cat_10_level_2 Cat_10_weight Cat_11_level_1  \\\n",
       "379             NaN            NaN           NaN            NaN   \n",
       "381             NaN            NaN           NaN            NaN   \n",
       "2139            NaN            NaN           NaN            NaN   \n",
       "2140            NaN            NaN           NaN            NaN   \n",
       "2232            NaN            NaN           NaN            NaN   \n",
       "\n",
       "     Cat_11_level_2  Cat_11_weight  Cat_12_level_1  Cat_12_level_2  \\\n",
       "379             NaN            NaN             NaN             NaN   \n",
       "381             NaN            NaN             NaN             NaN   \n",
       "2139            NaN            NaN             NaN             NaN   \n",
       "2140            NaN            NaN             NaN             NaN   \n",
       "2232            NaN            NaN             NaN             NaN   \n",
       "\n",
       "      Cat_12_weight  labeled  \n",
       "379             NaN     True  \n",
       "381             NaN     True  \n",
       "2139            NaN     True  \n",
       "2140            NaN     True  \n",
       "2232            NaN     True  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_emails_df.head()\n",
    "#labeled_emails_df[\"labeled\"].describe()\n",
    "labeled_messages = labeled_emails_df[labeled_emails_df[\"labeled\"]==True]\n",
    "print ('The dataset provides topic labels for ' + str(len(labeled_messages)) + ' emails')\n",
    "#labeled_messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the label information, we will use \"primary topics\", which corresponds to level_1 == 3 and level_2 ranges from 1 to 13. (Full description of the topic labels are available here: https://data.world/brianray/enron-email-dataset). I focus on primary topics because they are relevant to the company's business and strategies (as opposed to personal emails or administrative/editing/etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879 messages are labeled as primary topics\n"
     ]
    }
   ],
   "source": [
    "# Currently, Cat_1 implies that the first category that a human coder identified. Our category of interest, \"3\" can appear any of the 12 columns: Cat_1_level1, Cat_2_level1, ... Cat_12_level1. \n",
    "# So we will extract \"3\"s from the 12 columns. If 3 exists in any of the level 1 columns, we also want to extract the sub-category(i.e., level 2 category) from from the associated level_2 column.\n",
    "\n",
    "labeled_messages_red = labeled_messages[(labeled_messages[\"Cat_1_level_1\"].fillna(0.0).astype(int) == 3) | \n",
    "                        (labeled_messages[\"Cat_2_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_3_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_4_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_5_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_6_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_7_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_8_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_9_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_10_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_11_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_12_level_1\"].fillna(0.0).astype(int) == 3) \n",
    "                       ]\n",
    "print (str(len(labeled_messages_red)) + ' messages are labeled as primary topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cat_columns = [\"Cat_1_level_1\", \"Cat_1_level_2\", \n",
    "              \"Cat_2_level_1\", \"Cat_2_level_2\",\n",
    "              \"Cat_3_level_1\", \"Cat_3_level_2\", \n",
    "              \"Cat_4_level_1\", \"Cat_4_level_2\", \n",
    "              \"Cat_5_level_1\", \"Cat_5_level_2\", \n",
    "              \"Cat_6_level_1\", \"Cat_6_level_2\", \n",
    "              \"Cat_7_level_1\", \"Cat_7_level_2\", \n",
    "              \"Cat_8_level_1\", \"Cat_8_level_2\", \n",
    "              \"Cat_9_level_1\", \"Cat_9_level_2\", \n",
    "              \"Cat_10_level_1\", \"Cat_10_level_2\", \n",
    "              \"Cat_11_level_1\", \"Cat_11_level_2\", \n",
    "              \"Cat_12_level_1\", \"Cat_12_level_2\", \n",
    "              ]\n",
    "#cat_columns\n",
    "labeled_messages_red[cat_columns] = labeled_messages_red[cat_columns].fillna(value=0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict_dt = pd.DataFrame.from_dict(label_dict, orient = 'index', columns = [\"topic\", \"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now merge these labels to IDF for each topic.... \n",
    "To do so, unlist the topic category info... -- if topic column contains 3.1.\n",
    "--> then, 3.1 bring all the \"content\" and count the occurance of dict = 1 \n",
    "--> make a list of that count, and then attach it as a column \"count_topic_3_1\"  <##### I will be able to use this code in BERT? -- In any of the doc... do you see this vocab?\n",
    "\n",
    "and so on.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;10469240.1075863429356.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.2]</td>\n",
       "      <td>Greetings from London. What do you think about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;14585290.1075842999386.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.6]</td>\n",
       "      <td>***Sent on behalf of Sandi Thompson*** To All ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;21785136.1075846160406.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.4]</td>\n",
       "      <td>Thanks for the update. Congratulations on your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;14717550.1075846177238.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.4]</td>\n",
       "      <td>Please post the JP MOrgan doc. on our site ---...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;5369418.1075846152944.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.5]</td>\n",
       "      <td>Let's process this request. I think it's justi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;16848822.1075853125247.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.7]</td>\n",
       "      <td>David, You asked me to provide my opinion abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;6871897.1075858732063.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.1]</td>\n",
       "      <td>The EPSA leg. affairs cmt. met today during th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;20011465.1075847624589.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.5]</td>\n",
       "      <td>yes Linda Robertson 03/01/2001 07:47 AM To: St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;8348919.1075844026871.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.2, 3.8]</td>\n",
       "      <td>I've looked into whether we can terminate our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;33228374.1075851641742.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.1]</td>\n",
       "      <td>Another agenda item I got a call today from Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;17418001.1075847609913.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.1, 3.2, 3.5, 3.11]</td>\n",
       "      <td>To add to what Rick said, I would add that eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;14294698.1075846173741.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.1]</td>\n",
       "      <td>Restructuring Today quotes Jeff at length on r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;3882894.1075858706011.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.5, 3.9]</td>\n",
       "      <td>Want to pass on some info on India from a conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;11991339.1075842536086.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.6]</td>\n",
       "      <td>In anticipation of potential litigation involv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;375704.1075853198897.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.10]</td>\n",
       "      <td>-----Original Message----- From: Peebles, Lesl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;31848366.1075848230713.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.1, 3.6]</td>\n",
       "      <td>Please see the following articles: Sac Bee, We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;32327990.1075846161080.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.1]</td>\n",
       "      <td>---------------------- Forwarded by Steven J K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;9142227.1075843395436.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.1]</td>\n",
       "      <td>I think we need some clear indication that wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;372271.1075849301664.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.3, 3.6]</td>\n",
       "      <td>fyi ---------------------- Forwarded by Steven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;16986499.1075846180917.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.6, 3.9]</td>\n",
       "      <td>sounds good. I think we should not talk about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;412689.1075849875311.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.3]</td>\n",
       "      <td>Can one of you forward to Ed, or whoever you f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;13536979.1075842977296.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.3, 3.6]</td>\n",
       "      <td>CalPX Told To Release Confidential Trade Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;12458724.1075849864419.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.1]</td>\n",
       "      <td>Congrats Linda Robertson 07/19/2001 09:44 AM T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;23065188.1075846670968.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.10]</td>\n",
       "      <td>Elizabeth, Jeff forwarded me your questions ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;7671697.1075843395462.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.1]</td>\n",
       "      <td>----- Forwarded by Steven J Kean/NA/Enron on 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;3932628.1075846142843.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.2, 3.8]</td>\n",
       "      <td>Mary- Ken has been asked by Mayor Brown to cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;15337492.1075862231823.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.5]</td>\n",
       "      <td>Linda requested some background and talking po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;32536713.1075846173978.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.4]</td>\n",
       "      <td>calendar ---------------------- Forwarded by S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;12883710.1075858707258.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.5]</td>\n",
       "      <td>See e-mail exchange between Andy Black of Bart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;4131316.1075840896739.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.12]</td>\n",
       "      <td>Louise -- LIsa Yoho suggested I forward the at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;16318326.1075847572476.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.3]</td>\n",
       "      <td>I think it would be a good idea to get togethe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;346839.1075846171047.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.5]</td>\n",
       "      <td>Rosie -- please check Ken's calendar. This wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;22094025.1075842958662.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.1, 3.6]</td>\n",
       "      <td>When we have described the problems and soluti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;29468798.1075846168582.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.8]</td>\n",
       "      <td>In my interview I stressed that a big part of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;25968812.1075862231799.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.1]</td>\n",
       "      <td>Late yesterday afternoon, EPSA's legislative a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;3604966.1075848250156.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.3]</td>\n",
       "      <td>See natural gas price article below. Our buddy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;4164904.1075860358466.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.6]</td>\n",
       "      <td>Please see the following articles: Bay City Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;9977719.1075863426814.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.8]</td>\n",
       "      <td>Marilyn Yes, please call Shirley (3-5290) to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;2696973.1075846176739.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.1, 3.5]</td>\n",
       "      <td>yes Mark Palmer@ENRON 09/26/2000 08:51 AM To: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;12891612.1075853204313.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.2, 3.9]</td>\n",
       "      <td>I got your copy kicked back to me for some rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;22915457.1075852472836.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.12]</td>\n",
       "      <td>In our litigation meeting last Tuesday, we lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;18946631.1075849303512.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.6]</td>\n",
       "      <td>I know we are holding for a later filing, but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;20838439.1075846191576.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.1]</td>\n",
       "      <td>Attached are FERC reports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;5296197.1075846175989.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.6]</td>\n",
       "      <td>I understand from Shelley that you are going t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;1414510.1075846143230.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.9]</td>\n",
       "      <td>I agree with Mark's points. While we have advo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;31536214.1075849870907.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.7]</td>\n",
       "      <td>Please add Dave Delainey, John Lavorato and Lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;4618541.1075847627503.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.6]</td>\n",
       "      <td>----- Forwarded by Steven J Kean/NA/Enron on 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;20727074.1075846144600.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.2]</td>\n",
       "      <td>Please respond and copy me. ------------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;2573675.1075843395513.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.2]</td>\n",
       "      <td>The conversation was very positive (Glynn did ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;21698762.1075847618744.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.10]</td>\n",
       "      <td>----- Forwarded by Steven J Kean/NA/Enron on 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;11125397.1075846171861.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.8]</td>\n",
       "      <td>---------------------- Forwarded by Steven J K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;7389738.1075846175169.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.4]</td>\n",
       "      <td>Maureen -- please send Vince my California tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;17421089.1075847622402.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.11]</td>\n",
       "      <td>Here are the message points ----- Forwarded by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;17391493.1075847591366.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.8]</td>\n",
       "      <td>Go ahead. From: Elizabeth Linnell on 04/25/200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;12234255.1075851606483.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.6]</td>\n",
       "      <td>---------------------- Forwarded by John Shelk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;20949592.1075842958684.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.4, 3.6]</td>\n",
       "      <td>Skilling will be speaking at the National Pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;25033143.1075858499361.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.6]</td>\n",
       "      <td>-----Original Message----- From: Comnes, Alan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;23637727.1075847621388.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.8]</td>\n",
       "      <td>----- Forwarded by Steven J Kean/NA/Enron on 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;9385898.1075840855198.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.7]</td>\n",
       "      <td>To:KITCHEN, LOUISE - +1 713 853 3488 Enron Who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;16440916.1075846178432.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>[3.5]</td>\n",
       "      <td>Just a \"heads up\" ... Ken may get a call from ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>879 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               topic  \\\n",
       "<10469240.1075863429356.JavaMail.evans@thyme>                  [3.2]   \n",
       "<14585290.1075842999386.JavaMail.evans@thyme>                  [3.6]   \n",
       "<21785136.1075846160406.JavaMail.evans@thyme>                  [3.4]   \n",
       "<14717550.1075846177238.JavaMail.evans@thyme>                  [3.4]   \n",
       "<5369418.1075846152944.JavaMail.evans@thyme>                   [3.5]   \n",
       "<16848822.1075853125247.JavaMail.evans@thyme>                  [3.7]   \n",
       "<6871897.1075858732063.JavaMail.evans@thyme>                   [3.1]   \n",
       "<20011465.1075847624589.JavaMail.evans@thyme>                  [3.5]   \n",
       "<8348919.1075844026871.JavaMail.evans@thyme>              [3.2, 3.8]   \n",
       "<33228374.1075851641742.JavaMail.evans@thyme>                  [3.1]   \n",
       "<17418001.1075847609913.JavaMail.evans@thyme>  [3.1, 3.2, 3.5, 3.11]   \n",
       "<14294698.1075846173741.JavaMail.evans@thyme>                  [3.1]   \n",
       "<3882894.1075858706011.JavaMail.evans@thyme>              [3.5, 3.9]   \n",
       "<11991339.1075842536086.JavaMail.evans@thyme>                  [3.6]   \n",
       "<375704.1075853198897.JavaMail.evans@thyme>                   [3.10]   \n",
       "<31848366.1075848230713.JavaMail.evans@thyme>             [3.1, 3.6]   \n",
       "<32327990.1075846161080.JavaMail.evans@thyme>                  [3.1]   \n",
       "<9142227.1075843395436.JavaMail.evans@thyme>                   [3.1]   \n",
       "<372271.1075849301664.JavaMail.evans@thyme>               [3.3, 3.6]   \n",
       "<16986499.1075846180917.JavaMail.evans@thyme>             [3.6, 3.9]   \n",
       "<412689.1075849875311.JavaMail.evans@thyme>                    [3.3]   \n",
       "<13536979.1075842977296.JavaMail.evans@thyme>             [3.3, 3.6]   \n",
       "<12458724.1075849864419.JavaMail.evans@thyme>                  [3.1]   \n",
       "<23065188.1075846670968.JavaMail.evans@thyme>                 [3.10]   \n",
       "<7671697.1075843395462.JavaMail.evans@thyme>                   [3.1]   \n",
       "<3932628.1075846142843.JavaMail.evans@thyme>              [3.2, 3.8]   \n",
       "<15337492.1075862231823.JavaMail.evans@thyme>                  [3.5]   \n",
       "<32536713.1075846173978.JavaMail.evans@thyme>                  [3.4]   \n",
       "<12883710.1075858707258.JavaMail.evans@thyme>                  [3.5]   \n",
       "<4131316.1075840896739.JavaMail.evans@thyme>                  [3.12]   \n",
       "...                                                              ...   \n",
       "<16318326.1075847572476.JavaMail.evans@thyme>                  [3.3]   \n",
       "<346839.1075846171047.JavaMail.evans@thyme>                    [3.5]   \n",
       "<22094025.1075842958662.JavaMail.evans@thyme>             [3.1, 3.6]   \n",
       "<29468798.1075846168582.JavaMail.evans@thyme>                  [3.8]   \n",
       "<25968812.1075862231799.JavaMail.evans@thyme>                  [3.1]   \n",
       "<3604966.1075848250156.JavaMail.evans@thyme>                   [3.3]   \n",
       "<4164904.1075860358466.JavaMail.evans@thyme>                   [3.6]   \n",
       "<9977719.1075863426814.JavaMail.evans@thyme>                   [3.8]   \n",
       "<2696973.1075846176739.JavaMail.evans@thyme>              [3.1, 3.5]   \n",
       "<12891612.1075853204313.JavaMail.evans@thyme>             [3.2, 3.9]   \n",
       "<22915457.1075852472836.JavaMail.evans@thyme>                 [3.12]   \n",
       "<18946631.1075849303512.JavaMail.evans@thyme>                  [3.6]   \n",
       "<20838439.1075846191576.JavaMail.evans@thyme>                  [3.1]   \n",
       "<5296197.1075846175989.JavaMail.evans@thyme>                   [3.6]   \n",
       "<1414510.1075846143230.JavaMail.evans@thyme>                   [3.9]   \n",
       "<31536214.1075849870907.JavaMail.evans@thyme>                  [3.7]   \n",
       "<4618541.1075847627503.JavaMail.evans@thyme>                   [3.6]   \n",
       "<20727074.1075846144600.JavaMail.evans@thyme>                  [3.2]   \n",
       "<2573675.1075843395513.JavaMail.evans@thyme>                   [3.2]   \n",
       "<21698762.1075847618744.JavaMail.evans@thyme>                 [3.10]   \n",
       "<11125397.1075846171861.JavaMail.evans@thyme>                  [3.8]   \n",
       "<7389738.1075846175169.JavaMail.evans@thyme>                   [3.4]   \n",
       "<17421089.1075847622402.JavaMail.evans@thyme>                 [3.11]   \n",
       "<17391493.1075847591366.JavaMail.evans@thyme>                  [3.8]   \n",
       "<12234255.1075851606483.JavaMail.evans@thyme>                  [3.6]   \n",
       "<20949592.1075842958684.JavaMail.evans@thyme>             [3.4, 3.6]   \n",
       "<25033143.1075858499361.JavaMail.evans@thyme>                  [3.6]   \n",
       "<23637727.1075847621388.JavaMail.evans@thyme>                  [3.8]   \n",
       "<9385898.1075840855198.JavaMail.evans@thyme>                   [3.7]   \n",
       "<16440916.1075846178432.JavaMail.evans@thyme>                  [3.5]   \n",
       "\n",
       "                                                                                         content  \n",
       "<10469240.1075863429356.JavaMail.evans@thyme>  Greetings from London. What do you think about...  \n",
       "<14585290.1075842999386.JavaMail.evans@thyme>  ***Sent on behalf of Sandi Thompson*** To All ...  \n",
       "<21785136.1075846160406.JavaMail.evans@thyme>  Thanks for the update. Congratulations on your...  \n",
       "<14717550.1075846177238.JavaMail.evans@thyme>  Please post the JP MOrgan doc. on our site ---...  \n",
       "<5369418.1075846152944.JavaMail.evans@thyme>   Let's process this request. I think it's justi...  \n",
       "<16848822.1075853125247.JavaMail.evans@thyme>  David, You asked me to provide my opinion abou...  \n",
       "<6871897.1075858732063.JavaMail.evans@thyme>   The EPSA leg. affairs cmt. met today during th...  \n",
       "<20011465.1075847624589.JavaMail.evans@thyme>  yes Linda Robertson 03/01/2001 07:47 AM To: St...  \n",
       "<8348919.1075844026871.JavaMail.evans@thyme>   I've looked into whether we can terminate our ...  \n",
       "<33228374.1075851641742.JavaMail.evans@thyme>  Another agenda item I got a call today from Bo...  \n",
       "<17418001.1075847609913.JavaMail.evans@thyme>  To add to what Rick said, I would add that eff...  \n",
       "<14294698.1075846173741.JavaMail.evans@thyme>  Restructuring Today quotes Jeff at length on r...  \n",
       "<3882894.1075858706011.JavaMail.evans@thyme>   Want to pass on some info on India from a conv...  \n",
       "<11991339.1075842536086.JavaMail.evans@thyme>  In anticipation of potential litigation involv...  \n",
       "<375704.1075853198897.JavaMail.evans@thyme>    -----Original Message----- From: Peebles, Lesl...  \n",
       "<31848366.1075848230713.JavaMail.evans@thyme>  Please see the following articles: Sac Bee, We...  \n",
       "<32327990.1075846161080.JavaMail.evans@thyme>  ---------------------- Forwarded by Steven J K...  \n",
       "<9142227.1075843395436.JavaMail.evans@thyme>   I think we need some clear indication that wit...  \n",
       "<372271.1075849301664.JavaMail.evans@thyme>    fyi ---------------------- Forwarded by Steven...  \n",
       "<16986499.1075846180917.JavaMail.evans@thyme>  sounds good. I think we should not talk about ...  \n",
       "<412689.1075849875311.JavaMail.evans@thyme>    Can one of you forward to Ed, or whoever you f...  \n",
       "<13536979.1075842977296.JavaMail.evans@thyme>  CalPX Told To Release Confidential Trade Data ...  \n",
       "<12458724.1075849864419.JavaMail.evans@thyme>  Congrats Linda Robertson 07/19/2001 09:44 AM T...  \n",
       "<23065188.1075846670968.JavaMail.evans@thyme>  Elizabeth, Jeff forwarded me your questions ab...  \n",
       "<7671697.1075843395462.JavaMail.evans@thyme>   ----- Forwarded by Steven J Kean/NA/Enron on 0...  \n",
       "<3932628.1075846142843.JavaMail.evans@thyme>   Mary- Ken has been asked by Mayor Brown to cha...  \n",
       "<15337492.1075862231823.JavaMail.evans@thyme>  Linda requested some background and talking po...  \n",
       "<32536713.1075846173978.JavaMail.evans@thyme>  calendar ---------------------- Forwarded by S...  \n",
       "<12883710.1075858707258.JavaMail.evans@thyme>  See e-mail exchange between Andy Black of Bart...  \n",
       "<4131316.1075840896739.JavaMail.evans@thyme>   Louise -- LIsa Yoho suggested I forward the at...  \n",
       "...                                                                                          ...  \n",
       "<16318326.1075847572476.JavaMail.evans@thyme>  I think it would be a good idea to get togethe...  \n",
       "<346839.1075846171047.JavaMail.evans@thyme>    Rosie -- please check Ken's calendar. This wou...  \n",
       "<22094025.1075842958662.JavaMail.evans@thyme>  When we have described the problems and soluti...  \n",
       "<29468798.1075846168582.JavaMail.evans@thyme>  In my interview I stressed that a big part of ...  \n",
       "<25968812.1075862231799.JavaMail.evans@thyme>  Late yesterday afternoon, EPSA's legislative a...  \n",
       "<3604966.1075848250156.JavaMail.evans@thyme>   See natural gas price article below. Our buddy...  \n",
       "<4164904.1075860358466.JavaMail.evans@thyme>   Please see the following articles: Bay City Ne...  \n",
       "<9977719.1075863426814.JavaMail.evans@thyme>   Marilyn Yes, please call Shirley (3-5290) to s...  \n",
       "<2696973.1075846176739.JavaMail.evans@thyme>   yes Mark Palmer@ENRON 09/26/2000 08:51 AM To: ...  \n",
       "<12891612.1075853204313.JavaMail.evans@thyme>  I got your copy kicked back to me for some rea...  \n",
       "<22915457.1075852472836.JavaMail.evans@thyme>  In our litigation meeting last Tuesday, we lea...  \n",
       "<18946631.1075849303512.JavaMail.evans@thyme>  I know we are holding for a later filing, but ...  \n",
       "<20838439.1075846191576.JavaMail.evans@thyme>                          Attached are FERC reports  \n",
       "<5296197.1075846175989.JavaMail.evans@thyme>   I understand from Shelley that you are going t...  \n",
       "<1414510.1075846143230.JavaMail.evans@thyme>   I agree with Mark's points. While we have advo...  \n",
       "<31536214.1075849870907.JavaMail.evans@thyme>  Please add Dave Delainey, John Lavorato and Lo...  \n",
       "<4618541.1075847627503.JavaMail.evans@thyme>   ----- Forwarded by Steven J Kean/NA/Enron on 0...  \n",
       "<20727074.1075846144600.JavaMail.evans@thyme>  Please respond and copy me. ------------------...  \n",
       "<2573675.1075843395513.JavaMail.evans@thyme>   The conversation was very positive (Glynn did ...  \n",
       "<21698762.1075847618744.JavaMail.evans@thyme>  ----- Forwarded by Steven J Kean/NA/Enron on 0...  \n",
       "<11125397.1075846171861.JavaMail.evans@thyme>  ---------------------- Forwarded by Steven J K...  \n",
       "<7389738.1075846175169.JavaMail.evans@thyme>   Maureen -- please send Vince my California tes...  \n",
       "<17421089.1075847622402.JavaMail.evans@thyme>  Here are the message points ----- Forwarded by...  \n",
       "<17391493.1075847591366.JavaMail.evans@thyme>  Go ahead. From: Elizabeth Linnell on 04/25/200...  \n",
       "<12234255.1075851606483.JavaMail.evans@thyme>  ---------------------- Forwarded by John Shelk...  \n",
       "<20949592.1075842958684.JavaMail.evans@thyme>  Skilling will be speaking at the National Pres...  \n",
       "<25033143.1075858499361.JavaMail.evans@thyme>  -----Original Message----- From: Comnes, Alan ...  \n",
       "<23637727.1075847621388.JavaMail.evans@thyme>  ----- Forwarded by Steven J Kean/NA/Enron on 0...  \n",
       "<9385898.1075840855198.JavaMail.evans@thyme>   To:KITCHEN, LOUISE - +1 713 853 3488 Enron Who...  \n",
       "<16440916.1075846178432.JavaMail.evans@thyme>  Just a \"heads up\" ... Ken may get a call from ...  \n",
       "\n",
       "[879 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_3_1_list = []\n",
    "topic_3_2_list = []\n",
    "topic_3_3_list = []\n",
    "topic_3_4_list = []\n",
    "topic_3_5_list = []\n",
    "topic_3_6_list = []\n",
    "topic_3_7_list = []\n",
    "topic_3_8_list = []\n",
    "topic_3_9_list = []\n",
    "topic_3_10_list = []\n",
    "topic_3_11_list = []\n",
    "topic_3_12_list = []\n",
    "\n",
    "for i in range(len(label_dict_dt)): \n",
    "    if '3.1' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_1_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.2' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_2_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.3' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_3_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.4' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_4_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.5' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_5_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.6' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_6_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.7' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_7_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.8' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_8_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.9' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_9_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.10' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_10_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.11' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_11_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.12' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_12_list.append(label_dict_dt.iloc[i]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For words in our key word dictionary, let's count the occurance in eachof the topic document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_3_1_str = ' '.join(topic_3_1_list).lower()\n",
    "topic_3_2_str = ' '.join(topic_3_2_list).lower()\n",
    "topic_3_3_str = ' '.join(topic_3_3_list).lower()\n",
    "topic_3_4_str = ' '.join(topic_3_4_list).lower()\n",
    "topic_3_5_str = ' '.join(topic_3_5_list).lower()\n",
    "topic_3_6_str = ' '.join(topic_3_6_list).lower()\n",
    "topic_3_7_str = ' '.join(topic_3_7_list).lower()\n",
    "topic_3_8_str = ' '.join(topic_3_8_list).lower()\n",
    "topic_3_9_str = ' '.join(topic_3_9_list).lower()\n",
    "topic_3_10_str = ' '.join(topic_3_10_list).lower()\n",
    "topic_3_11_str = ' '.join(topic_3_11_list).lower()\n",
    "topic_3_12_str = ' '.join(topic_3_12_list).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_bin_3_1 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_1_str: \n",
    "        topic_bin_3_1.append(1)\n",
    "    else: \n",
    "        topic_bin_3_1.append(0)\n",
    "\n",
    "        \n",
    "topic_bin_3_2 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_2_str: \n",
    "        topic_bin_3_2.append(1)\n",
    "    else: \n",
    "        topic_bin_3_2.append(0)\n",
    "\n",
    "\n",
    "topic_bin_3_3 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_3_str: \n",
    "        topic_bin_3_3.append(1)\n",
    "    else: \n",
    "        topic_bin_3_3.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_4 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_4_str: \n",
    "        topic_bin_3_4.append(1)\n",
    "    else: \n",
    "        topic_bin_3_4.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_5 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_5_str: \n",
    "        topic_bin_3_5.append(1)\n",
    "    else: \n",
    "        topic_bin_3_5.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_6 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_6_str: \n",
    "        topic_bin_3_6.append(1)\n",
    "    else: \n",
    "        topic_bin_3_6.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_7 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_7_str: \n",
    "        topic_bin_3_7.append(1)\n",
    "    else: \n",
    "        topic_bin_3_7.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_8 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_8_str: \n",
    "        topic_bin_3_8.append(1)\n",
    "    else: \n",
    "        topic_bin_3_8.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_9 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_9_str: \n",
    "        topic_bin_3_9.append(1)\n",
    "    else: \n",
    "        topic_bin_3_9.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_10 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_10_str: \n",
    "        topic_bin_3_10.append(1)\n",
    "    else: \n",
    "        topic_bin_3_10.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_11 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_11_str: \n",
    "        topic_bin_3_11.append(1)\n",
    "    else: \n",
    "        topic_bin_3_11.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_12 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_12_str: \n",
    "        topic_bin_3_12.append(1)\n",
    "    else: \n",
    "        topic_bin_3_12.append(0)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2571\n",
      "1521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(topic_bin_3_11))\n",
    "print(sum(topic_bin_3_11))\n",
    "type(topic_bin_3_1)\n",
    "#keywords_dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Margeum\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Margeum\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Margeum\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Margeum\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Margeum\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\Margeum\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Margeum\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "C:\\Users\\Margeum\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Margeum\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Margeum\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Margeum\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Margeum\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "keywords_dictionary['topic_bin_3_1']= topic_bin_3_1\n",
    "keywords_dictionary['topic_bin_3_2']= topic_bin_3_2\n",
    "keywords_dictionary['topic_bin_3_3']= topic_bin_3_3\n",
    "keywords_dictionary['topic_bin_3_4']= topic_bin_3_4\n",
    "keywords_dictionary['topic_bin_3_5']= topic_bin_3_5\n",
    "keywords_dictionary['topic_bin_3_6']= topic_bin_3_6\n",
    "keywords_dictionary['topic_bin_3_7']= topic_bin_3_7\n",
    "keywords_dictionary['topic_bin_3_8']= topic_bin_3_8\n",
    "keywords_dictionary['topic_bin_3_9']= topic_bin_3_9\n",
    "keywords_dictionary['topic_bin_3_10']= topic_bin_3_10\n",
    "keywords_dictionary['topic_bin_3_11']= topic_bin_3_11\n",
    "keywords_dictionary['topic_bin_3_12']= topic_bin_3_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Margeum\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    2571.000000\n",
       "mean       10.508751\n",
       "std         2.021341\n",
       "min         0.000000\n",
       "25%        10.000000\n",
       "50%        11.000000\n",
       "75%        12.000000\n",
       "max        12.000000\n",
       "Name: count_topics, dtype: float64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_cat_list = ['topic_bin_3_1', 'topic_bin_3_2', 'topic_bin_3_3', \n",
    "                 'topic_bin_3_4', 'topic_bin_3_5', 'topic_bin_3_6',\n",
    "                 'topic_bin_3_7', 'topic_bin_3_8', 'topic_bin_3_9', \n",
    "                 'topic_bin_3_10', 'topic_bin_3_11', 'topic_bin_3_12']\n",
    "\n",
    "keywords_dictionary['count_topics'] = keywords_dictionary[topic_cat_list].sum(axis=1)\n",
    "keywords_dictionary['count_topics'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among 2571 words in the keyword dictionary, 1168 appear in all of the primary category\n"
     ]
    }
   ],
   "source": [
    "print ('Among ' + str(len(keywords_dictionary)) +\n",
    "       ' words in the keyword dictionary, ' + \n",
    "       str(len(keywords_dictionary[keywords_dictionary['count_topics'] == 12])) +\n",
    "       ' appear in all of the primary category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep those 1168 words as our final keywords of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(lower,)</th>\n",
       "      <td>20720</td>\n",
       "      <td>0.002863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(member,)</th>\n",
       "      <td>62784</td>\n",
       "      <td>0.009659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(taken,)</th>\n",
       "      <td>16616</td>\n",
       "      <td>0.002193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(summer,)</th>\n",
       "      <td>35574</td>\n",
       "      <td>0.005372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(act,)</th>\n",
       "      <td>21014</td>\n",
       "      <td>0.002895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Frequency      mean\n",
       "Term                          \n",
       "(lower,)       20720  0.002863\n",
       "(member,)      62784  0.009659\n",
       "(taken,)       16616  0.002193\n",
       "(summer,)      35574  0.005372\n",
       "(act,)         21014  0.002895"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_keywords_dictionary = keywords_dictionary[keywords_dictionary['count_topics'] == 12][['Frequency', 'mean']]\n",
    "final_keywords_dictionary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Store the key word dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_keywords_dictionary.to_csv('final_keywords_dictionary.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
