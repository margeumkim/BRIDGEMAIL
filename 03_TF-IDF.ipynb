{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding most-widely used, multi-contextual words\n",
    "\n",
    "\n",
    "To build the third function, \"Context Detector\", I will extract the word-sense associatoin from Bert model. \n",
    "But I certainly do not want to serach for contextual meaning for every possible word that a user can possibly use. Therefore, in this notebook, I will come up with a dictionary of words that Enron employees commonly use and potentially in different contexts. To do so, I will use email topic labels, which were hand-coded by CMU students (available from https://data.world/brianray/enron-email-dataset)\n",
    "\n",
    "\n",
    "* 1) Load libraries and define functions\n",
    "* 2) Import data: email data and labeld data \n",
    "* 3) Calculate TF-IDF scores for each words in the company-wide email corpus by selecting words that occur frequently, across many people's \n",
    "* 4) Using the topic labels data, calculate topic-level term frequency\n",
    "    * Join the labeled data with TF-IDF results\n",
    "* 5) Build and store a dictionary of words that widely-used (according to TF-IDF scores) and multi-contextual words (that appear across all categories, quiet frequently).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading libraries and defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from subprocess import check_output\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.util import ngrams \n",
    "from nltk.probability import FreqDist\n",
    "import os, re, nltk, string\n",
    "\n",
    "def email_clean(text):\n",
    "    text = re.sub(r'\\n--.*?\\n', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'enron.com', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'Forwarded by.*?Subject:', '', text, flags=re.DOTALL) \n",
    "    text = re.sub(r'Fwd:.*?Subject:', '', text, flags=re.DOTALL) \n",
    "    text = re.sub(r'Fw:.*?Subject:', '', text, flags=re.DOTALL)     \n",
    "    text = re.sub(r'FW:.*?Subject:', '', text, flags=re.DOTALL)         \n",
    "    text = re.sub(r'Forwarded:.*?Subject:', '', text, flags=re.DOTALL)         \n",
    "    text = re.sub(r'From:.*?Subject:', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'PM', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'AM', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\",\n",
    "                 \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \n",
    "                 \"enron america corp\", \"enron\", \"etc\", \"na\"))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    porter= PorterStemmer()\n",
    "    \n",
    "    text=text.rstrip()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()))])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    #stem = \" \".join(porter.stem(token) for token in normalized.split())\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_email_data = 'C:/Users/Margeum/Dropbox/DS projects/05. Email data/emails_in_csv'\n",
    "os.chdir(path_to_email_data)\n",
    "emails_df = pd.read_csv('emails_parsed.csv')\n",
    "labeled_emails_df = pd.read_csv(\"enron_05_17_2015_with_labels_v2.csv\") # Labeled data \n",
    "address_user_df = pd.read_csv('./address_user_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculating TF-IDF scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate word frequencies for the email corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_list = []\n",
    "\n",
    "for index, row in address_user_df.iterrows():\n",
    "    \n",
    "    text_cleaned_i = []\n",
    "\n",
    "    user_i = row['user']\n",
    "    print user_i\n",
    "    \n",
    "    lastname = row['address'].split('@')[0].split('.')[-1]\n",
    "    firstname = str(row['address'].split('.')[0])\n",
    "\n",
    "    text_to_clean_df_i = emails_df[emails_df[\"user\"] == user_i][[\"content\", \"user\"]].reset_index()\n",
    "        \n",
    "    for text in text_to_clean_df_i['content']:\n",
    "        text_cleaned_i.append(clean(email_clean(text)).split())\n",
    "\n",
    "    unlisted_text_cleaned_i = [item for sublist in text_cleaned_i for item in sublist]\n",
    "    freqdist_user_i = nltk.FreqDist(ngrams(unlisted_text_cleaned_i, 1))\n",
    "\n",
    "    tf_list.append(freqdist_user_i)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating word frequencies at the user-account level (Documet-level frequencies where all emails in a user's account are treated as a document) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_text_cleaned_i = []\n",
    "\n",
    "for text in emails_df[\"content\"]:\n",
    "    idf_text_cleaned_i.append(clean(email_clean(text)).split())\n",
    "\n",
    "idf_unlisted_text_cleaned_i = [item for sublist in idf_text_cleaned_i for item in sublist]\n",
    "idf_list = nltk.FreqDist(ngrams(idf_unlisted_text_cleaned_i, 1))\n",
    "\n",
    "df_idf = pd.DataFrame.from_dict(idf_list, orient='index')\n",
    "df_idf.columns = ['Frequency']\n",
    "df_idf.index.name = 'Term'\n",
    "df_idf.sort_values(by = \"Frequency\", ascending = False)\n",
    "#df_idf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge the corpus-level frequencies and account-level frequencies by using word as key. Then, calculate the TF-IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf_idf_stacked = df_idf\n",
    "\n",
    "for i in range(len(address_user_df)):\n",
    "    my_td = pd.DataFrame.from_dict(tf_list[i], orient='index')\n",
    "    my_td.columns = ['Frequency']\n",
    "    my_td.index.name = 'Term'\n",
    "\n",
    "    tf_idf_i = my_td.reset_index().set_index('Term').join(df_idf.reset_index().set_index('Term'), on= 'Term', how='left',\n",
    "                                              lsuffix='_left', rsuffix='_right')\n",
    "\n",
    "    tf_idf_i[\"idf\"] =  np.log(tf_idf_i['Frequency_right'])/(len(df_idf)+1)\n",
    "    tf_idf_i[\"tf_idf\"] = tf_idf_i['Frequency_left']*tf_idf_i['idf']\n",
    "    tf_idf_i.sort_values(by = \"tf_idf\", ascending = False)\n",
    "\n",
    "    tf_idf_i = pd.DataFrame(tf_idf_i[\"tf_idf\"])\n",
    "    tf_idf_i.columns = tf_idf_i.columns + \"_\" + str(i)\n",
    "\n",
    "    tf_idf_stacked = tf_idf_stacked.join(tf_idf_i, on = 'Term', how ='left', lsuffix='_left', rsuffix  ='right')\n",
    "\n",
    "\n",
    "col_list= list(tf_idf_stacked)\n",
    "col_list.remove('Frequency')\n",
    "\n",
    "\n",
    "tf_idf_stacked['mean'] = tf_idf_stacked[col_list].mean(axis=1)\n",
    "tf_idf_stacked['variance'] = tf_idf_stacked[col_list].var(axis=1)\n",
    "tf_idf_stacked['count_notnull'] = tf_idf_stacked[col_list].count(axis=1)\n",
    "tf_idf_stacked.sort_values(by = \"mean\", ascending = False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522762 unique words are in the email corpus\n"
     ]
    }
   ],
   "source": [
    "print (str(len(tf_idf_stacked[\"mean\"])) + ' unique words are in the email corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to cut the words from this complete list. First, we will keep only top 1 percent words in terms of mean TF-IDF score. In other words, we will keep words with high importance in terms of their frequency across different accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5156 unique words are in this list of top 1 percent words)\n",
      "Top 1 percent words account for 0.798830791283 unique words are in this list of top 1 percent words)% in terms of frequency\n"
     ]
    }
   ],
   "source": [
    "mean_val_cut_99 = tf_idf_stacked[\"mean\"].quantile(.99)\n",
    "df_mean_val_cut_99 = tf_idf_stacked[tf_idf_stacked['mean'] > mean_val_cut_99]\n",
    "\n",
    "print (str(len(df_mean_val_cut_99)) + ' unique words are in this list of top 1 percent words)')\n",
    "print ('Top 1 percent words account for ' + \n",
    "       str(float(df_mean_val_cut_99[\"Frequency\"].sum())/float(tf_idf_stacked[\"Frequency\"].sum())) +\n",
    "       ' unique words are in this list of top 1 percent words)' + '% in terms of frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>tf_idf_0</th>\n",
       "      <th>tf_idf_1</th>\n",
       "      <th>tf_idf_2</th>\n",
       "      <th>tf_idf_3</th>\n",
       "      <th>tf_idf_4</th>\n",
       "      <th>tf_idf_5</th>\n",
       "      <th>tf_idf_6</th>\n",
       "      <th>tf_idf_7</th>\n",
       "      <th>tf_idf_8</th>\n",
       "      <th>...</th>\n",
       "      <th>tf_idf_130</th>\n",
       "      <th>tf_idf_131</th>\n",
       "      <th>tf_idf_132</th>\n",
       "      <th>tf_idf_133</th>\n",
       "      <th>tf_idf_134</th>\n",
       "      <th>tf_idf_135</th>\n",
       "      <th>tf_idf_136</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "      <th>count_notnull</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(com,)</th>\n",
       "      <td>664513</td>\n",
       "      <td>0.064602</td>\n",
       "      <td>0.136463</td>\n",
       "      <td>0.035238</td>\n",
       "      <td>0.040803</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.536490</td>\n",
       "      <td>0.230609</td>\n",
       "      <td>0.045009</td>\n",
       "      <td>0.041880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064295</td>\n",
       "      <td>0.027775</td>\n",
       "      <td>0.041008</td>\n",
       "      <td>0.075271</td>\n",
       "      <td>0.031134</td>\n",
       "      <td>0.030083</td>\n",
       "      <td>0.006822</td>\n",
       "      <td>0.121391</td>\n",
       "      <td>0.054427</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(ect,)</th>\n",
       "      <td>595113</td>\n",
       "      <td>0.051099</td>\n",
       "      <td>0.114941</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.283041</td>\n",
       "      <td>0.022942</td>\n",
       "      <td>0.842460</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013989</td>\n",
       "      <td>0.049141</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.007758</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109993</td>\n",
       "      <td>0.067947</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(please,)</th>\n",
       "      <td>379986</td>\n",
       "      <td>0.042960</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>0.014132</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.009904</td>\n",
       "      <td>0.093490</td>\n",
       "      <td>0.057190</td>\n",
       "      <td>0.243459</td>\n",
       "      <td>0.049375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021529</td>\n",
       "      <td>0.045836</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.029836</td>\n",
       "      <td>0.028927</td>\n",
       "      <td>0.022144</td>\n",
       "      <td>0.009462</td>\n",
       "      <td>0.065944</td>\n",
       "      <td>0.010998</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(e,)</th>\n",
       "      <td>378364</td>\n",
       "      <td>0.030563</td>\n",
       "      <td>0.067662</td>\n",
       "      <td>0.011867</td>\n",
       "      <td>0.027738</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.136676</td>\n",
       "      <td>0.055550</td>\n",
       "      <td>0.097857</td>\n",
       "      <td>0.021375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029532</td>\n",
       "      <td>0.018377</td>\n",
       "      <td>0.008624</td>\n",
       "      <td>0.027591</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>0.012677</td>\n",
       "      <td>0.006756</td>\n",
       "      <td>0.065836</td>\n",
       "      <td>0.022772</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(would,)</th>\n",
       "      <td>325382</td>\n",
       "      <td>0.035473</td>\n",
       "      <td>0.071772</td>\n",
       "      <td>0.008231</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.080464</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.194751</td>\n",
       "      <td>0.042077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030666</td>\n",
       "      <td>0.018186</td>\n",
       "      <td>0.010902</td>\n",
       "      <td>0.008887</td>\n",
       "      <td>0.012868</td>\n",
       "      <td>0.019618</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.056304</td>\n",
       "      <td>0.018686</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(power,)</th>\n",
       "      <td>309093</td>\n",
       "      <td>0.019926</td>\n",
       "      <td>0.049984</td>\n",
       "      <td>0.005755</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>0.010519</td>\n",
       "      <td>0.041472</td>\n",
       "      <td>0.068072</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025996</td>\n",
       "      <td>0.043382</td>\n",
       "      <td>0.009769</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>0.009262</td>\n",
       "      <td>0.015549</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>0.053380</td>\n",
       "      <td>0.047698</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(hou,)</th>\n",
       "      <td>273226</td>\n",
       "      <td>0.024497</td>\n",
       "      <td>0.050358</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.157157</td>\n",
       "      <td>0.010776</td>\n",
       "      <td>0.302318</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007567</td>\n",
       "      <td>0.020162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004837</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.052184</td>\n",
       "      <td>0.012847</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(energy,)</th>\n",
       "      <td>298781</td>\n",
       "      <td>0.010515</td>\n",
       "      <td>0.073026</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.021464</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.022019</td>\n",
       "      <td>0.054794</td>\n",
       "      <td>0.020162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031497</td>\n",
       "      <td>0.007524</td>\n",
       "      <td>0.005740</td>\n",
       "      <td>0.016279</td>\n",
       "      <td>0.011624</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.051578</td>\n",
       "      <td>0.030421</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(company,)</th>\n",
       "      <td>288719</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>0.122422</td>\n",
       "      <td>0.005604</td>\n",
       "      <td>0.010342</td>\n",
       "      <td>0.005147</td>\n",
       "      <td>0.015994</td>\n",
       "      <td>0.022055</td>\n",
       "      <td>0.072010</td>\n",
       "      <td>0.030377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084517</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.041657</td>\n",
       "      <td>0.007793</td>\n",
       "      <td>0.007312</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.049604</td>\n",
       "      <td>0.025251</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(venturewire,)</th>\n",
       "      <td>8506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.048956</td>\n",
       "      <td>0.007185</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Frequency  tf_idf_0  tf_idf_1  tf_idf_2  tf_idf_3  tf_idf_4  \\\n",
       "Term                                                                          \n",
       "(com,)             664513  0.064602  0.136463  0.035238  0.040803  0.002539   \n",
       "(ect,)             595113  0.051099  0.114941  0.002162  0.004349  0.000076   \n",
       "(please,)          379986  0.042960  0.069430  0.014132  0.014648  0.009904   \n",
       "(e,)               378364  0.030563  0.067662  0.011867  0.027738  0.002580   \n",
       "(would,)           325382  0.035473  0.071772  0.008231  0.011096  0.003156   \n",
       "(power,)           309093  0.019926  0.049984  0.005755  0.024375  0.002442   \n",
       "(hou,)             273226  0.024497  0.050358  0.001461  0.002490       NaN   \n",
       "(energy,)          298781  0.010515  0.073026  0.002870  0.021464  0.006077   \n",
       "(company,)         288719  0.010054  0.122422  0.005604  0.010342  0.005147   \n",
       "(venturewire,)       8506       NaN       NaN  0.000017       NaN       NaN   \n",
       "\n",
       "                tf_idf_5  tf_idf_6  tf_idf_7  tf_idf_8  ...  tf_idf_130  \\\n",
       "Term                                                    ...               \n",
       "(com,)          0.536490  0.230609  0.045009  0.041880  ...    0.064295   \n",
       "(ect,)          0.283041  0.022942  0.842460  0.000153  ...    0.013989   \n",
       "(please,)       0.093490  0.057190  0.243459  0.049375  ...    0.021529   \n",
       "(e,)            0.136676  0.055550  0.097857  0.021375  ...    0.029532   \n",
       "(would,)        0.080464  0.023600  0.194751  0.042077  ...    0.030666   \n",
       "(power,)        0.010519  0.041472  0.068072  0.011486  ...    0.025996   \n",
       "(hou,)          0.157157  0.010776  0.302318  0.000623  ...    0.007567   \n",
       "(energy,)       0.010877  0.022019  0.054794  0.020162  ...    0.031497   \n",
       "(company,)      0.015994  0.022055  0.072010  0.030377  ...    0.084517   \n",
       "(venturewire,)       NaN       NaN       NaN       NaN  ...         NaN   \n",
       "\n",
       "                tf_idf_131  tf_idf_132  tf_idf_133  tf_idf_134  tf_idf_135  \\\n",
       "Term                                                                         \n",
       "(com,)            0.027775    0.041008    0.075271    0.031134    0.030083   \n",
       "(ect,)            0.049141    0.000127    0.011700    0.007758    0.003001   \n",
       "(please,)         0.045836    0.014574    0.029836    0.028927    0.022144   \n",
       "(e,)              0.018377    0.008624    0.027591    0.021989    0.012677   \n",
       "(would,)          0.018186    0.010902    0.008887    0.012868    0.019618   \n",
       "(power,)          0.043382    0.009769    0.006045    0.009262    0.015549   \n",
       "(hou,)            0.020162         NaN    0.004837    0.003712    0.000479   \n",
       "(energy,)         0.007524    0.005740    0.016279    0.011624    0.009888   \n",
       "(company,)        0.005772    0.005243    0.041657    0.007793    0.007312   \n",
       "(venturewire,)         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "                tf_idf_136      mean  variance  count_notnull  \n",
       "Term                                                           \n",
       "(com,)            0.006822  0.121391  0.054427            137  \n",
       "(ect,)                 NaN  0.109993  0.067947            134  \n",
       "(please,)         0.009462  0.065944  0.010998            137  \n",
       "(e,)              0.006756  0.065836  0.022772            137  \n",
       "(would,)          0.005609  0.056304  0.018686            137  \n",
       "(power,)          0.005634  0.053380  0.047698            137  \n",
       "(hou,)                 NaN  0.052184  0.012847            122  \n",
       "(energy,)         0.003087  0.051578  0.030421            137  \n",
       "(company,)        0.001756  0.049604  0.025251            137  \n",
       "(venturewire,)         NaN  0.048956  0.007185              3  \n",
       "\n",
       "[10 rows x 141 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_val_cut_99.sort_values(by = \"mean\", ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column in the df_mean_val_cut_99 dataframe, It shows that some of these top-1-percent words appear only in some of the accounts. These words might be important for those user who use them, but less likely to be a multi-context words. Therefore, I will remove them from the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0% of words have more than 105.0 accounts\n"
     ]
    }
   ],
   "source": [
    "count_null_cut = 0.5\n",
    "print (str(count_null_cut*100) + '% of words have more than ' + str(df_mean_val_cut_99[\"count_notnull\"].quantile(count_null_cut)) + ' accounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although 50% is an arbitrary cut, we will use that as our cut point for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 2571 words in the list\n"
     ]
    }
   ],
   "source": [
    "keywords_dictionary = df_mean_val_cut_99[df_mean_val_cut_99[\"count_notnull\"]> df_mean_val_cut_99[\"count_notnull\"].quantile(count_null_cut)]\n",
    "print ('We now have ' + str(len(keywords_dictionary)) + ' words in the list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(keywords_dictionary)\n",
    "#keywords_dictionary.to_csv('keywords_dictionary.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Count topic-level word frequencies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I want to figure out whether the key words in our dictionary appears across differen topic areas. The labeled_emails_df has email topic labels, but only for a subset of emails. Using message IDs, I will merge the labeles to our keywords dictionary so that we can further trim down the dictionary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_emails_df.head()\n",
    "#labeled_emails_df[\"labeled\"].describe()\n",
    "labeled_messages = labeled_emails_df[labeled_emails_df[\"labeled\"]==True]\n",
    "print ('The dataset provides topic labels for ' + str(len(labeled_messages)) + ' emails')\n",
    "#labeled_messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the label information, we will use \"primary topics\", which corresponds to level_1 == 3 and level_2 ranges from 1 to 13. (Full description of the topic labels are available here: https://data.world/brianray/enron-email-dataset). I focus on primary topics because they are relevant to the company's business and strategies (as opposed to personal emails or administrative/editing/etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879 messages are labeled as primary topics\n"
     ]
    }
   ],
   "source": [
    "# Currently, Cat_1 implies that the first category that a human coder identified. Our category of interest, \"3\" can appear any of the 12 columns: Cat_1_level1, Cat_2_level1, ... Cat_12_level1. \n",
    "# So we will extract \"3\"s from the 12 columns. If 3 exists in any of the level 1 columns, we also want to extract the sub-category(i.e., level 2 category) from from the associated level_2 column.\n",
    "\n",
    "labeled_messages_red = labeled_messages[(labeled_messages[\"Cat_1_level_1\"].fillna(0.0).astype(int) == 3) | \n",
    "                        (labeled_messages[\"Cat_2_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_3_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_4_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_5_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_6_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_7_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_8_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_9_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_10_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_11_level_1\"].fillna(0.0).astype(int) == 3) |\n",
    "                        (labeled_messages[\"Cat_12_level_1\"].fillna(0.0).astype(int) == 3) \n",
    "                       ]\n",
    "print (str(len(labeled_messages_red)) + ' messages are labeled as primary topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cat_columns = [\"Cat_1_level_1\", \"Cat_1_level_2\", \n",
    "              \"Cat_2_level_1\", \"Cat_2_level_2\",\n",
    "              \"Cat_3_level_1\", \"Cat_3_level_2\", \n",
    "              \"Cat_4_level_1\", \"Cat_4_level_2\", \n",
    "              \"Cat_5_level_1\", \"Cat_5_level_2\", \n",
    "              \"Cat_6_level_1\", \"Cat_6_level_2\", \n",
    "              \"Cat_7_level_1\", \"Cat_7_level_2\", \n",
    "              \"Cat_8_level_1\", \"Cat_8_level_2\", \n",
    "              \"Cat_9_level_1\", \"Cat_9_level_2\", \n",
    "              \"Cat_10_level_1\", \"Cat_10_level_2\", \n",
    "              \"Cat_11_level_1\", \"Cat_11_level_2\", \n",
    "              \"Cat_12_level_1\", \"Cat_12_level_2\", \n",
    "              ]\n",
    "#cat_columns\n",
    "labeled_messages_red[cat_columns] = labeled_messages_red[cat_columns].fillna(value=0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict_dt = pd.DataFrame.from_dict(label_dict, orient = 'index', columns = [\"topic\", \"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_3_1_list = []\n",
    "topic_3_2_list = []\n",
    "topic_3_3_list = []\n",
    "topic_3_4_list = []\n",
    "topic_3_5_list = []\n",
    "topic_3_6_list = []\n",
    "topic_3_7_list = []\n",
    "topic_3_8_list = []\n",
    "topic_3_9_list = []\n",
    "topic_3_10_list = []\n",
    "topic_3_11_list = []\n",
    "topic_3_12_list = []\n",
    "\n",
    "for i in range(len(label_dict_dt)): \n",
    "    if '3.1' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_1_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.2' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_2_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.3' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_3_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.4' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_4_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.5' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_5_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.6' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_6_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.7' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_7_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.8' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_8_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.9' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_9_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.10' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_10_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.11' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_11_list.append(label_dict_dt.iloc[i]['content'])\n",
    "    if '3.12' in label_dict_dt.iloc[i]['topic']:\n",
    "        topic_3_12_list.append(label_dict_dt.iloc[i]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For words in our key word dictionary, let's count the occurance in eachof the topic document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_3_1_str = ' '.join(topic_3_1_list).lower()\n",
    "topic_3_2_str = ' '.join(topic_3_2_list).lower()\n",
    "topic_3_3_str = ' '.join(topic_3_3_list).lower()\n",
    "topic_3_4_str = ' '.join(topic_3_4_list).lower()\n",
    "topic_3_5_str = ' '.join(topic_3_5_list).lower()\n",
    "topic_3_6_str = ' '.join(topic_3_6_list).lower()\n",
    "topic_3_7_str = ' '.join(topic_3_7_list).lower()\n",
    "topic_3_8_str = ' '.join(topic_3_8_list).lower()\n",
    "topic_3_9_str = ' '.join(topic_3_9_list).lower()\n",
    "topic_3_10_str = ' '.join(topic_3_10_list).lower()\n",
    "topic_3_11_str = ' '.join(topic_3_11_list).lower()\n",
    "topic_3_12_str = ' '.join(topic_3_12_list).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_bin_3_1 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_1_str: \n",
    "        topic_bin_3_1.append(1)\n",
    "    else: \n",
    "        topic_bin_3_1.append(0)\n",
    "\n",
    "        \n",
    "topic_bin_3_2 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_2_str: \n",
    "        topic_bin_3_2.append(1)\n",
    "    else: \n",
    "        topic_bin_3_2.append(0)\n",
    "\n",
    "\n",
    "topic_bin_3_3 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_3_str: \n",
    "        topic_bin_3_3.append(1)\n",
    "    else: \n",
    "        topic_bin_3_3.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_4 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_4_str: \n",
    "        topic_bin_3_4.append(1)\n",
    "    else: \n",
    "        topic_bin_3_4.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_5 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_5_str: \n",
    "        topic_bin_3_5.append(1)\n",
    "    else: \n",
    "        topic_bin_3_5.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_6 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_6_str: \n",
    "        topic_bin_3_6.append(1)\n",
    "    else: \n",
    "        topic_bin_3_6.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_7 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_7_str: \n",
    "        topic_bin_3_7.append(1)\n",
    "    else: \n",
    "        topic_bin_3_7.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_8 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_8_str: \n",
    "        topic_bin_3_8.append(1)\n",
    "    else: \n",
    "        topic_bin_3_8.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_9 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_9_str: \n",
    "        topic_bin_3_9.append(1)\n",
    "    else: \n",
    "        topic_bin_3_9.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_10 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_10_str: \n",
    "        topic_bin_3_10.append(1)\n",
    "    else: \n",
    "        topic_bin_3_10.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_11 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_11_str: \n",
    "        topic_bin_3_11.append(1)\n",
    "    else: \n",
    "        topic_bin_3_11.append(0)\n",
    "        \n",
    "\n",
    "topic_bin_3_12 = []\n",
    "for i in range(len(keywords_dictionary)):\n",
    "    word = list((keywords_dictionary.index[i]))[0]\n",
    "    if word in topic_3_12_str: \n",
    "        topic_bin_3_12.append(1)\n",
    "    else: \n",
    "        topic_bin_3_12.append(0)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dictionary['topic_bin_3_1']= topic_bin_3_1\n",
    "keywords_dictionary['topic_bin_3_2']= topic_bin_3_2\n",
    "keywords_dictionary['topic_bin_3_3']= topic_bin_3_3\n",
    "keywords_dictionary['topic_bin_3_4']= topic_bin_3_4\n",
    "keywords_dictionary['topic_bin_3_5']= topic_bin_3_5\n",
    "keywords_dictionary['topic_bin_3_6']= topic_bin_3_6\n",
    "keywords_dictionary['topic_bin_3_7']= topic_bin_3_7\n",
    "keywords_dictionary['topic_bin_3_8']= topic_bin_3_8\n",
    "keywords_dictionary['topic_bin_3_9']= topic_bin_3_9\n",
    "keywords_dictionary['topic_bin_3_10']= topic_bin_3_10\n",
    "keywords_dictionary['topic_bin_3_11']= topic_bin_3_11\n",
    "keywords_dictionary['topic_bin_3_12']= topic_bin_3_12\n",
    "\n",
    "topic_cat_list = ['topic_bin_3_1', 'topic_bin_3_2', 'topic_bin_3_3', \n",
    "                 'topic_bin_3_4', 'topic_bin_3_5', 'topic_bin_3_6',\n",
    "                 'topic_bin_3_7', 'topic_bin_3_8', 'topic_bin_3_9', \n",
    "                 'topic_bin_3_10', 'topic_bin_3_11', 'topic_bin_3_12']\n",
    "\n",
    "keywords_dictionary['count_topics'] = keywords_dictionary[topic_cat_list].sum(axis=1)\n",
    "#keywords_dictionary['count_topics'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among 2571 words in the keyword dictionary, 1168 appear in all of the primary category\n"
     ]
    }
   ],
   "source": [
    "print ('Among ' + str(len(keywords_dictionary)) +\n",
    "       ' words in the keyword dictionary, ' + \n",
    "       str(len(keywords_dictionary[keywords_dictionary['count_topics'] == 12])) +\n",
    "       ' appear in all of the primary category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep those 1168 words as our keywords of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dictionary = keywords_dictionary[keywords_dictionary['count_topics'] == 12][['Frequency', 'mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(com,)</th>\n",
       "      <td>664513</td>\n",
       "      <td>0.121391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(ect,)</th>\n",
       "      <td>595113</td>\n",
       "      <td>0.109993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(please,)</th>\n",
       "      <td>379986</td>\n",
       "      <td>0.065944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(e,)</th>\n",
       "      <td>378364</td>\n",
       "      <td>0.065836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(would,)</th>\n",
       "      <td>325382</td>\n",
       "      <td>0.056304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(power,)</th>\n",
       "      <td>309093</td>\n",
       "      <td>0.053380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(energy,)</th>\n",
       "      <td>298781</td>\n",
       "      <td>0.051578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(company,)</th>\n",
       "      <td>288719</td>\n",
       "      <td>0.049604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(new,)</th>\n",
       "      <td>275770</td>\n",
       "      <td>0.047004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(hou,)</th>\n",
       "      <td>273226</td>\n",
       "      <td>0.052184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(time,)</th>\n",
       "      <td>265203</td>\n",
       "      <td>0.044612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(said,)</th>\n",
       "      <td>248860</td>\n",
       "      <td>0.042411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(market,)</th>\n",
       "      <td>238347</td>\n",
       "      <td>0.040329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(u,)</th>\n",
       "      <td>237553</td>\n",
       "      <td>0.040015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(may,)</th>\n",
       "      <td>223198</td>\n",
       "      <td>0.037238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(gas,)</th>\n",
       "      <td>214974</td>\n",
       "      <td>0.036238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(price,)</th>\n",
       "      <td>212958</td>\n",
       "      <td>0.035711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(know,)</th>\n",
       "      <td>201039</td>\n",
       "      <td>0.033183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(one,)</th>\n",
       "      <td>199525</td>\n",
       "      <td>0.033196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(message,)</th>\n",
       "      <td>198076</td>\n",
       "      <td>0.032248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Frequency      mean\n",
       "Term                           \n",
       "(com,)         664513  0.121391\n",
       "(ect,)         595113  0.109993\n",
       "(please,)      379986  0.065944\n",
       "(e,)           378364  0.065836\n",
       "(would,)       325382  0.056304\n",
       "(power,)       309093  0.053380\n",
       "(energy,)      298781  0.051578\n",
       "(company,)     288719  0.049604\n",
       "(new,)         275770  0.047004\n",
       "(hou,)         273226  0.052184\n",
       "(time,)        265203  0.044612\n",
       "(said,)        248860  0.042411\n",
       "(market,)      238347  0.040329\n",
       "(u,)           237553  0.040015\n",
       "(may,)         223198  0.037238\n",
       "(gas,)         214974  0.036238\n",
       "(price,)       212958  0.035711\n",
       "(know,)        201039  0.033183\n",
       "(one,)         199525  0.033196\n",
       "(message,)     198076  0.032248"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_dictionary.sort_values(by='Frequency', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further trim down the list by eliminating common English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \n",
    "                \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\",\n",
    "                \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \n",
    "                \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \n",
    "                \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \n",
    "                \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\",\n",
    "                \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\",\n",
    "                \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \n",
    "                \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \n",
    "                \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \n",
    "                \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \n",
    "                \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\",\n",
    "                \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\",\n",
    "                \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \n",
    "                \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \n",
    "                \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \n",
    "                \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \n",
    "                \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \n",
    "                \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \n",
    "                \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \n",
    "                \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \n",
    "                \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \n",
    "                \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \n",
    "                \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \n",
    "                \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \n",
    "                \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \n",
    "                \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \n",
    "                \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \n",
    "                \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \n",
    "                \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \n",
    "                \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\",\n",
    "                \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \n",
    "                \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \n",
    "                \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \n",
    "                \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \n",
    "                \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \n",
    "                \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \n",
    "                \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\",\n",
    "                \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\",\n",
    "                \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \n",
    "                \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \n",
    "                \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \n",
    "                \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \n",
    "                \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \n",
    "                \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \n",
    "                \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \n",
    "                \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \n",
    "                \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \n",
    "                \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \n",
    "                \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \n",
    "                \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \n",
    "                \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \n",
    "                \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \n",
    "                \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \n",
    "                \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \n",
    "                \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \n",
    "                \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \n",
    "                \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \n",
    "                \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \n",
    "                \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \n",
    "                \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \n",
    "                \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \n",
    "                \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \n",
    "                \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \n",
    "                \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \n",
    "                \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \n",
    "                \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \n",
    "                \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \n",
    "                \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \n",
    "                \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \n",
    "                \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \n",
    "                \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \n",
    "                \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \n",
    "                \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \n",
    "                \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \n",
    "                \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \n",
    "                \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \n",
    "                \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \n",
    "                \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \n",
    "                \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \n",
    "                \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \n",
    "                \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \n",
    "                \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \n",
    "                \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \n",
    "                \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \n",
    "                \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \n",
    "                \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \n",
    "                \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab = []\n",
    "\n",
    "for item in list(keywords_dictionary.index):\n",
    "    my_vocab.append(str(item).split('\\'')[1])\n",
    "\n",
    "new_dict = keywords_dictionary[~keywords_dictionary['my_vocab'].isin(my_stopwords)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further trim down the list by excluding names, Enron-specific abbreviations, and days/months/etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = ['john', 'jeff', 'vince', 'david', 'steve', 'smith', 'chris', \n",
    "'michael', 'richard', 'bush', 'paul', 'jim', 'susan', 'joe', \n",
    "'tom', 'duke', 'rick', 'eric', 'dan', 'steven', 'ben', 'lisa', \n",
    "'frank', 'carol', 'karen', 'lee',  'gary', 'tim', 'martin', \n",
    "'rob', 'sue', 'allen', 'stephen', 'linda', 'hey', 'matt', \n",
    "'sarah', 'ted', 'harry', 'barbara', 'stanley', 'anderson', 'pat', \n",
    "'jane', 'louis', 'phil', 'jean', 'stan', \n",
    "'day', 'year', 'morning', 'today', 'yesterday', 'tomorrow', \n",
    "'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', \n",
    "'januray', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december',\n",
    "'jan', 'dec', 'nov', 'fri', 'oct', 'sept', 'mar', 'apr', 'aug', 'jun', 'sep', 'li', \n",
    "'ect', 'hou', 'fax', 'ena', 'asp', 'san', 'iso', 'pg', 'eb', 'dow', 'mp', 'mw', 'gov',  'ceo', 'fyi', \n",
    "'mm', 'cal', 'usa', 'calif', 'pre', 'tel', 'ene', 'ew', 'var', 'pa', \n",
    "'ext', 'ets', 'rep', 'ha', 'fed', 'sea', 'mi', 'min', \n",
    "'sw', 'ing', 'isc', 'edi', 'ferc','doc','corp', 'north', 'america']\n",
    "\n",
    "my_custom_vocab = []\n",
    "\n",
    "for item in list(new_dict.index):\n",
    "    my_custom_vocab.append(str(item).split('\\'')[1])\n",
    "\n",
    "#new_dict['my_custom_vocab'] = my_custom_vocab\n",
    "reduced_dict = new_dict[~new_dict['my_custom_vocab'].isin(custom_stopwords)]\n",
    "reduced_dict = reduced_dict.sort_values(by='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "711"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_dict[50:100]\n",
    "len(reduced_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Store the key word dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_keywords_dictionary.to_csv('final_keywords_dictionary.csv', index=True)\n",
    "reduced_dict.to_csv('keywords_reduced.csv', index=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
