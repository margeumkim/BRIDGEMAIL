{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_word_sense_disambiguation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmKLhSlTdbgwEM5D7jKzs9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/margeumkim/BRIDGEMAIL/blob/master/04_word_sense_disambiguation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n28E3O--mBdp",
        "colab_type": "text"
      },
      "source": [
        "# BERT "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6xaeaHCmPG8",
        "colab_type": "text"
      },
      "source": [
        "##1. Connecting to a GPU and mounting a google drive folder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw2nNit3k8hN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aegyfrdk-UN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otj0zYQklAWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime\n",
        "import pandas as pd \n",
        "import re\n",
        "\n",
        "# Import the parsed email data \n",
        "email_directory = \"/content/drive/My Drive/data/emails_parsed.csv\"\n",
        "emails_df  = pd.read_csv(email_directory)\n",
        "keyword_dict_directory = \"/content/drive/My Drive/data/keywords_reduced.csv\"  #### long list = final_keywords_dictionary.csv\n",
        "keyword_dict  = pd.read_csv(keyword_dict_directory)   \n",
        "keyword_dict = keyword_dict.sort_values(by='mean', ascending=False)\n",
        "keyword_dict_100 = keyword_dict[0:100]  # Keeping the top 100 context-dependent words\n",
        "address_user_directory = \"/content/drive/My Drive/data/address_user_df.csv\"\n",
        "address_user_df  = pd.read_csv(address_user_directory)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUPo272HmDwd",
        "colab_type": "text"
      },
      "source": [
        "## 2. Importing BERT pretrained model and tokenizer and defining functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq8gIhqblHQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "from transformers import *\n",
        "import pdb\n",
        "import operator\n",
        "from collections import OrderedDict\n",
        "import sys\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "PATH='bert-base-uncased'\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(PATH,do_lower_case=True)\n",
        "model = BertForMaskedLM.from_pretrained(PATH)\n",
        "model.eval()\n",
        "\n",
        "def get_sent():\n",
        "    #print(\"Enter sentence:\")\n",
        "    sent = my_sentence\n",
        "    if (not sent.endswith(\".\")):\n",
        "#        print(\"Appending period to do dummy masking\")\n",
        "        sent = sent + \" .\"\n",
        "    return '[CLS] ' + sent + '[SEP]'\n",
        "\n",
        "def print_tokens(tokenized_text):\n",
        "    dstr = \"\"\n",
        "    for i in range(len(tokenized_text)):\n",
        "        dstr += \"   \" +  str(i) + \":\"+tokenized_text[i]\n",
        "    print(dstr)\n",
        "    print()\n",
        "\n",
        "\n",
        "def get_pos():\n",
        "    while True:\n",
        "        masked_index = 0\n",
        "        try:\n",
        "                masked_index = int(my_index)\n",
        "                return masked_index\n",
        "        except:\n",
        "            print(\"Enter valid number: (0 to quit)\")\n",
        "            masked_index = int(my_index)\n",
        "            if (masked_index == 0):\n",
        "                print(\"Quitting\")\n",
        "                sys.exit()\n",
        "            return masked_index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_Q6kJnWpj2Y",
        "colab_type": "text"
      },
      "source": [
        "## 3. Subsetting into a user, find sentences that contains a keyword"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjiDoZkVkhj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_columns = ['user', 'keyword', 'high_att_words']\n",
        "user_word_meaning_dict_df = pd.DataFrame(columns=dict_columns)\n",
        "\n",
        "# First layer: Over user\n",
        "for l in range(len(address_user_df)):      \n",
        "  my_user = address_user_df.iloc[l][\"user\"]     \n",
        "  my_emails = emails_df[emails_df[\"user\"] == my_user]\n",
        "  #len(my_emails)\n",
        "\n",
        "  my_sentences = []\n",
        "  \n",
        "  user_word_meaning_dict = []  # this will store my outputs\n",
        "\n",
        "  for index, row in my_emails.iterrows():\n",
        "      if type(row['content']) ==  str:\n",
        "        for j in range(len(row['content'].split('.'))):\n",
        "          my_sentences.append(row['content'].split('.')[j])\n",
        "      else:\n",
        "        #print (index)\n",
        "        pass\n",
        "  \n",
        "  my_sentences = pd.DataFrame(my_sentences)\n",
        "  \n",
        "\n",
        "  # Second layer: For each user, iterate over keywords \n",
        "  for m in range(len(keyword_dict_100)):                               \n",
        "\n",
        "    keyword = [keyword_dict_100.iloc[m]['Term'].split('\\'')[1]]            \n",
        "\n",
        "    my_sentences.columns = ['Sentence']\n",
        "\n",
        "    sentence_for_keyword = []\n",
        "\n",
        "      # Third layer: For keyword, let's find sentences that contain the keyword\n",
        "    for index, row in my_sentences.iterrows():\n",
        "\n",
        "        if type(row['Sentence']) == str:\n",
        "      \n",
        "            if any(item in list(row['Sentence'].split(' ')) for item in keyword):\n",
        "            #print (my_bool)\n",
        "              sentence_for_keyword.append(row['Sentence'])\n",
        "    \n",
        "    #print(len(sentence_for_keyword))\n",
        "    if len(sentence_for_keyword) > 10:\n",
        "     \n",
        "      # I will store top attention words for the keyword in this list\n",
        "      keyword_meaning = []\n",
        "\n",
        "      # Fourth layer: Now that we know the sentences, let's evaluate each sentence using pretrained BERT. And extract top attention words\n",
        "      for k in range(len(sentence_for_keyword)):\n",
        "\n",
        "        my_sentence = sentence_for_keyword[k]\n",
        "\n",
        "        my_breaker = True\n",
        "        text = get_sent()\n",
        "\n",
        "        tokenized_text = tokenizer.tokenize(text)    # BERT takes \"tokens\" (index associated with each word) instead of words. This step tokenizes my sentence\n",
        "\n",
        "        # Fifth layer: Some sentences have just too many words/tokens to use BERT. Skip those sentences\n",
        "        if len(tokenized_text) > 500:    \n",
        "          pass\n",
        "\n",
        "        else:\n",
        "          \n",
        "          # Some keywords does not exist in the BERT's dictionary. \n",
        "          # For example,  = ['fax']  does not exist in BERT's dictionary -- Tokenizer breakes it down to : fa / ##x; \n",
        "          # I am skipping these cases\n",
        "          try: \n",
        "            my_index = tokenized_text.index(keyword[0]) \n",
        "          except: \n",
        "            pass\n",
        "          #print_tokens(tokenized_text)\n",
        "\n",
        "          indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "\n",
        "          # Create the segments tensors.\n",
        "          segments_ids = [0] * len(tokenized_text)\n",
        "          masked_index = len(tokenized_text) - 2\n",
        "          tokenized_text[masked_index] = \"[MASK]\"\n",
        "          indexed_tokens[masked_index] = 103\n",
        "          results_dict = {}\n",
        "\n",
        "          # Convert inputs to PyTorch tensors\n",
        "          tokens_tensor = torch.tensor([indexed_tokens])\n",
        "          segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "          with torch.no_grad():\n",
        "              predictions = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "          # Sixth layer: While within top 10 range, extract high attention words              \n",
        "          while (my_breaker == True):\n",
        "              #print_tokens(tokenized_text)\n",
        "              #print(\"Enter any term position neighbor:\")\n",
        "              masked_index = get_pos()\n",
        "              results_dict = {}\n",
        "\n",
        "              # Some sentences are from the headings from a forwarded messages. I will ignore these headings\n",
        "              if (masked_index) <= len(tokenized_text):\n",
        "                for i in range(len(predictions[0][0,masked_index])):\n",
        "                    tok = tokenizer.convert_ids_to_tokens([i])[0]\n",
        "                    results_dict[tok] = float(predictions[0][0,masked_index][i].tolist())\n",
        "\n",
        "                k = 0\n",
        "                hist_d = {}\n",
        "                sorted_d = OrderedDict(sorted(results_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
        "                first = True\n",
        "                max_val = 0\n",
        "                for i in sorted_d:\n",
        "                    if (first):\n",
        "                        max_val = sorted_d[i]\n",
        "                        first = False\n",
        "                    val = round(float(sorted_d[i])/max_val,1)\n",
        "                    if (val in hist_d):\n",
        "                        hist_d[val] += 1\n",
        "                    else:\n",
        "                        hist_d[val] = 1\n",
        "                    k += 1\n",
        "        #            if (k <= 10):    # If you activate this if statement, you will be able to see top 10 attention words \n",
        "        #                pass\n",
        "                        #print(i,sorted_d[i])    \n",
        "                    if (k > 10):\n",
        "                      my_breaker = False\n",
        "\n",
        "              # Append the top 10 words in terms of the attention scores to the keyword_meaning list\n",
        "              for j in range(10):\n",
        "                keyword_meaning.append(list(list(sorted_d.items())[j]))\n",
        "              \n",
        "    keyword_meaning_df = pd.DataFrame(keyword_meaning)\n",
        "    keyword_meaning_df.columns = ['keyword', 'attention']\n",
        "    keyword_meaning_df = keyword_meaning_df[keyword_meaning_df['keyword'] != keyword[0]]    # Often, the keyword itself is the top attention word. Let's remove those instances\n",
        "    keyword_meaning_df = keyword_meaning_df.groupby('keyword',as_index=False).mean()    # If a attention word was extracted from many different sentences (instances), take the mean of attention scores from each instances.\n",
        "    keyword_meaning_df = keyword_meaning_df.sort_values(by='attention', ascending=False).reset_index()\n",
        "    #keyword_meaning_df.head(20)\n",
        "\n",
        "    # BERT sometimes splits a word into pieces and use ## to remark such cases. Since tokens like ##er or pre## do not give much context to users, I am removing them from the keyword_meaning list. \n",
        "    drop_list = []\n",
        "    for i in range(len(keyword_meaning_df)):\n",
        "      if len(re.findall(r'##', keyword_meaning_df['keyword'][i])) > 0:\n",
        "        drop_list.append(1)\n",
        "      else: \n",
        "        drop_list.append(0)    \n",
        "\n",
        "    keyword_meaning_df['drop_flag'] = drop_list\n",
        "    keyword_meaning_df = keyword_meaning_df[keyword_meaning_df['drop_flag'] == 0]\n",
        "\n",
        "    # Store the final top 10 attention words to a list\n",
        "    my_keyword_meaning_top10 = keyword_meaning_df[0:10][['keyword']]\n",
        "    user_word_meaning_dict.append([my_user, keyword[0], list(my_keyword_meaning_top10['keyword'])])\n",
        "    #print(m)\n",
        "\n",
        "  # Store the list to pickle so that we can access the information using streamlit \n",
        "  #print(my_user)\n",
        "  user_word_meaning_dict_df = user_word_meaning_dict_df.append(pd.DataFrame(user_word_meaning_dict, columns = dict_columns))\n",
        "\n",
        "filename_to_pickle = \"/content/drive/My Drive/data/word_meaning.pkl\"\n",
        "user_word_meaning_dict_df.to_pickle(filename_to_pickle) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}