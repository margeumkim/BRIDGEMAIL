{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Evaluation_Vector_Variances.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QaRCIIqzsDIE",
        "55KOhfKIKG0r",
        "5E4dtcQeJ-2z",
        "JXTKJnb_KSgp"
      ],
      "authorship_tag": "ABX9TyMNnGJj5ARKBGCH/AxYZMBf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "521125a8c4a74521892efb95cd0a7ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8cce79e7280d4eafbd7cc5a35517b4e0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fca986d32f464a34bc2f47ce3bdad3f8",
              "IPY_MODEL_d25165cdd0364318a33c4fc67fae9a53"
            ]
          }
        },
        "8cce79e7280d4eafbd7cc5a35517b4e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fca986d32f464a34bc2f47ce3bdad3f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d5a0db32d4c24a8c9d7593159ca2fd1b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99c6048cb74247758f0b27ec3d318de3"
          }
        },
        "d25165cdd0364318a33c4fc67fae9a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3ed3691b5aca4ea3b7f4fb169fa48fec",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 675kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3c7f838a78d9476fb5dfc9a837a1828f"
          }
        },
        "d5a0db32d4c24a8c9d7593159ca2fd1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99c6048cb74247758f0b27ec3d318de3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ed3691b5aca4ea3b7f4fb169fa48fec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3c7f838a78d9476fb5dfc9a837a1828f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/margeumkim/BRIDGEMAIL/blob/master/BERT_Evaluation_Vector_Variances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVRVPbF0W0LW",
        "colab_type": "text"
      },
      "source": [
        "# 1. Loading Pre-Trained BERT\n",
        "\n",
        "\n",
        "Install the pytorch interface for BERT by Hugging Face. (This library contains interfaces for other pretrained language models like OpenAI's GPT and GPT-2.)\n",
        "\n",
        "We've selected the pytorch interface because it strikes a nice balance between the high-level APIs (which are easy to use but don't provide insight into how things work) and tensorflow code (which contains lots of details but often sidetracks us into lessons about tensorflow, when the purpose here is BERT!).\n",
        "\n",
        "If you're running this code on Google Colab, you will have to install this library each time you reconnect; the following cell will take care of that for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC5sPrQIXIuK",
        "colab_type": "code",
        "outputId": "62098260-242e-4baf-d4a6-b338dca131b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 5.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 15.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a15a327d9bfcaf59de3b1647e287fa39d36f25722d454c783dc886287ac50dbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zS3LS8mIRIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upgrading RAM to 25GB from 12GB\n",
        "#a = []\n",
        "#while(1):\n",
        "#    a.append(‘1’)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d66nvn-XQ1s",
        "colab_type": "text"
      },
      "source": [
        "Now let's import pytorch, the pretrained BERT model, and a BERT tokenizer.\n",
        "\n",
        "We'll explain the BERT model in detail in a later tutorial, but this is the pre-trained model released by Google that ran for many, many hours on Wikipedia and Book Corpus, a dataset containing +10,000 books of different genres. This model is responsible (with a little modification) for beating NLP benchmarks across a range of tasks. Google released a few variations of BERT models, but the one we'll use here is the smaller of the two available sizes (\"base\" and \"large\") and ignores casing, hence \"uncased.\"\"\n",
        "\n",
        "**transformers** provides a number of classes for applying BERT to different tasks (token classification, text classification, ...). Here, we're using the basic **BertModel** which has no specific output task--it's a good choice for using BERT just to extract embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlhbL_pyXQKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "521125a8c4a74521892efb95cd0a7ca2",
            "8cce79e7280d4eafbd7cc5a35517b4e0",
            "fca986d32f464a34bc2f47ce3bdad3f8",
            "d25165cdd0364318a33c4fc67fae9a53",
            "d5a0db32d4c24a8c9d7593159ca2fd1b",
            "99c6048cb74247758f0b27ec3d318de3",
            "3ed3691b5aca4ea3b7f4fb169fa48fec",
            "3c7f838a78d9476fb5dfc9a837a1828f"
          ]
        },
        "outputId": "c2fa747a-17c7-4f99-ef94-d5c5a93a88c7"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "#logging.basicCOnfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "521125a8c4a74521892efb95cd0a7ca2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqneXVsRxSt1",
        "colab_type": "text"
      },
      "source": [
        "# 2. Loading Enron emails data \n",
        "We'll load the Enron Emails dataset. For the purpose of exploratory data analysis, I will subset two accounts -- Kate Symes and Phillip Love. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhLzEOgSxr2q",
        "colab_type": "text"
      },
      "source": [
        "Let's import data from my Google drive \n",
        "** (but later, will need to combine with git - you must slice file so that each is <25MB) **\n",
        "\n",
        "> NOTE: When prompted, click on the link to get authentication to allow Google to access your Drive. You should see a screen with “Google Cloud SDK wants to access your Google Account” at the top. After you allow permission, copy the given verification code and paste it in the box in Colab.\n",
        "Once you have completed verification, go to the CSV file in Google Drive, right-click on it and select “Get shareable link”. The link will be copied into your clipboard. Paste this link into a string variable in Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4lrsWfeya7G",
        "colab_type": "code",
        "outputId": "4f7cd07e-3632-4a92-ecde-1b0dde7a1e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNzhm56N0y_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = \"/content/drive/My Drive/data/emails.csv\"\n",
        "emails_df = pd.read_csv(path)\n",
        "\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcyCPYVPzng2",
        "colab_type": "code",
        "outputId": "6f459bca-62d1-471e-9564-fbe8ff9d7ed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Checking whether the file is loaded properly\n",
        "print(emails_df.shape)\n",
        "emails_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(517401, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>allen-p/_sent_mail/1.</td>\n",
              "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>allen-p/_sent_mail/10.</td>\n",
              "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>allen-p/_sent_mail/100.</td>\n",
              "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>allen-p/_sent_mail/1000.</td>\n",
              "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>allen-p/_sent_mail/1001.</td>\n",
              "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       file                                            message\n",
              "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
              "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
              "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
              "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
              "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dL4J3ErzvCu",
        "colab_type": "code",
        "outputId": "c3b97cb7-8ef0-4273-ece9-c55d342d78e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# A single message looks like this\n",
        "print(emails_df['message'][0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Message-ID: <18782981.1075855378110.JavaMail.evans@thyme>\n",
            "Date: Mon, 14 May 2001 16:39:00 -0700 (PDT)\n",
            "From: phillip.allen@enron.com\n",
            "To: tim.belden@enron.com\n",
            "Subject: \n",
            "Mime-Version: 1.0\n",
            "Content-Type: text/plain; charset=us-ascii\n",
            "Content-Transfer-Encoding: 7bit\n",
            "X-From: Phillip K Allen\n",
            "X-To: Tim Belden <Tim Belden/Enron@EnronXGate>\n",
            "X-cc: \n",
            "X-bcc: \n",
            "X-Folder: \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Sent Mail\n",
            "X-Origin: Allen-P\n",
            "X-FileName: pallen (Non-Privileged).pst\n",
            "\n",
            "Here is our forecast\n",
            "\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dzu4OVOz2mf",
        "colab_type": "text"
      },
      "source": [
        "Let's split the information in each message into different fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtXtb0arzz2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Helper functions\n",
        "def get_text_from_email(msg):\n",
        "    '''To get the content from email objects'''\n",
        "    parts = []\n",
        "    for part in msg.walk():\n",
        "        if part.get_content_type() == 'text/plain':\n",
        "            parts.append( part.get_payload() )\n",
        "    return ''.join(parts)\n",
        "\n",
        "def split_email_addresses(line):\n",
        "    '''To separate multiple email addresses'''\n",
        "    if line:\n",
        "        addrs = line.split(',')\n",
        "        addrs = frozenset(map(lambda x: x.strip(), addrs))\n",
        "    else:\n",
        "        addrs = None\n",
        "    return addrs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk6c8ZF0zzve",
        "colab_type": "code",
        "outputId": "fde46125-d26e-4839-a39a-a9b94f0edae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "import email \n",
        "\n",
        "# Parse the emails into a list email objects\n",
        "messages = list(map(email.message_from_string, emails_df['message']))\n",
        "emails_df.drop('message', axis=1, inplace=True)\n",
        "# Get fields from parsed email objects\n",
        "keys = messages[0].keys()\n",
        "for key in keys:\n",
        "    emails_df[key] = [doc[key] for doc in messages]\n",
        "# Parse content from emails\n",
        "emails_df['content'] = list(map(get_text_from_email, messages))\n",
        "# Split multiple email addresses\n",
        "emails_df['From'] = emails_df['From'].map(split_email_addresses)\n",
        "emails_df['To'] = emails_df['To'].map(split_email_addresses)\n",
        "\n",
        "# Extract the root of 'file' as 'user'\n",
        "emails_df['user'] = emails_df['file'].map(lambda x:x.split('/')[0])\n",
        "del messages\n",
        "\n",
        "emails_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>Message-ID</th>\n",
              "      <th>Date</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>Subject</th>\n",
              "      <th>Mime-Version</th>\n",
              "      <th>Content-Type</th>\n",
              "      <th>Content-Transfer-Encoding</th>\n",
              "      <th>X-From</th>\n",
              "      <th>X-To</th>\n",
              "      <th>X-cc</th>\n",
              "      <th>X-bcc</th>\n",
              "      <th>X-Folder</th>\n",
              "      <th>X-Origin</th>\n",
              "      <th>X-FileName</th>\n",
              "      <th>content</th>\n",
              "      <th>user</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>allen-p/_sent_mail/1.</td>\n",
              "      <td>&lt;18782981.1075855378110.JavaMail.evans@thyme&gt;</td>\n",
              "      <td>Mon, 14 May 2001 16:39:00 -0700 (PDT)</td>\n",
              "      <td>(phillip.allen@enron.com)</td>\n",
              "      <td>(tim.belden@enron.com)</td>\n",
              "      <td></td>\n",
              "      <td>1.0</td>\n",
              "      <td>text/plain; charset=us-ascii</td>\n",
              "      <td>7bit</td>\n",
              "      <td>Phillip K Allen</td>\n",
              "      <td>Tim Belden &lt;Tim Belden/Enron@EnronXGate&gt;</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>\\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...</td>\n",
              "      <td>Allen-P</td>\n",
              "      <td>pallen (Non-Privileged).pst</td>\n",
              "      <td>Here is our forecast\\n\\n</td>\n",
              "      <td>allen-p</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>allen-p/_sent_mail/10.</td>\n",
              "      <td>&lt;15464986.1075855378456.JavaMail.evans@thyme&gt;</td>\n",
              "      <td>Fri, 4 May 2001 13:51:00 -0700 (PDT)</td>\n",
              "      <td>(phillip.allen@enron.com)</td>\n",
              "      <td>(john.lavorato@enron.com)</td>\n",
              "      <td>Re:</td>\n",
              "      <td>1.0</td>\n",
              "      <td>text/plain; charset=us-ascii</td>\n",
              "      <td>7bit</td>\n",
              "      <td>Phillip K Allen</td>\n",
              "      <td>John J Lavorato &lt;John J Lavorato/ENRON@enronXg...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>\\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...</td>\n",
              "      <td>Allen-P</td>\n",
              "      <td>pallen (Non-Privileged).pst</td>\n",
              "      <td>Traveling to have a business meeting takes the...</td>\n",
              "      <td>allen-p</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>allen-p/_sent_mail/100.</td>\n",
              "      <td>&lt;24216240.1075855687451.JavaMail.evans@thyme&gt;</td>\n",
              "      <td>Wed, 18 Oct 2000 03:00:00 -0700 (PDT)</td>\n",
              "      <td>(phillip.allen@enron.com)</td>\n",
              "      <td>(leah.arsdall@enron.com)</td>\n",
              "      <td>Re: test</td>\n",
              "      <td>1.0</td>\n",
              "      <td>text/plain; charset=us-ascii</td>\n",
              "      <td>7bit</td>\n",
              "      <td>Phillip K Allen</td>\n",
              "      <td>Leah Van Arsdall</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>\\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail</td>\n",
              "      <td>Allen-P</td>\n",
              "      <td>pallen.nsf</td>\n",
              "      <td>test successful.  way to go!!!</td>\n",
              "      <td>allen-p</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>allen-p/_sent_mail/1000.</td>\n",
              "      <td>&lt;13505866.1075863688222.JavaMail.evans@thyme&gt;</td>\n",
              "      <td>Mon, 23 Oct 2000 06:13:00 -0700 (PDT)</td>\n",
              "      <td>(phillip.allen@enron.com)</td>\n",
              "      <td>(randall.gay@enron.com)</td>\n",
              "      <td></td>\n",
              "      <td>1.0</td>\n",
              "      <td>text/plain; charset=us-ascii</td>\n",
              "      <td>7bit</td>\n",
              "      <td>Phillip K Allen</td>\n",
              "      <td>Randall L Gay</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>\\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail</td>\n",
              "      <td>Allen-P</td>\n",
              "      <td>pallen.nsf</td>\n",
              "      <td>Randy,\\n\\n Can you send me a schedule of the s...</td>\n",
              "      <td>allen-p</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>allen-p/_sent_mail/1001.</td>\n",
              "      <td>&lt;30922949.1075863688243.JavaMail.evans@thyme&gt;</td>\n",
              "      <td>Thu, 31 Aug 2000 05:07:00 -0700 (PDT)</td>\n",
              "      <td>(phillip.allen@enron.com)</td>\n",
              "      <td>(greg.piper@enron.com)</td>\n",
              "      <td>Re: Hello</td>\n",
              "      <td>1.0</td>\n",
              "      <td>text/plain; charset=us-ascii</td>\n",
              "      <td>7bit</td>\n",
              "      <td>Phillip K Allen</td>\n",
              "      <td>Greg Piper</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>\\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail</td>\n",
              "      <td>Allen-P</td>\n",
              "      <td>pallen.nsf</td>\n",
              "      <td>Let's shoot for Tuesday at 11:45.</td>\n",
              "      <td>allen-p</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       file  ...     user\n",
              "0     allen-p/_sent_mail/1.  ...  allen-p\n",
              "1    allen-p/_sent_mail/10.  ...  allen-p\n",
              "2   allen-p/_sent_mail/100.  ...  allen-p\n",
              "3  allen-p/_sent_mail/1000.  ...  allen-p\n",
              "4  allen-p/_sent_mail/1001.  ...  allen-p\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vUa9S9O74vN",
        "colab_type": "text"
      },
      "source": [
        "Now, randomly sample #n emails from the dataset to run BERT and find tokens with contextual meanings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZtz43uG8Cwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's randomly sample #N sentences \n",
        "import random \n",
        "\n",
        "sample_size = 100\n",
        "emails_sample = emails_df.sample(n = sample_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwL4RnsZ9P96",
        "colab_type": "text"
      },
      "source": [
        "Select contents and for each person"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Sfytyx-G3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_df = emails_sample[\"content\"].reset_index()\n",
        "#content_i_df.head()\n",
        "#content_j_df = account_j_df[\"content\"].reset_index()\n",
        "#content_j_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SzhnhbL-YUi",
        "colab_type": "text"
      },
      "source": [
        "Pre-process text content:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BneABW8--wO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def email_clean(text):\n",
        "    text = text.rstrip().lower()\n",
        "    text = \" \".join([i for i in text.split() if(not i.isdigit())])\n",
        "    text = re.sub(r'\\n--.*?\\n', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'Forwarded by.*?Subject:', '', text, flags=re.DOTALL) \n",
        "    text = re.sub(r'Fwd:.*?Subject:', '', text, flags=re.DOTALL) \n",
        "    text = re.sub(r'Fw:.*?Subject:', '', text, flags=re.DOTALL)     \n",
        "    text = re.sub(r'FW:.*?Subject:', '', text, flags=re.DOTALL)         \n",
        "    text = re.sub(r'Forwarded:.*?Subject:', '', text, flags=re.DOTALL)         \n",
        "    text = re.sub(r'From:.*?Subject:', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'PM', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'AM', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'enron america corp', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'enron', '', text, flags=re.DOTALL)    \n",
        "    text = re.sub(r'etc', '', text, flags=re.DOTALL)    \n",
        "    text = re.sub(r'\\n', '', text, flags=re.DOTALL)    \n",
        "    text = re.sub(r'.*?@.*?', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'http://.*?', '', text, flags=re.DOTALL)    \n",
        "    text = re.sub(r'cc.*?', '', text, flags=re.DOTALL)    \n",
        "    text = re.sub(r'subject', '', text, flags=re.DOTALL)    \n",
        "    text = re.sub(r'from', '', text, flags=re.DOTALL)    \n",
        "    text = re.sub(r'sent', '', text, flags=re.DOTALL)        \n",
        "    text = re.sub(r'to', '', text, flags=re.DOTALL)       \n",
        "    text = re.sub(r'=', '', text, flags=re.DOTALL)            \n",
        "#    text = re.sub(signoff_1, '', text, flags=re.DOTALL)    \n",
        "#    text = re.sub(signoff_2, '', text, flags=re.DOTALL)       \n",
        "#    text = re.sub(signoff_3, '', text, flags=re.DOTALL)   \n",
        "    return text\n",
        "\n",
        "\n",
        "def clean(text):\n",
        "#    stop = set(stopwords.words('english'))\n",
        "#    stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\",\n",
        "#                 \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \n",
        "#                 \"enron america corp\", \"enron\", \"etc\", \"na\", signoff_1, signoff_2, signoff_3))\n",
        "    exclude = set(string.punctuation) - {'.'}\n",
        "    lemma = WordNetLemmatizer()\n",
        "    porter= PorterStemmer()\n",
        "    \n",
        "#    text = text.rstrip().lower()\n",
        "    digit_free = \" \".join([i for i in text.lower().split() if(not i.isdigit())])\n",
        "    punc_free = ''.join(ch for ch in digit_free if ch not in exclude)\n",
        "#    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free)\n",
        "    #stem = \" \".join(porter.stem(token) for token in normalized.split())\n",
        "    \n",
        "    return punc_free\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzglQNly-x7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('wordnet')\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "from nltk.stem.porter import PorterStemmer \n",
        "\n",
        "\n",
        "#signoff_1 = str(account_i_df[\"user\"][0].split('-')[1] + list(account_i_df[\"user\"][0].split('-')[0])[0])\n",
        "#print(signoff_1)\n",
        "#signoff_2 = str(list(signoff_1)[0])\n",
        "#print(signoff_2)\n",
        "#signoff_3 = str(account_i_sent_df[\"user\"][0].split('-')[0])\n",
        "#print(signoff_3)\n",
        "\n",
        "#account_i_clean_content =[]\n",
        "#for text in account_i_sent_df['content']:\n",
        "#    email_dot_split = email_clean(text).split('.')\n",
        "#    for item in email_dot_split:\n",
        "#        account_i_clean_content.append(item.split('>'))\n",
        "#    account_i_clean_content.append(clean(email_clean(text)))\n",
        "\n",
        "#signoff_1 = str(account_j_df[\"user\"][0].split('-')[1] + list(account_j_df[\"user\"][0].split('-')[0])[0])\n",
        "#print(signoff_1)\n",
        "#signoff_2 = str(list(signoff_1)[0])\n",
        "#print(signoff_2)\n",
        "#signoff_3 = str(account_j_sent_df[\"user\"][0].split('-')[0])\n",
        "#print(signoff_3)\n",
        "\n",
        "text_clean_content = []\n",
        "for text in content_df['content']:\n",
        "    email_dot_split = email_clean(text).split('.')\n",
        "    for item in email_dot_split:\n",
        "        text_clean_content.append(item.split('>'))\n",
        "#    account_j_clean_content.append(clean(email_clean(text)))\n",
        "\n",
        "## Flattening nested lists\n",
        "#account_i_content = [] \n",
        "#for text in account_i_clean_content:\n",
        "#    for item in text:\n",
        "#      if len(list(item.split())) >3: \n",
        "#        if len(list(item.split())) < 101: \n",
        "#          account_i_content.append(item)\n",
        "\n",
        "text_content = [] \n",
        "for text in text_clean_content:\n",
        "    for item in text:\n",
        "      if len(list(item.split())) >3: \n",
        "        if len(list(item.split())) < 101: \n",
        "          text_content.append(item)      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiYEWClBbAMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create labels \n",
        "#account_i_content = pd.DataFrame(account_i_content)\n",
        "#account_i_content.loc[account_i_content.label==0]\n",
        "#df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yenB06gCXnSS",
        "colab_type": "code",
        "outputId": "066f698d-a5be-4bc1-fe87-2a90ad8ab38d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(text_content[0])\n",
        "len(list(text_content[0].split()))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "com] : friday, ocber 19, 10:40 am : anderson, phelps; balling, robert; barthel, scott; bayless, tucker; bear, jon; broadhead, ron; burns, morris; byrom, john; chase, robert; coll, chuck; corbett, john; cox, george; creel, craig; david, edward; debrine, earl; flanagin, mary; french, terri; gallagher, bob; gallegos, gene; garlinger, bob; gilliland, richard; girand, dan; gorham, frank; gratn, patrick; greer, al; guajardo, ronnie; hajny, david; hannifin, patrick; harringn, gerald; hart, korbi; harvard, jeff; hatch, lloyd; heatly, rosemary; hegarty, patrick; henderson, david; hope, sonny; hyatt, kevin; krakauskas, ny; kvasnicka, sally; lee, knute; lee, robert; maw, bill; merrion, t\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PznC5T1pdTjE",
        "colab_type": "text"
      },
      "source": [
        "# 3. Tokenization & Input Formatting\n",
        "Because BERT is a pretrained model that expects input data in a specific format, we will need:\n",
        "\n",
        "1. A **special token**, [SEP], to mark the end of a sentence, or the separation between two sentences\n",
        "2. A **special token**, [CLS], at the beginning of our text. This token is used for classification tasks, but BERT expects it no matter what your application is.\n",
        "3. Tokens that conform with the fixed vocabulary used in BERT\n",
        "4. The **Token IDs** for the tokens, from BERT's tokenizer\n",
        "5. **Mask IDs** to indicate which elements in the sequence are tokens and which are padding elements\n",
        "6. **Segment IDs** used to distinguish different sentences\n",
        "7. **Positional Embeddings** used to show token position within the sequence\n",
        "Luckily, the transformers interface takes care of all of the above requirements (using the tokenizer.encode_plus function).\n",
        "\n",
        "Since this is intended as an introduction to working with BERT, though, we're going to perform these steps in a (mostly) manual way.\n",
        "\n",
        "For an example of using tokenizer.encode_plus, see the next post on Sentence Classification [here](http://mccormickml.com/2019/07/22/BERT-fine-tuning/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aynvBYCleFTH",
        "colab_type": "text"
      },
      "source": [
        "##3.1. BERT Tokenizer\n",
        "To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n",
        "\n",
        "The tokenization must be performed by the tokenizer included with BERT--the below cell will download this for us. We'll be using the \"uncased\" version here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxLm5Qb4YhzL",
        "colab_type": "code",
        "outputId": "ffbe765d-0cde-449d-98e0-0d6029801918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNhnF1kUeinf",
        "colab_type": "text"
      },
      "source": [
        "Le'ts apply the tokenizer to one sentence just to see the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e0lfdv6FCHs",
        "colab_type": "code",
        "outputId": "427e5640-ba01-4064-9507-19613e194fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', content_df['content'][0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(text_content[1]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_content[1])))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Kate, this trade has an expiry date of 2001 when it is suppose to be 2003. \n",
            "Mike Driskil entered the trade. I have BPA's confirm that also verifies and \n",
            "the notes in the bottom of the comment section. Please change if correct.\n",
            "\n",
            "Thanks,\n",
            "\n",
            "Kimberly\n",
            "Tokenized:  ['mike', 'dr', '##isk', '##il', 'entered', 'the', 'trade']\n",
            "Token IDs:  [3505, 2852, 20573, 4014, 3133, 1996, 3119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH03C0KuG1pU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30600518-3040-4603-ce37-c81447f75a20"
      },
      "source": [
        "len(text_content)  #2411 sentences"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "677"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syJqBhZKe3S9",
        "colab_type": "text"
      },
      "source": [
        "When we actually convert all of our sentences, we'll use the tokenize.encode function to handle both steps, rather than calling tokenize and convert_tokens_to_ids separately.\n",
        "\n",
        "Before we can do that, though, we need to talk about some of BERT's formatting requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_OcHWoNfSp_",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Required Formatting\n",
        "\n",
        "The above code left out a few required formatting steps that we'll look at here.\n",
        "\n",
        "Side Note: The input format to BERT seems \"over-specified\" to me... We are required to give it a number of pieces of information which seem redundant, or like they could easily be inferred from the data without us explicity providing it. But it is what it is, and I suspect it will make more sense once I have a deeper understanding of the BERT internals.\n",
        "\n",
        "We are required to:\n",
        "\n",
        "1. Add special tokens to the start and end of each sentence.\n",
        "2. Pad & truncate all sentences to a single constant length.\n",
        "3. Explicitly differentiate real tokens from padding tokens with the \"attention mask\".\n",
        "\n",
        "#### Special tokens\n",
        "* [SEP] at the end of every sentence\n",
        "* [CLS] for classification tasks -- the beginning of every sentence.\n",
        "\n",
        "#### Sentence Length & Attention Mask\n",
        "The sentences in our dataset obviously have varying lengths, so how does BERT handle this?\n",
        "\n",
        "BERT has two constraints:\n",
        "\n",
        "1. All sentences must be padded or truncated to a single, fixed length.\n",
        "2. The maximum sentence length is 512 tokens.\n",
        "\n",
        "Padding is done with a special [PAD] token, which is at index 0 in the BERT vocabulary. The below illustration demonstrates padding out to a \"MAX_LEN\" of 8 tokens.\n",
        "\n",
        "The \"Attention Mask\" is simply an array of 1s and 0s indicating which tokens are padding and which aren't (seems kind of redundant, doesn't it?!). This mask tells the \"Self-Attention\" mechanism in BERT not to incorporate these PAD tokens into its interpretation of the sentence.\n",
        "\n",
        "The maximum length does impact training and evaluation speed, however. For example, with a Tesla K80:\n",
        "\n",
        "MAX_LEN = 128 --> Training epochs take ~5:28 each\n",
        "\n",
        "MAX_LEN = 64 --> Training epochs take ~2:57 each"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiaY0Rl6ZrdT",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 Tokenize Dataset\n",
        "The transformers library provides a helpful encode function which will handle most of the parsing and data prep steps for us.\n",
        "\n",
        "Before we are ready to encode our text, though, we need to decide on a maximum sentence length for padding / truncating to.\n",
        "\n",
        "The below cell will perform one tokenization pass of the dataset in order to measure the maximum sentence length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akPXsSttZvHH",
        "colab_type": "code",
        "outputId": "59a90109-1dfe-4e5d-ef79-36afddaf3124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in text_content:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nENDqtifaFbX",
        "colab_type": "text"
      },
      "source": [
        "As there are some longer test sentences, I'll set the maximum length to 500.\n",
        "\n",
        "Now we're ready to perform the real tokenization.\n",
        "\n",
        "The tokenizer.encode_plus function combines multiple steps for us:\n",
        "\n",
        "1. Split the sentence into tokens.\n",
        "2. Add the special [CLS] and [SEP] tokens.\n",
        "3. Map the tokens to their IDs.\n",
        "4. Pad or truncate all sentences to the same length.\n",
        "5. Create the attention masks which explicitly differentiate real tokens from [PAD] tokens.\n",
        "\n",
        "The first four features are in tokenizer.encode, but I'm using tokenizer.encode_plus to get the fifth item (attention masks). Documentation is [here](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yHWEcD5aW_m",
        "colab_type": "code",
        "outputId": "da93598b-1673-4291-cb79-23311df7551b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in text_content:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 100,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "token_tensor = torch.cat(input_ids, dim=0)\n",
        "attention_masks_tensor = torch.cat(attention_masks, dim=0)\n",
        "#labels = torch.tensor(labels)  # currently, not using labels (won't evaluate it... using just pre-trained model)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', text_content[2])\n",
        "print('Token IDs:', token_tensor[2])\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:   if you would prefer  receive a hard copy of the newsletter in the mail, rather than viewing it on the website, please contact us\n",
            "Token IDs: tensor([  101,  2065,  2017,  2052,  9544,  4374,  1037,  2524,  6100,  1997,\n",
            "         1996, 17178,  1999,  1996,  5653,  1010,  2738,  2084, 10523,  2009,\n",
            "         2006,  1996,  4037,  1010,  3531,  3967,  2149,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YdfCtP-gjtG",
        "colab_type": "code",
        "outputId": "21b10748-6ef4-4626-9fca-742f268fc343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print (len(attention_masks_tensor)) #73725 sentences for Kate Symes  #42121 sentences for Phillip Love\n",
        "# after subsetting into sent mails, 18865 sentences for Phillip Love\n",
        "print (len(token_tensor[2]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "677\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7IBEE1Jg-T8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvodT7z1hjMY",
        "colab_type": "code",
        "outputId": "c4df3727-a984-483c-e358-a0308c504d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "segment_ids = get_segments(token_tensor, 100) "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f3be61cd9b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msegment_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_segments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-51e936c5a624>\u001b[0m in \u001b[0;36mget_segments\u001b[0;34m(tokens, max_seq_length)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Token length more than max seq length!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcurrent_segment_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Token length more than max seq length!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSieur4AAudc",
        "colab_type": "text"
      },
      "source": [
        "## 3.4. Tokenizing in JSON file to extract attentions (link [here](https://github.com/clarkkev/attention-analysis/blob/master/preprocess_unlabeled.py))\n",
        "\n",
        "Don't think I will use this method. \n",
        "Just use the plain way--and use the config.attention... function to extract attention matrices and summarize!! \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgjpCdEHfwP2",
        "colab_type": "text"
      },
      "source": [
        "# 4. Extracting Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB9FeKRwfyhO",
        "colab_type": "text"
      },
      "source": [
        "## 4.1. Running BERT on our text\n",
        "Next we need to convert our data to torch tensors and call the BERT model. The BERT PyTorch interface requires that the data be in torch tensors rather than Python lists, so we convert the lists here - this does not change the shape or the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC9NFuXKKkI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Detaching the emails dataset for the memory space before running BERT\n",
        "del emails_df "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UENYAez7gFje",
        "colab_type": "text"
      },
      "source": [
        "Calling from_pretrained will fetch the model from the internet. When we load the bert-base-uncased, we see the definition of the model printed in the logging. The model is a deep neural network with 12 layers! Explaining the layers and their functions is outside the scope of this post, and you can skip over this output for now.\n",
        "\n",
        "model.eval() puts our model in evaluation mode as opposed to training mode. In this case, evaluation mode turns off dropout regularization which is used in training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Alk4SwcgJWY",
        "colab_type": "code",
        "outputId": "2d327aba-7563-4566-812c-ee07b2f6b926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True, # whether the model returns all hidden-states.\n",
        "                                  output_attentions=True,  # whether attention layer is returned\n",
        "                                  )\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation\n",
        "model.eval()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk0pfcc6oojt",
        "colab_type": "text"
      },
      "source": [
        "Next, let's evaluate BERT on our text in each account, and fetch the hidden states of the network!\n",
        "\n",
        "> Side note: torch.no_grad tells PyTorch not to construct the compute graph during this forward pass (since we won't be running backprop here)--this just reduces memory consumption and speeds things up a little."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jKEdtaLop0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run the text through BERT, and collect all of the hidden states produced\n",
        "# from all 12 layers. \n",
        "with torch.no_grad():\n",
        "\n",
        "    outputs = model(token_tensor)   # currently, ignoring the segment tensor\n",
        "\n",
        "    # Evaluating the model will return a different number of objects based on \n",
        "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
        "    # becase we set `output_hidden_states = True`, the third item will be the \n",
        "    # hidden states from all layers. See the documentation for more details:\n",
        "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "    hidden_states = outputs[2]  # this is the hidden state info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xNqxuVNW8Z9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f786a89-20f5-4d1d-c14a-4d616fb3e3aa"
      },
      "source": [
        "hidden_states"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
              "            3.8253e-02,  1.6400e-01],\n",
              "          [ 2.5102e-01, -1.3555e+00, -5.1271e-01,  ...,  4.3443e-01,\n",
              "           -5.2151e-01,  5.9567e-01],\n",
              "          [ 7.7984e-01,  5.3886e-01,  1.9515e-01,  ...,  5.5381e-01,\n",
              "            1.0014e+00, -8.7890e-01],\n",
              "          ...,\n",
              "          [ 5.8332e-01, -1.9910e-01,  8.1386e-01,  ..., -2.4236e-01,\n",
              "            3.5850e-01,  1.5621e-01],\n",
              "          [-2.4192e-01,  5.2873e-01,  5.1534e-01,  ...,  3.7666e-02,\n",
              "            5.9429e-01,  7.3446e-01],\n",
              "          [-4.4828e-01,  4.6353e-01,  3.9365e-01,  ..., -5.3819e-01,\n",
              "           -9.1579e-02, -3.9472e-02]],\n",
              " \n",
              "         [[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
              "            3.8253e-02,  1.6400e-01],\n",
              "          [ 5.4667e-01,  3.3008e-01, -9.2265e-01,  ...,  9.1499e-01,\n",
              "            8.3508e-01, -2.4775e-01],\n",
              "          [-6.2884e-01,  4.4862e-01,  6.3140e-01,  ...,  5.9740e-01,\n",
              "            5.3136e-01,  2.4762e-01],\n",
              "          ...,\n",
              "          [ 7.4217e-03, -3.4600e-01,  5.9140e-01,  ..., -4.2907e-01,\n",
              "           -1.6266e-01,  2.5802e-01],\n",
              "          [ 2.7745e-02, -3.2084e-01,  4.6328e-01,  ..., -2.3633e-01,\n",
              "           -2.0553e-01,  2.4353e-01],\n",
              "          [ 7.1794e-02, -1.0214e-01,  2.0979e-01,  ..., -9.0257e-02,\n",
              "           -4.9573e-01,  2.4432e-01]],\n",
              " \n",
              "         [[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
              "            3.8253e-02,  1.6400e-01],\n",
              "          [-3.4061e-01,  7.0248e-01, -6.4828e-01,  ...,  2.2401e-01,\n",
              "            7.5120e-01,  2.3857e-01],\n",
              "          [-6.2884e-01,  4.4862e-01,  6.3140e-01,  ...,  5.9740e-01,\n",
              "            5.3136e-01,  2.4762e-01],\n",
              "          ...,\n",
              "          [ 7.4217e-03, -3.4600e-01,  5.9140e-01,  ..., -4.2907e-01,\n",
              "           -1.6266e-01,  2.5802e-01],\n",
              "          [ 2.7745e-02, -3.2084e-01,  4.6328e-01,  ..., -2.3633e-01,\n",
              "           -2.0553e-01,  2.4353e-01],\n",
              "          [ 7.1794e-02, -1.0214e-01,  2.0979e-01,  ..., -9.0257e-02,\n",
              "           -4.9573e-01,  2.4432e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
              "            3.8253e-02,  1.6400e-01],\n",
              "          [ 2.4835e-01,  5.5139e-01, -3.9806e-01,  ...,  1.0440e-01,\n",
              "            8.7286e-01, -5.1980e-01],\n",
              "          [ 4.6709e-04,  1.6225e-01, -6.4443e-02,  ...,  4.9443e-01,\n",
              "            6.9413e-01,  3.6286e-01],\n",
              "          ...,\n",
              "          [ 7.4217e-03, -3.4600e-01,  5.9140e-01,  ..., -4.2907e-01,\n",
              "           -1.6266e-01,  2.5802e-01],\n",
              "          [ 2.7745e-02, -3.2084e-01,  4.6328e-01,  ..., -2.3633e-01,\n",
              "           -2.0553e-01,  2.4353e-01],\n",
              "          [ 7.1794e-02, -1.0214e-01,  2.0979e-01,  ..., -9.0257e-02,\n",
              "           -4.9573e-01,  2.4432e-01]],\n",
              " \n",
              "         [[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
              "            3.8253e-02,  1.6400e-01],\n",
              "          [-3.4061e-01,  7.0248e-01, -6.4828e-01,  ...,  2.2401e-01,\n",
              "            7.5120e-01,  2.3857e-01],\n",
              "          [-6.2884e-01,  4.4862e-01,  6.3140e-01,  ...,  5.9740e-01,\n",
              "            5.3136e-01,  2.4762e-01],\n",
              "          ...,\n",
              "          [ 7.4217e-03, -3.4600e-01,  5.9140e-01,  ..., -4.2907e-01,\n",
              "           -1.6266e-01,  2.5802e-01],\n",
              "          [ 2.7745e-02, -3.2084e-01,  4.6328e-01,  ..., -2.3633e-01,\n",
              "           -2.0553e-01,  2.4353e-01],\n",
              "          [ 7.1794e-02, -1.0214e-01,  2.0979e-01,  ..., -9.0257e-02,\n",
              "           -4.9573e-01,  2.4432e-01]],\n",
              " \n",
              "         [[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
              "            3.8253e-02,  1.6400e-01],\n",
              "          [-2.2891e-01, -1.3277e-01, -3.8282e-01,  ...,  4.8968e-01,\n",
              "            1.2217e+00, -6.2570e-01],\n",
              "          [ 2.7278e-01,  2.5008e-02,  2.4040e-01,  ...,  6.9014e-01,\n",
              "            5.9034e-01,  1.2269e-01],\n",
              "          ...,\n",
              "          [ 7.4217e-03, -3.4600e-01,  5.9140e-01,  ..., -4.2907e-01,\n",
              "           -1.6266e-01,  2.5802e-01],\n",
              "          [ 2.7745e-02, -3.2084e-01,  4.6328e-01,  ..., -2.3633e-01,\n",
              "           -2.0553e-01,  2.4353e-01],\n",
              "          [ 7.1794e-02, -1.0214e-01,  2.0979e-01,  ..., -9.0257e-02,\n",
              "           -4.9573e-01,  2.4432e-01]]]),\n",
              " tensor([[[ 9.8090e-02, -9.4412e-02, -2.2304e-01,  ...,  1.9290e-01,\n",
              "           -2.2194e-01, -9.8043e-02],\n",
              "          [ 2.1231e-01, -1.6219e+00, -1.2017e+00,  ...,  8.2448e-01,\n",
              "           -4.7486e-01,  8.2890e-01],\n",
              "          [ 6.2011e-01, -4.8434e-01, -1.1286e-01,  ...,  7.1804e-01,\n",
              "            1.0495e+00, -8.1551e-01],\n",
              "          ...,\n",
              "          [ 8.0938e-01, -4.2518e-01,  9.0779e-01,  ..., -2.2736e-01,\n",
              "           -5.0902e-01, -1.4900e-01],\n",
              "          [ 1.7695e-01, -1.5820e-01,  6.7958e-02,  ..., -8.3534e-02,\n",
              "            1.1594e-01,  1.2972e+00],\n",
              "          [-3.0728e-01,  3.1357e-02,  6.0797e-02,  ..., -2.0825e-01,\n",
              "           -3.2563e-01, -1.1141e-01]],\n",
              " \n",
              "         [[ 2.1163e-01, -8.9361e-02, -4.1263e-02,  ...,  1.0470e-01,\n",
              "            9.2672e-02,  8.9617e-02],\n",
              "          [ 5.0650e-01, -3.4353e-01, -7.2688e-01,  ...,  1.0633e+00,\n",
              "            5.9021e-01, -1.2075e-01],\n",
              "          [-1.2982e+00, -4.6132e-01,  4.0019e-01,  ...,  6.7277e-01,\n",
              "            1.0698e-01,  1.1494e-01],\n",
              "          ...,\n",
              "          [ 3.0182e-01, -2.6693e-01,  4.9627e-01,  ...,  9.3976e-04,\n",
              "            1.2621e-01,  6.5110e-02],\n",
              "          [ 2.2722e-01, -2.4038e-01,  4.0151e-01,  ...,  1.6264e-01,\n",
              "            5.4024e-02,  2.0749e-02],\n",
              "          [ 3.4675e-01,  3.7212e-02,  1.6514e-01,  ...,  2.2783e-01,\n",
              "           -1.5599e-01,  1.4108e-02]],\n",
              " \n",
              "         [[ 6.1831e-02, -3.3899e-02, -1.2840e-01,  ...,  2.7087e-01,\n",
              "           -3.5462e-02, -3.9552e-03],\n",
              "          [-3.8381e-01,  7.8027e-01, -8.9096e-01,  ...,  9.0710e-02,\n",
              "            2.8900e-01,  1.3447e-01],\n",
              "          [-1.4023e+00, -7.6034e-01,  3.3787e-01,  ...,  6.6572e-01,\n",
              "            1.4914e-01,  1.7495e-01],\n",
              "          ...,\n",
              "          [ 3.0136e-01, -2.3121e-01,  4.7225e-01,  ..., -2.3469e-02,\n",
              "            1.6722e-01,  9.5730e-02],\n",
              "          [ 2.2620e-01, -2.0560e-01,  3.7710e-01,  ...,  1.4654e-01,\n",
              "            9.1443e-02,  4.8121e-02],\n",
              "          [ 3.4774e-01,  8.4205e-02,  1.2522e-01,  ...,  2.0854e-01,\n",
              "           -1.2569e-01,  4.3766e-02]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[ 8.7455e-02, -5.7181e-02, -1.2951e-01,  ...,  2.8686e-01,\n",
              "           -5.4754e-02, -1.8028e-02],\n",
              "          [ 1.4339e-01, -1.9578e-01, -3.5684e-01,  ...,  5.2188e-01,\n",
              "            1.0387e+00, -5.5389e-01],\n",
              "          [-8.0651e-01, -3.4829e-01, -2.5577e-01,  ...,  4.0873e-01,\n",
              "            7.5377e-01,  4.0172e-01],\n",
              "          ...,\n",
              "          [ 3.0059e-01, -2.3841e-01,  4.8033e-01,  ..., -2.4175e-02,\n",
              "            1.5300e-01,  8.6980e-02],\n",
              "          [ 2.2093e-01, -2.1165e-01,  3.8257e-01,  ...,  1.4769e-01,\n",
              "            7.5441e-02,  3.9805e-02],\n",
              "          [ 3.4285e-01,  7.2501e-02,  1.3399e-01,  ...,  2.1600e-01,\n",
              "           -1.3780e-01,  3.5177e-02]],\n",
              " \n",
              "         [[ 1.4245e-01, -1.1885e-01, -6.5535e-02,  ...,  2.7101e-01,\n",
              "           -4.5842e-02, -4.0018e-02],\n",
              "          [-2.0205e-01,  6.6770e-01, -7.2228e-01,  ...,  2.3463e-01,\n",
              "            2.1587e-01,  2.1646e-01],\n",
              "          [-1.3701e+00, -5.5116e-01,  4.2242e-01,  ...,  7.5704e-01,\n",
              "            6.9295e-02,  7.1187e-02],\n",
              "          ...,\n",
              "          [ 3.0630e-01, -2.6344e-01,  4.9311e-01,  ..., -7.5527e-03,\n",
              "            1.1242e-01,  5.3693e-02],\n",
              "          [ 2.3153e-01, -2.3776e-01,  4.0002e-01,  ...,  1.5613e-01,\n",
              "            4.1569e-02,  1.3640e-02],\n",
              "          [ 3.4909e-01,  3.7080e-02,  1.6457e-01,  ...,  2.2744e-01,\n",
              "           -1.6971e-01,  7.8654e-03]],\n",
              " \n",
              "         [[ 1.7887e-01, -1.8135e-01, -4.3186e-04,  ...,  1.1896e-01,\n",
              "            9.4295e-02,  9.1812e-03],\n",
              "          [-2.6729e-01, -3.7308e-01, -4.3475e-01,  ...,  6.1918e-01,\n",
              "            1.2093e+00, -7.1653e-01],\n",
              "          [ 1.2968e-01, -7.4605e-01, -2.2557e-01,  ...,  4.9887e-01,\n",
              "            5.6065e-01,  1.4529e-01],\n",
              "          ...,\n",
              "          [ 3.0811e-01, -2.6888e-01,  4.8623e-01,  ..., -8.0043e-03,\n",
              "            1.1547e-01,  5.8899e-02],\n",
              "          [ 2.3274e-01, -2.4298e-01,  3.9053e-01,  ...,  1.5545e-01,\n",
              "            4.2051e-02,  1.5469e-02],\n",
              "          [ 3.5016e-01,  3.3532e-02,  1.5639e-01,  ...,  2.2462e-01,\n",
              "           -1.6840e-01,  8.9980e-03]]]),\n",
              " tensor([[[ 1.2428e-01, -4.1009e-01, -3.4457e-01,  ...,  3.8982e-01,\n",
              "           -1.2138e-01, -1.8808e-01],\n",
              "          [ 2.8964e-01, -2.1485e+00, -1.2731e+00,  ...,  4.4637e-01,\n",
              "           -5.2852e-02,  3.0505e-01],\n",
              "          [ 6.3027e-01, -8.9389e-01,  7.3122e-01,  ...,  7.2811e-01,\n",
              "            1.3094e+00, -1.6334e+00],\n",
              "          ...,\n",
              "          [ 8.6349e-01, -6.9385e-01,  7.8448e-01,  ..., -5.5363e-01,\n",
              "           -1.6658e-01, -2.3671e-01],\n",
              "          [-3.3346e-01, -1.9147e-01,  3.3964e-01,  ...,  2.8120e-02,\n",
              "            1.0771e-01,  1.0859e+00],\n",
              "          [-1.1428e-01, -2.1687e-01, -1.9215e-02,  ..., -8.8972e-02,\n",
              "            6.3128e-02, -5.3679e-01]],\n",
              " \n",
              "         [[-6.1146e-03, -3.4565e-01,  5.8470e-02,  ...,  1.3070e-01,\n",
              "           -1.7234e-01,  7.0663e-03],\n",
              "          [ 5.8076e-01, -4.0999e-01, -3.0574e-01,  ...,  1.3070e+00,\n",
              "            3.2672e-01, -3.8171e-01],\n",
              "          [-1.4901e+00, -6.8442e-01,  6.5970e-01,  ...,  2.4517e-02,\n",
              "           -6.0581e-02,  2.0623e-01],\n",
              "          ...,\n",
              "          [ 6.2415e-01, -6.2872e-01,  1.1345e+00,  ..., -4.8556e-02,\n",
              "            2.2338e-01,  1.6754e-01],\n",
              "          [ 3.6881e-01, -5.4096e-01,  9.4966e-01,  ...,  1.5684e-01,\n",
              "            9.9741e-02,  6.1318e-02],\n",
              "          [ 7.1596e-01, -1.3836e-01,  7.8325e-01,  ...,  2.6301e-01,\n",
              "           -2.2459e-01,  6.6929e-02]],\n",
              " \n",
              "         [[-2.3908e-02, -2.8441e-01, -1.9466e-01,  ...,  2.3227e-01,\n",
              "           -1.9448e-01, -9.3181e-02],\n",
              "          [-3.5436e-01,  1.0140e+00, -2.8283e-01,  ...,  3.0699e-01,\n",
              "           -2.7644e-01,  5.3358e-02],\n",
              "          [-1.7123e+00, -9.8728e-01,  6.6428e-01,  ...,  1.4671e-01,\n",
              "           -4.1697e-02,  2.7244e-01],\n",
              "          ...,\n",
              "          [ 6.3751e-01, -6.3312e-01,  1.0147e+00,  ...,  2.5726e-02,\n",
              "            3.3041e-01,  1.9495e-01],\n",
              "          [ 5.1713e-01, -5.4087e-01,  9.0210e-01,  ...,  2.0474e-01,\n",
              "            1.9146e-01,  4.8565e-02],\n",
              "          [ 6.9088e-01, -1.5771e-01,  6.6007e-01,  ...,  3.2843e-01,\n",
              "           -1.7847e-01,  3.9477e-02]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-8.2618e-02, -3.1777e-01, -1.7613e-01,  ...,  2.5728e-01,\n",
              "           -2.9551e-01, -1.3225e-01],\n",
              "          [ 1.0988e-01,  2.1462e-01, -1.2762e-01,  ...,  8.9311e-01,\n",
              "            1.2889e+00, -1.0177e+00],\n",
              "          [-8.9571e-01, -9.2447e-02, -6.8078e-02,  ...,  3.5369e-01,\n",
              "           -6.0237e-02,  2.0662e-01],\n",
              "          ...,\n",
              "          [ 5.9383e-01, -7.1511e-01,  1.1219e+00,  ..., -2.0206e-02,\n",
              "            2.3938e-01,  1.8649e-01],\n",
              "          [ 4.6168e-01, -5.8243e-01,  1.0048e+00,  ...,  1.5950e-01,\n",
              "            1.1709e-01,  4.1945e-02],\n",
              "          [ 6.1267e-01, -2.1863e-01,  7.6612e-01,  ...,  2.8176e-01,\n",
              "           -2.4619e-01,  3.1578e-02]],\n",
              " \n",
              "         [[ 8.3494e-04, -4.4885e-01, -6.5732e-02,  ...,  3.0732e-01,\n",
              "           -4.3916e-01, -8.6963e-02],\n",
              "          [-2.2325e-01,  1.1117e+00,  3.8672e-02,  ...,  3.9581e-01,\n",
              "           -4.1519e-01, -7.9112e-02],\n",
              "          [-1.9940e+00, -6.3638e-01,  5.7199e-01,  ...,  4.9479e-01,\n",
              "           -4.0256e-01,  3.4742e-01],\n",
              "          ...,\n",
              "          [ 6.5554e-01, -8.1657e-01,  1.0714e+00,  ...,  1.7255e-02,\n",
              "            1.8548e-01,  2.1520e-01],\n",
              "          [ 6.5641e-01, -6.7396e-01,  9.8624e-01,  ...,  2.0338e-01,\n",
              "            8.9989e-02,  1.2253e-01],\n",
              "          [ 7.9988e-01, -2.9912e-01,  7.3390e-01,  ...,  3.3325e-01,\n",
              "           -2.7148e-01,  1.2016e-01]],\n",
              " \n",
              "         [[ 1.8232e-02, -5.1657e-01,  1.2159e-01,  ...,  2.8189e-01,\n",
              "           -2.4579e-01, -2.6987e-02],\n",
              "          [-3.8279e-01, -2.0734e-01, -4.3412e-01,  ...,  1.0809e+00,\n",
              "            1.4506e+00, -5.8736e-01],\n",
              "          [ 4.1138e-01, -1.1072e+00, -4.0037e-01,  ...,  5.6597e-01,\n",
              "            9.4993e-01, -2.6398e-02],\n",
              "          ...,\n",
              "          [ 7.0686e-01, -7.7178e-01,  1.1032e+00,  ..., -2.1670e-02,\n",
              "            1.5540e-01,  1.7993e-01],\n",
              "          [ 5.7043e-01, -6.5601e-01,  1.0072e+00,  ...,  1.3051e-01,\n",
              "            1.0614e-02,  4.3354e-02],\n",
              "          [ 7.4379e-01, -3.3032e-01,  7.7839e-01,  ...,  2.2709e-01,\n",
              "           -3.0671e-01,  2.9387e-02]]]),\n",
              " tensor([[[ 0.2002, -0.3555, -0.1635,  ...,  0.3893,  0.1020,  0.0566],\n",
              "          [ 0.4498, -1.6764, -0.9905,  ..., -0.0226,  0.1379,  0.8496],\n",
              "          [ 0.9168, -1.0000,  0.7939,  ...,  0.4955,  1.1987, -1.0932],\n",
              "          ...,\n",
              "          [ 1.2848, -0.5167,  0.7728,  ..., -0.4395, -0.3317, -0.1346],\n",
              "          [-0.3167,  0.0369,  0.3878,  ..., -0.1839, -0.2693,  0.7859],\n",
              "          [-0.0643, -0.1019,  0.0651,  ...,  0.0406,  0.0483, -0.0892]],\n",
              " \n",
              "         [[-0.0103, -0.1808,  0.0651,  ..., -0.0184, -0.1442,  0.0216],\n",
              "          [ 0.4542, -0.3419, -0.1444,  ...,  0.8697, -0.2416, -0.8302],\n",
              "          [-1.4030, -0.4400,  1.0180,  ...,  0.2091,  0.0264, -0.1114],\n",
              "          ...,\n",
              "          [ 0.5394, -0.9608,  1.0210,  ..., -0.1139,  0.1446,  0.3751],\n",
              "          [ 0.2552, -0.8258,  0.8117,  ...,  0.0778, -0.0252,  0.2140],\n",
              "          [ 0.5065, -0.4918,  0.5487,  ...,  0.0902, -0.3410,  0.2178]],\n",
              " \n",
              "         [[-0.0408, -0.1971, -0.1121,  ...,  0.1704, -0.0795,  0.0034],\n",
              "          [-0.5334,  0.6323,  0.0602,  ...,  0.2695, -0.5748, -0.1514],\n",
              "          [-1.3424, -0.4075,  0.9462,  ...,  0.0927, -0.1316, -0.1315],\n",
              "          ...,\n",
              "          [ 0.5114, -0.8755,  0.8283,  ..., -0.0128,  0.3156,  0.3307],\n",
              "          [ 0.4278, -0.7875,  0.7251,  ...,  0.1352,  0.1838,  0.1658],\n",
              "          [ 0.5560, -0.5009,  0.4886,  ...,  0.1971, -0.1954,  0.1581]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-0.0865, -0.2224, -0.0805,  ...,  0.1735, -0.1622, -0.0102],\n",
              "          [ 0.4778, -0.0878,  0.3969,  ...,  0.5010,  1.2220, -0.7668],\n",
              "          [-0.8256,  0.2744,  0.2413,  ..., -0.1358,  0.0232,  0.0925],\n",
              "          ...,\n",
              "          [ 0.5078, -0.9857,  0.9735,  ..., -0.1327,  0.2377,  0.3638],\n",
              "          [ 0.4321, -0.8419,  0.8563,  ...,  0.0082,  0.1133,  0.1821],\n",
              "          [ 0.5246, -0.5427,  0.6368,  ...,  0.0862, -0.2634,  0.1769]],\n",
              " \n",
              "         [[-0.0744, -0.2777, -0.0104,  ...,  0.1594, -0.3111,  0.0130],\n",
              "          [-0.3120,  0.6343,  0.3566,  ...,  0.2619, -0.8116, -0.3769],\n",
              "          [-1.4201, -0.3650,  0.5270,  ...,  1.2007, -0.8747, -0.0849],\n",
              "          ...,\n",
              "          [ 0.5189, -1.1183,  0.8901,  ..., -0.1827,  0.0368,  0.4456],\n",
              "          [ 0.4901, -0.9856,  0.7879,  ..., -0.0552, -0.0815,  0.2673],\n",
              "          [ 0.5704, -0.6411,  0.4872,  ...,  0.0475, -0.4393,  0.2144]],\n",
              " \n",
              "         [[-0.0229, -0.2704,  0.1198,  ...,  0.0685, -0.2489,  0.0093],\n",
              "          [-0.2114,  0.0489, -0.0035,  ...,  1.0904,  0.7048, -0.7361],\n",
              "          [ 0.6361, -0.5388, -0.7258,  ...,  0.8344,  0.2002, -0.0097],\n",
              "          ...,\n",
              "          [ 0.5036, -1.0322,  0.8879,  ..., -0.1666,  0.0480,  0.4194],\n",
              "          [ 0.4244, -0.9089,  0.7978,  ..., -0.0397, -0.1280,  0.2281],\n",
              "          [ 0.5547, -0.6746,  0.5376,  ...,  0.0255, -0.4205,  0.2357]]]),\n",
              " tensor([[[ 0.3251, -0.6821, -0.3999,  ...,  0.2893,  0.1526,  0.2897],\n",
              "          [ 0.5851, -2.0242, -0.4260,  ..., -0.0133, -0.0811,  0.9301],\n",
              "          [ 1.0006, -1.0083,  0.4602,  ...,  0.3237,  1.5100, -1.0801],\n",
              "          ...,\n",
              "          [ 1.3331, -0.8203,  0.7087,  ..., -0.2262, -0.6932,  0.3080],\n",
              "          [-0.4679, -0.3017,  0.4207,  ...,  0.1347, -0.2922,  1.1752],\n",
              "          [-0.0180, -0.0669,  0.0186,  ...,  0.0047,  0.0522, -0.0373]],\n",
              " \n",
              "         [[ 0.0389, -0.5931, -0.3182,  ..., -0.1035,  0.0304,  0.3846],\n",
              "          [ 0.2809, -0.7129, -0.4276,  ...,  0.5653,  0.4045, -0.4916],\n",
              "          [-1.7063, -0.4930,  0.9245,  ...,  0.1539,  0.6701, -0.0362],\n",
              "          ...,\n",
              "          [ 0.5187, -1.0882,  1.1107,  ...,  0.1393,  0.3497,  0.6510],\n",
              "          [ 0.1059, -0.8800,  0.9519,  ...,  0.2719,  0.1489,  0.5441],\n",
              "          [ 0.3858, -0.5524,  0.7995,  ...,  0.2325, -0.0728,  0.5528]],\n",
              " \n",
              "         [[ 0.0517, -0.7011, -0.4410,  ..., -0.0428,  0.2068,  0.3656],\n",
              "          [-0.2356,  0.5430, -0.0493,  ...,  0.3274, -0.1609,  0.1420],\n",
              "          [-1.4167, -0.3768,  1.2485,  ..., -0.1070,  0.6205, -0.4134],\n",
              "          ...,\n",
              "          [ 0.5374, -0.8976,  0.9874,  ...,  0.1228,  0.4485,  0.7137],\n",
              "          [ 0.3593, -0.7631,  0.8791,  ...,  0.1821,  0.2865,  0.5920],\n",
              "          [ 0.5222, -0.4441,  0.6637,  ...,  0.1885, -0.0348,  0.4678]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[ 0.0100, -0.7934, -0.4204,  ..., -0.0128,  0.1658,  0.2855],\n",
              "          [ 0.5232, -0.6869,  0.1410,  ...,  0.1358,  1.1057, -0.6392],\n",
              "          [-0.7945, -0.0177, -0.1402,  ...,  0.6907,  0.8006,  0.5867],\n",
              "          ...,\n",
              "          [ 0.5365, -1.0075,  1.0959,  ...,  0.0550,  0.4228,  0.8048],\n",
              "          [ 0.3656, -0.8492,  0.9503,  ...,  0.0915,  0.2538,  0.6265],\n",
              "          [ 0.5384, -0.4962,  0.7165,  ...,  0.0820, -0.1170,  0.5137]],\n",
              " \n",
              "         [[-0.0495, -0.8062, -0.3877,  ...,  0.0082,  0.0342,  0.3460],\n",
              "          [-0.2405,  0.4732,  0.3257,  ...,  0.5182, -0.3688, -0.0093],\n",
              "          [-1.6038, -0.4001,  0.7142,  ...,  0.6978, -0.1350, -0.3918],\n",
              "          ...,\n",
              "          [ 0.5151, -1.1478,  1.0565,  ...,  0.1242,  0.2094,  0.7923],\n",
              "          [ 0.4091, -1.0275,  0.9668,  ...,  0.2192,  0.0904,  0.6654],\n",
              "          [ 0.4959, -0.6877,  0.5289,  ...,  0.1192, -0.1433,  0.5323]],\n",
              " \n",
              "         [[-0.0295, -0.6921, -0.3111,  ..., -0.0544,  0.0233,  0.3303],\n",
              "          [ 0.1609,  0.2315,  0.2925,  ...,  1.0664,  1.3488, -0.9238],\n",
              "          [ 0.3107, -0.6469, -1.1769,  ...,  0.7123,  0.6673,  0.0039],\n",
              "          ...,\n",
              "          [ 0.4464, -0.9945,  0.9492,  ...,  0.0572,  0.4253,  0.7767],\n",
              "          [ 0.2850, -0.8752,  0.8502,  ...,  0.0917,  0.2262,  0.6359],\n",
              "          [ 0.4585, -0.6165,  0.6396,  ...,  0.0559, -0.0336,  0.5605]]]),\n",
              " tensor([[[-2.5427e-02, -9.1225e-01, -6.0903e-02,  ...,  3.0499e-02,\n",
              "            2.6829e-01,  4.2117e-01],\n",
              "          [ 2.2231e-01, -2.1115e+00, -6.1254e-01,  ...,  5.0031e-02,\n",
              "           -3.2126e-01,  7.1247e-01],\n",
              "          [ 1.0450e+00, -1.3899e+00,  9.5089e-02,  ...,  3.0521e-01,\n",
              "            1.3791e+00, -1.3701e+00],\n",
              "          ...,\n",
              "          [ 1.1224e+00, -6.5231e-01,  1.1582e+00,  ...,  5.6972e-02,\n",
              "           -4.9254e-01,  1.7029e-01],\n",
              "          [-1.8289e-02, -5.5064e-01,  7.8353e-01,  ..., -8.6808e-02,\n",
              "           -6.8551e-01,  8.8796e-01],\n",
              "          [-1.5597e-02, -4.9574e-02,  3.6889e-02,  ...,  1.3057e-02,\n",
              "            1.2891e-02, -4.2152e-02]],\n",
              " \n",
              "         [[-1.6720e-01, -7.6311e-01, -1.6173e-01,  ...,  2.0666e-01,\n",
              "            1.6781e-01,  2.9275e-01],\n",
              "          [ 6.6571e-01, -3.0280e-01, -4.7907e-01,  ...,  8.2472e-01,\n",
              "            3.6649e-01, -6.1927e-01],\n",
              "          [-1.2482e+00, -5.5441e-01,  1.0988e+00,  ...,  9.3784e-01,\n",
              "            7.1288e-01,  6.8893e-02],\n",
              "          ...,\n",
              "          [ 7.0901e-01, -5.0654e-01,  1.1023e+00,  ...,  2.2353e-01,\n",
              "            4.4192e-01,  5.6095e-01],\n",
              "          [ 2.6227e-01, -3.7975e-01,  9.7992e-01,  ...,  3.4262e-01,\n",
              "            2.9912e-01,  4.8622e-01],\n",
              "          [ 4.9599e-01, -3.2865e-02,  8.3695e-01,  ...,  3.4504e-01,\n",
              "            7.4651e-02,  4.1869e-01]],\n",
              " \n",
              "         [[-1.7692e-01, -7.5098e-01, -2.8154e-01,  ...,  2.8768e-01,\n",
              "            1.7018e-01,  3.9481e-01],\n",
              "          [-7.1403e-02,  4.2184e-01, -1.3790e-03,  ...,  6.5933e-01,\n",
              "           -4.9502e-01,  5.6484e-01],\n",
              "          [-1.4885e+00, -1.2671e-01,  1.0248e+00,  ...,  2.8849e-01,\n",
              "            6.3528e-01, -2.9859e-01],\n",
              "          ...,\n",
              "          [ 8.4850e-01, -3.0941e-01,  1.0125e+00,  ...,  1.3422e-01,\n",
              "            5.6898e-01,  6.9620e-01],\n",
              "          [ 6.1732e-01, -3.2134e-01,  8.9986e-01,  ...,  2.4902e-01,\n",
              "            3.5845e-01,  6.7490e-01],\n",
              "          [ 7.4160e-01, -1.0910e-02,  6.8252e-01,  ...,  2.6520e-01,\n",
              "            7.9877e-02,  5.3061e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-2.1730e-01, -7.8304e-01, -2.9272e-01,  ...,  3.3060e-01,\n",
              "            2.4142e-01,  3.4742e-01],\n",
              "          [ 6.7508e-01, -5.8977e-01,  5.0367e-03,  ...,  1.9265e-01,\n",
              "            1.1707e+00, -6.0837e-01],\n",
              "          [-7.8417e-01, -3.7350e-02,  3.9959e-02,  ...,  5.2580e-01,\n",
              "            1.0587e+00,  5.9785e-01],\n",
              "          ...,\n",
              "          [ 8.1428e-01, -4.3107e-01,  1.0576e+00,  ...,  1.2576e-01,\n",
              "            5.6969e-01,  7.9428e-01],\n",
              "          [ 6.1464e-01, -4.2039e-01,  9.0917e-01,  ...,  2.2334e-01,\n",
              "            3.7528e-01,  7.0675e-01],\n",
              "          [ 7.3629e-01, -6.9120e-02,  6.5876e-01,  ...,  2.1454e-01,\n",
              "            8.3486e-02,  5.6053e-01]],\n",
              " \n",
              "         [[-2.5293e-01, -8.8681e-01, -2.7831e-01,  ...,  3.8535e-01,\n",
              "            1.7717e-01,  3.4164e-01],\n",
              "          [-1.8054e-01,  2.6347e-01,  3.9295e-01,  ...,  7.7844e-01,\n",
              "           -3.7876e-01,  1.0463e-01],\n",
              "          [-1.3558e+00, -3.6609e-01,  5.3537e-01,  ...,  8.7223e-01,\n",
              "            1.2865e-01, -2.5961e-01],\n",
              "          ...,\n",
              "          [ 7.7425e-01, -5.5076e-01,  1.0214e+00,  ...,  3.0903e-01,\n",
              "            4.0004e-01,  7.8157e-01],\n",
              "          [ 6.0823e-01, -5.0047e-01,  9.3573e-01,  ...,  4.2714e-01,\n",
              "            1.7735e-01,  6.9932e-01],\n",
              "          [ 6.7255e-01, -2.2264e-01,  5.4851e-01,  ...,  3.2386e-01,\n",
              "            6.5056e-02,  5.1541e-01]],\n",
              " \n",
              "         [[-1.7802e-01, -8.9069e-01, -1.7319e-01,  ...,  2.9686e-01,\n",
              "            2.4216e-01,  2.8281e-01],\n",
              "          [ 6.2665e-01,  5.4123e-01,  1.0728e-01,  ...,  1.7621e+00,\n",
              "            2.0323e+00, -8.6807e-01],\n",
              "          [ 1.3772e-01, -5.8124e-01, -1.2327e+00,  ...,  5.9669e-01,\n",
              "            1.1260e+00, -4.3243e-01],\n",
              "          ...,\n",
              "          [ 6.9970e-01, -5.0794e-01,  9.5416e-01,  ...,  1.8009e-01,\n",
              "            5.5712e-01,  7.5645e-01],\n",
              "          [ 4.9113e-01, -5.0401e-01,  8.9471e-01,  ...,  2.7705e-01,\n",
              "            3.6049e-01,  7.0534e-01],\n",
              "          [ 6.0784e-01, -2.5853e-01,  7.1835e-01,  ...,  3.0261e-01,\n",
              "            1.6845e-01,  5.8721e-01]]]),\n",
              " tensor([[[ 1.5230e-01, -1.1541e+00, -1.9242e-01,  ..., -1.5201e-01,\n",
              "            5.7114e-01,  1.9194e-01],\n",
              "          [ 1.4038e-01, -2.3843e+00, -3.6855e-01,  ..., -3.0713e-01,\n",
              "           -6.9971e-02,  5.7315e-01],\n",
              "          [ 5.2072e-01, -1.8373e+00,  2.5591e-01,  ...,  3.4204e-01,\n",
              "            1.2437e+00, -8.7774e-01],\n",
              "          ...,\n",
              "          [ 9.7359e-01, -9.6361e-01,  1.1023e+00,  ...,  6.4672e-01,\n",
              "            9.8530e-02,  3.8244e-01],\n",
              "          [ 7.4254e-03,  4.3839e-02, -1.4521e-01,  ..., -2.6297e-01,\n",
              "           -7.7184e-01,  7.1117e-01],\n",
              "          [ 2.4518e-02, -4.7184e-02, -1.4434e-02,  ..., -1.0588e-02,\n",
              "            8.1167e-03, -4.4916e-02]],\n",
              " \n",
              "         [[-6.7934e-02, -1.6032e-01,  4.6201e-01,  ...,  2.7853e-02,\n",
              "            1.8441e-01, -1.8428e-01],\n",
              "          [ 7.7978e-01, -5.4357e-01, -4.4851e-01,  ...,  5.5222e-01,\n",
              "            5.5591e-01, -7.1115e-01],\n",
              "          [-1.0395e+00, -5.2553e-01,  1.1106e+00,  ...,  8.7745e-01,\n",
              "            7.2885e-01,  6.1899e-02],\n",
              "          ...,\n",
              "          [ 2.2222e-01, -3.5038e-01,  1.2471e+00,  ..., -2.6167e-01,\n",
              "            5.2998e-01,  5.5509e-03],\n",
              "          [-1.2718e-01, -2.8074e-01,  1.2146e+00,  ..., -2.7448e-01,\n",
              "            4.1461e-01, -9.4703e-02],\n",
              "          [ 9.2935e-02,  1.3729e-01,  1.1066e+00,  ..., -3.3631e-01,\n",
              "            2.7843e-01, -1.1773e-01]],\n",
              " \n",
              "         [[-1.3453e-01, -4.1363e-01,  2.2774e-01,  ...,  2.0988e-01,\n",
              "            2.3241e-01, -4.4460e-02],\n",
              "          [-3.9010e-01,  6.4449e-01, -2.0124e-01,  ..., -1.1344e-01,\n",
              "           -1.0261e+00,  7.9646e-01],\n",
              "          [-1.5012e+00, -7.0006e-02,  6.8015e-01,  ...,  1.4592e-01,\n",
              "            5.2770e-01, -1.5754e-01],\n",
              "          ...,\n",
              "          [ 4.0693e-01, -1.8961e-01,  1.0293e+00,  ..., -2.3580e-01,\n",
              "            5.7402e-01,  1.7868e-01],\n",
              "          [ 1.9933e-01, -2.1720e-01,  9.5856e-01,  ..., -1.5814e-01,\n",
              "            3.7313e-01,  1.6286e-01],\n",
              "          [ 3.7091e-01,  1.0470e-01,  7.7763e-01,  ..., -2.4700e-01,\n",
              "            1.7365e-01,  4.6639e-03]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-1.4179e-01, -4.0941e-01,  1.9653e-01,  ...,  2.0667e-01,\n",
              "            3.4180e-01, -6.7635e-02],\n",
              "          [ 5.9097e-01, -6.0792e-01,  3.7284e-02,  ...,  5.6224e-01,\n",
              "            8.4011e-01, -6.8557e-01],\n",
              "          [-3.3591e-01, -2.3030e-01,  9.4582e-02,  ..., -1.4236e-03,\n",
              "            1.4254e+00,  8.9217e-01],\n",
              "          ...,\n",
              "          [ 3.5415e-01, -2.9426e-01,  1.0609e+00,  ..., -2.2221e-01,\n",
              "            5.6628e-01,  2.3408e-01],\n",
              "          [ 1.8011e-01, -3.1043e-01,  9.5669e-01,  ..., -1.6087e-01,\n",
              "            3.8885e-01,  1.8686e-01],\n",
              "          [ 3.9278e-01,  7.3040e-02,  7.5814e-01,  ..., -2.9921e-01,\n",
              "            1.6656e-01, -7.1529e-03]],\n",
              " \n",
              "         [[-1.8102e-01, -2.7155e-01,  3.1942e-01,  ...,  1.1170e-01,\n",
              "            7.8007e-02, -2.0624e-01],\n",
              "          [-5.2931e-01,  7.6196e-02,  1.7436e-01,  ...,  3.1321e-04,\n",
              "           -8.4969e-01,  9.4202e-02],\n",
              "          [-1.2645e+00, -8.0763e-01,  3.7018e-01,  ...,  2.2860e-01,\n",
              "           -1.0531e-01, -2.9531e-01],\n",
              "          ...,\n",
              "          [ 3.1896e-01, -4.0075e-01,  1.0190e+00,  ..., -1.3371e-01,\n",
              "            4.3265e-01,  1.8707e-01],\n",
              "          [ 1.5019e-01, -3.0334e-01,  9.4855e-01,  ..., -3.8962e-02,\n",
              "            2.4480e-01,  1.3015e-01],\n",
              "          [ 2.5303e-01, -2.4007e-03,  6.9132e-01,  ..., -2.1534e-01,\n",
              "            1.9819e-01, -1.1716e-02]],\n",
              " \n",
              "         [[-1.1900e-01, -3.8508e-01,  4.3982e-01,  ...,  1.0382e-01,\n",
              "            3.1584e-01, -1.5108e-01],\n",
              "          [ 5.9830e-01,  4.1172e-01, -2.3847e-02,  ...,  1.9161e+00,\n",
              "            1.6007e+00, -6.7072e-01],\n",
              "          [-2.0559e-01, -7.5104e-01, -1.3538e+00,  ...,  7.3906e-01,\n",
              "            1.3591e+00, -4.1879e-02],\n",
              "          ...,\n",
              "          [ 2.3714e-01, -4.0975e-01,  9.8471e-01,  ..., -2.3558e-01,\n",
              "            6.0320e-01,  2.2810e-01],\n",
              "          [ 2.5913e-02, -4.2596e-01,  9.7512e-01,  ..., -1.8967e-01,\n",
              "            4.2112e-01,  1.8715e-01],\n",
              "          [ 1.8024e-01, -1.5169e-01,  8.8167e-01,  ..., -2.9488e-01,\n",
              "            3.2143e-01,  1.0319e-01]]]),\n",
              " tensor([[[ 0.1763, -0.7583, -0.9306,  ..., -0.3842, -0.1496,  0.3396],\n",
              "          [ 0.1737, -1.8374, -0.2400,  ...,  0.1046,  0.2481,  0.2619],\n",
              "          [ 0.3904, -1.4999,  0.3853,  ...,  0.1424,  1.3781, -0.3486],\n",
              "          ...,\n",
              "          [ 0.5401, -0.8692,  0.8303,  ...,  0.7172, -0.0907,  0.4401],\n",
              "          [ 0.1401, -0.3524, -1.1065,  ..., -0.0571, -1.1422,  1.2176],\n",
              "          [ 0.0644,  0.0462, -0.0223,  ..., -0.0375, -0.0117, -0.0410]],\n",
              " \n",
              "         [[-0.3085, -0.4645,  0.7360,  ...,  0.0808,  0.3271, -0.3567],\n",
              "          [ 0.4415, -0.3872, -0.1899,  ...,  0.7410,  0.3488, -0.6080],\n",
              "          [-1.2366, -0.4154,  1.8128,  ...,  1.2109,  1.0131, -0.0076],\n",
              "          ...,\n",
              "          [ 0.3405, -0.2124,  1.3162,  ..., -0.1215,  0.4798, -0.0386],\n",
              "          [ 0.0047, -0.0614,  1.3384,  ..., -0.1536,  0.3230, -0.0469],\n",
              "          [ 0.1567,  0.3423,  1.2013,  ..., -0.2215,  0.1675, -0.0155]],\n",
              " \n",
              "         [[-0.3851, -0.8003,  0.7289,  ...,  0.2964,  0.3456, -0.4908],\n",
              "          [-0.7276,  1.1802, -0.1201,  ...,  0.0952, -1.1096,  0.3949],\n",
              "          [-1.8549,  0.3421,  1.0100,  ...,  0.2750,  0.3442, -0.4195],\n",
              "          ...,\n",
              "          [ 0.6004, -0.2527,  0.9806,  ..., -0.2449,  0.5070, -0.0217],\n",
              "          [ 0.3871, -0.2838,  0.9414,  ..., -0.1711,  0.2720,  0.0187],\n",
              "          [ 0.5124,  0.0953,  0.6914,  ..., -0.2656,  0.0502, -0.0247]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-0.5020, -0.7789,  0.6858,  ...,  0.2854,  0.3669, -0.4763],\n",
              "          [ 0.4474, -0.3675,  0.6639,  ...,  0.9073,  0.8230, -0.5582],\n",
              "          [-0.8055, -0.2682,  0.5553,  ...,  0.1469,  1.8284,  1.4151],\n",
              "          ...,\n",
              "          [ 0.5303, -0.3220,  1.0152,  ..., -0.2055,  0.4870,  0.0384],\n",
              "          [ 0.3668, -0.3213,  0.9149,  ..., -0.1150,  0.2703,  0.0489],\n",
              "          [ 0.5611,  0.1464,  0.6520,  ..., -0.2792,  0.0089, -0.0383]],\n",
              " \n",
              "         [[-0.5021, -0.6360,  0.5455,  ...,  0.0886,  0.1776, -0.5728],\n",
              "          [-0.8863,  0.6958,  0.0767,  ..., -0.3255, -1.2671, -0.1136],\n",
              "          [-1.5389, -0.4337,  0.7761,  ...,  0.2033, -0.0890, -0.2553],\n",
              "          ...,\n",
              "          [ 0.4058, -0.3540,  0.9949,  ..., -0.1111,  0.3821,  0.0856],\n",
              "          [ 0.2179, -0.2553,  0.9866,  ..., -0.0253,  0.1892,  0.0373],\n",
              "          [ 0.3031,  0.0732,  0.6609,  ..., -0.1817,  0.1072,  0.0021]],\n",
              " \n",
              "         [[-0.4106, -0.6932,  0.6999,  ...,  0.0321,  0.2985, -0.3858],\n",
              "          [ 0.7464,  0.7103,  0.3291,  ...,  1.4978,  1.1381, -0.7941],\n",
              "          [-0.3415, -0.9094, -0.8171,  ...,  0.6730,  1.4061,  0.3812],\n",
              "          ...,\n",
              "          [ 0.3392, -0.4332,  0.9935,  ..., -0.1891,  0.5418,  0.1124],\n",
              "          [ 0.1536, -0.4142,  1.0209,  ..., -0.1382,  0.3560,  0.1200],\n",
              "          [ 0.2822, -0.0734,  0.9088,  ..., -0.2256,  0.2357,  0.1491]]]),\n",
              " tensor([[[ 2.0224e-01, -2.8557e-01, -1.1479e+00,  ..., -4.6217e-01,\n",
              "           -2.4781e-02,  6.9723e-01],\n",
              "          [ 3.1313e-01, -1.2938e+00,  2.9583e-01,  ...,  3.0302e-01,\n",
              "            2.5777e-01, -1.2625e-01],\n",
              "          [-4.4825e-02, -7.6633e-01,  2.6371e-01,  ...,  6.6922e-01,\n",
              "            1.2252e+00, -2.7870e-01],\n",
              "          ...,\n",
              "          [ 4.8342e-01, -6.2083e-01,  4.0217e-01,  ...,  6.6522e-02,\n",
              "            8.7139e-02,  3.9575e-01],\n",
              "          [-8.1872e-01, -3.0016e-01, -1.4088e+00,  ..., -6.7491e-02,\n",
              "           -1.4967e+00,  1.0090e+00],\n",
              "          [ 8.0185e-02,  6.7664e-02,  3.3324e-02,  ..., -2.3148e-02,\n",
              "           -3.8549e-02, -5.4089e-02]],\n",
              " \n",
              "         [[-1.0746e-01, -1.3775e-01,  3.9823e-01,  ..., -6.2166e-01,\n",
              "            2.6150e-01, -4.8556e-01],\n",
              "          [ 5.6239e-01, -6.9977e-02, -2.1371e-01,  ...,  8.7612e-01,\n",
              "           -3.1739e-02, -6.4277e-01],\n",
              "          [-1.3543e+00, -1.1849e-01,  1.0261e+00,  ...,  5.7786e-01,\n",
              "            2.0409e-01, -2.6975e-01],\n",
              "          ...,\n",
              "          [ 2.8546e-01, -1.7006e-01,  1.4592e+00,  ..., -9.8819e-02,\n",
              "            4.6859e-01, -3.2426e-01],\n",
              "          [-2.4422e-02, -3.1474e-02,  1.5630e+00,  ..., -1.1681e-01,\n",
              "            2.5447e-01, -3.0274e-01],\n",
              "          [ 1.4396e-01,  4.0612e-01,  1.4923e+00,  ..., -3.0039e-01,\n",
              "            1.9563e-01, -3.3468e-01]],\n",
              " \n",
              "         [[-3.2122e-01, -5.6702e-01,  2.9665e-01,  ..., -2.0077e-01,\n",
              "            3.7147e-01, -5.5911e-01],\n",
              "          [-6.5100e-01,  1.3537e+00, -6.8920e-02,  ..., -1.8211e-01,\n",
              "           -1.0612e+00,  5.3910e-01],\n",
              "          [-1.7157e+00,  2.7712e-01,  9.7552e-01,  ..., -6.7899e-02,\n",
              "            9.1608e-02, -7.2085e-01],\n",
              "          ...,\n",
              "          [ 5.1971e-01, -3.3771e-01,  1.1489e+00,  ..., -8.4551e-02,\n",
              "            5.7863e-01, -3.2733e-01],\n",
              "          [ 3.1784e-01, -4.3020e-01,  1.1083e+00,  ..., -4.5237e-02,\n",
              "            2.8277e-01, -3.0391e-01],\n",
              "          [ 4.1928e-01, -8.4282e-02,  9.4924e-01,  ..., -1.6716e-01,\n",
              "            1.0396e-01, -3.3831e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-4.0923e-01, -4.9527e-01,  5.4387e-01,  ..., -3.7051e-01,\n",
              "            2.8057e-01, -5.0081e-01],\n",
              "          [ 6.2441e-01, -2.4689e-01,  4.9157e-01,  ...,  1.1502e+00,\n",
              "            7.9121e-01, -5.2466e-01],\n",
              "          [-2.8385e-01, -4.1263e-01,  2.8244e-01,  ...,  2.9235e-01,\n",
              "            1.5236e+00,  1.7014e+00],\n",
              "          ...,\n",
              "          [ 4.6263e-01, -3.9061e-01,  1.1600e+00,  ..., -4.1660e-02,\n",
              "            5.5738e-01, -2.8487e-01],\n",
              "          [ 3.0593e-01, -4.3796e-01,  1.0873e+00,  ...,  2.0691e-02,\n",
              "            2.6319e-01, -2.8773e-01],\n",
              "          [ 4.6801e-01, -1.6876e-03,  9.5492e-01,  ..., -1.7007e-01,\n",
              "            3.0271e-02, -3.5943e-01]],\n",
              " \n",
              "         [[-3.0127e-01, -2.8753e-01,  2.4112e-01,  ..., -6.1549e-01,\n",
              "            2.9027e-01, -6.6094e-01],\n",
              "          [-7.5871e-01,  8.3155e-01, -2.9322e-01,  ..., -5.9565e-01,\n",
              "           -1.0360e+00,  1.4677e-01],\n",
              "          [-1.2998e+00, -6.5596e-01,  6.0645e-01,  ..., -2.8949e-01,\n",
              "            1.4403e-02, -4.4373e-01],\n",
              "          ...,\n",
              "          [ 3.5566e-01, -3.8770e-01,  1.2091e+00,  ...,  4.8686e-02,\n",
              "            4.9587e-01, -2.6392e-01],\n",
              "          [ 2.2553e-01, -3.2948e-01,  1.1955e+00,  ...,  5.7923e-02,\n",
              "            3.1165e-01, -3.5654e-01],\n",
              "          [ 2.8504e-01,  2.6746e-02,  1.0085e+00,  ..., -8.2573e-02,\n",
              "            2.4760e-01, -3.6033e-01]],\n",
              " \n",
              "         [[-1.5661e-01, -2.1729e-01,  3.6797e-01,  ..., -6.7995e-01,\n",
              "            3.3191e-01, -4.5254e-01],\n",
              "          [ 8.5125e-01,  7.8111e-01,  2.1879e-01,  ...,  1.7984e+00,\n",
              "            8.8366e-01, -4.9279e-01],\n",
              "          [-5.8242e-01, -7.8331e-01, -7.5028e-01,  ...,  4.9252e-01,\n",
              "            1.5147e+00,  4.3649e-01],\n",
              "          ...,\n",
              "          [ 2.2573e-01, -4.4253e-01,  1.1520e+00,  ..., -1.1182e-01,\n",
              "            6.5080e-01, -2.2152e-01],\n",
              "          [ 6.3548e-02, -4.5592e-01,  1.1831e+00,  ..., -1.0498e-01,\n",
              "            4.1015e-01, -2.3173e-01],\n",
              "          [ 1.9070e-01, -1.1236e-01,  1.1337e+00,  ..., -2.3677e-01,\n",
              "            2.9426e-01, -2.1879e-01]]]),\n",
              " tensor([[[ 7.4370e-02, -3.2157e-01, -4.7364e-01,  ..., -5.0415e-02,\n",
              "            2.8511e-01,  7.0135e-01],\n",
              "          [ 3.2116e-02, -1.6920e+00,  7.9243e-01,  ...,  1.0430e-02,\n",
              "            4.8217e-01, -5.2433e-02],\n",
              "          [ 2.1984e-01, -1.1186e+00,  9.4807e-01,  ...,  6.5144e-01,\n",
              "            8.4357e-01, -6.8431e-01],\n",
              "          ...,\n",
              "          [ 3.1699e-01, -7.4015e-01, -1.0696e-01,  ...,  2.0166e-02,\n",
              "           -7.6728e-02,  3.0652e-01],\n",
              "          [-9.2885e-01, -1.4620e-02, -8.1023e-01,  ...,  1.0126e-01,\n",
              "           -7.5005e-01,  1.0210e+00],\n",
              "          [ 2.8482e-02,  1.0694e-01,  7.3378e-02,  ..., -1.1565e-02,\n",
              "           -1.8278e-02,  4.1657e-02]],\n",
              " \n",
              "         [[-9.9159e-02,  1.8472e-01,  4.0476e-01,  ..., -3.7910e-01,\n",
              "            1.3399e-01, -5.0880e-01],\n",
              "          [ 7.7486e-01,  5.0038e-02,  9.8164e-03,  ...,  1.9800e-01,\n",
              "           -3.2280e-01, -3.8145e-01],\n",
              "          [-1.1109e+00, -2.3572e-02,  1.0471e+00,  ...,  3.5464e-01,\n",
              "            7.4664e-01, -5.8943e-01],\n",
              "          ...,\n",
              "          [ 2.1541e-01, -2.4872e-01,  1.3278e+00,  ..., -5.0120e-01,\n",
              "            4.6199e-01, -3.7083e-01],\n",
              "          [-8.8830e-02, -1.0710e-01,  1.4322e+00,  ..., -5.3653e-01,\n",
              "            2.6034e-01, -3.3170e-01],\n",
              "          [ 1.0364e-01,  3.5116e-01,  1.3530e+00,  ..., -6.7900e-01,\n",
              "            1.7580e-01, -3.6794e-01]],\n",
              " \n",
              "         [[-2.8577e-01, -2.6036e-01,  3.1970e-01,  ...,  5.4728e-02,\n",
              "            2.2052e-01, -6.1485e-01],\n",
              "          [-8.3845e-01,  1.2205e+00,  5.8842e-01,  ..., -2.2075e-01,\n",
              "           -9.9285e-01,  3.1031e-01],\n",
              "          [-1.4145e+00,  1.6596e-01,  1.0553e+00,  ...,  8.8401e-02,\n",
              "            2.3930e-01, -7.5715e-01],\n",
              "          ...,\n",
              "          [ 3.6432e-01, -5.6463e-01,  1.1309e+00,  ..., -5.6394e-01,\n",
              "            5.4662e-01, -4.2446e-01],\n",
              "          [ 1.5822e-01, -6.6208e-01,  1.1507e+00,  ..., -5.0951e-01,\n",
              "            2.7474e-01, -3.9654e-01],\n",
              "          [ 2.9882e-01, -3.4452e-01,  9.8351e-01,  ..., -5.9960e-01,\n",
              "            6.0354e-02, -4.2386e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-3.8257e-01, -1.8255e-01,  4.3025e-01,  ..., -4.3062e-02,\n",
              "            1.7558e-01, -5.2382e-01],\n",
              "          [ 6.7361e-01, -2.5296e-01,  1.9027e-01,  ...,  8.5698e-01,\n",
              "            5.8361e-01, -5.6151e-01],\n",
              "          [-9.5750e-02, -2.7453e-02,  2.3855e-01,  ..., -5.2901e-02,\n",
              "            9.2878e-01,  1.0822e+00],\n",
              "          ...,\n",
              "          [ 2.7641e-01, -6.0071e-01,  1.1118e+00,  ..., -5.0836e-01,\n",
              "            5.5084e-01, -3.8894e-01],\n",
              "          [ 1.2222e-01, -6.5744e-01,  1.0839e+00,  ..., -4.3435e-01,\n",
              "            2.6324e-01, -3.7787e-01],\n",
              "          [ 3.7273e-01, -2.1264e-01,  9.5303e-01,  ..., -5.4258e-01,\n",
              "           -1.5039e-03, -4.2934e-01]],\n",
              " \n",
              "         [[-3.0748e-01,  2.9616e-02,  2.0536e-01,  ..., -2.0361e-01,\n",
              "            1.9612e-01, -6.9095e-01],\n",
              "          [-5.0145e-01,  8.3720e-01,  2.7669e-01,  ..., -4.0580e-01,\n",
              "           -1.1621e+00, -3.1745e-01],\n",
              "          [-8.4650e-01, -6.3720e-01,  7.2941e-01,  ..., -8.6684e-02,\n",
              "           -4.8394e-03, -5.2115e-01],\n",
              "          ...,\n",
              "          [ 1.6840e-01, -5.2949e-01,  1.1409e+00,  ..., -4.4291e-01,\n",
              "            5.0368e-01, -3.5225e-01],\n",
              "          [ 3.5594e-02, -4.3703e-01,  1.1635e+00,  ..., -4.2035e-01,\n",
              "            3.2445e-01, -4.3891e-01],\n",
              "          [ 1.0685e-01, -7.2334e-02,  9.9002e-01,  ..., -5.4092e-01,\n",
              "            1.9710e-01, -4.4523e-01]],\n",
              " \n",
              "         [[-2.2657e-01,  1.0191e-01,  3.2887e-01,  ..., -3.8216e-01,\n",
              "            1.4521e-01, -5.6615e-01],\n",
              "          [ 9.3122e-01,  1.2159e+00,  2.3399e-01,  ...,  1.8380e+00,\n",
              "            3.5519e-01, -3.3505e-01],\n",
              "          [-4.6515e-01, -5.8466e-01, -7.7893e-01,  ...,  3.2795e-01,\n",
              "            5.2231e-01, -7.4709e-02],\n",
              "          ...,\n",
              "          [ 6.2045e-02, -5.2040e-01,  1.1313e+00,  ..., -5.4752e-01,\n",
              "            6.2234e-01, -3.1730e-01],\n",
              "          [-9.2700e-02, -5.1432e-01,  1.1749e+00,  ..., -5.2881e-01,\n",
              "            3.8384e-01, -3.1559e-01],\n",
              "          [ 6.9901e-02, -1.5260e-01,  1.1262e+00,  ..., -6.0471e-01,\n",
              "            2.1942e-01, -2.8417e-01]]]),\n",
              " tensor([[[-1.0036e-01, -6.6719e-01, -2.1741e-01,  ...,  2.7273e-03,\n",
              "           -2.0406e-01,  7.3674e-01],\n",
              "          [ 3.3273e-01, -1.9241e+00,  6.5684e-01,  ..., -1.2966e-01,\n",
              "            8.1105e-01, -2.8639e-02],\n",
              "          [ 5.3457e-01, -1.2543e+00,  1.0817e+00,  ...,  8.4170e-01,\n",
              "            9.9791e-01, -5.1204e-01],\n",
              "          ...,\n",
              "          [ 3.8299e-01, -4.5513e-01, -1.8313e-01,  ...,  5.5713e-02,\n",
              "            3.3693e-01,  3.7675e-02],\n",
              "          [-1.1411e+00, -5.9698e-01, -1.3164e+00,  ...,  3.9607e-01,\n",
              "           -5.7694e-01,  3.4199e-01],\n",
              "          [ 2.8907e-03,  2.2905e-02, -1.0157e-01,  ...,  6.3998e-02,\n",
              "           -1.6383e-02,  1.6479e-02]],\n",
              " \n",
              "         [[-3.8769e-01,  4.5301e-02,  4.2075e-01,  ..., -3.6207e-01,\n",
              "            3.3760e-01, -5.0038e-01],\n",
              "          [ 1.0412e+00,  1.7496e-01,  1.3832e-01,  ...,  1.3246e-01,\n",
              "            1.2020e-01, -4.6331e-01],\n",
              "          [-9.1011e-01,  1.0859e-01,  1.9681e+00,  ..., -9.1152e-02,\n",
              "            1.2551e+00, -1.1912e+00],\n",
              "          ...,\n",
              "          [ 1.3301e-01, -1.9613e-01,  1.3059e+00,  ..., -7.5245e-01,\n",
              "            2.1327e-01, -7.3833e-01],\n",
              "          [-1.6787e-01, -1.3806e-01,  1.3479e+00,  ..., -7.6077e-01,\n",
              "            5.8315e-02, -6.8173e-01],\n",
              "          [ 2.1142e-02,  3.5629e-01,  1.2980e+00,  ..., -9.1487e-01,\n",
              "           -7.2787e-02, -7.4234e-01]],\n",
              " \n",
              "         [[-5.6305e-01, -3.3898e-01,  2.3727e-01,  ..., -9.1119e-02,\n",
              "            4.3032e-01, -6.4671e-01],\n",
              "          [-6.7610e-01,  1.5102e+00,  9.8446e-01,  ..., -6.3324e-01,\n",
              "           -6.4966e-01,  1.2795e-01],\n",
              "          [-1.2564e+00,  2.6960e-01,  1.5197e+00,  ..., -1.4027e-01,\n",
              "            5.6104e-01, -1.0968e+00],\n",
              "          ...,\n",
              "          [ 2.6111e-01, -4.4274e-01,  1.2401e+00,  ..., -6.8456e-01,\n",
              "            1.7798e-01, -8.5238e-01],\n",
              "          [ 6.7247e-02, -5.7273e-01,  1.2561e+00,  ..., -6.1735e-01,\n",
              "           -3.8295e-02, -8.1756e-01],\n",
              "          [ 1.4353e-01, -2.8003e-01,  1.0360e+00,  ..., -6.5604e-01,\n",
              "           -1.7615e-01, -8.8162e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-7.5474e-01, -3.0411e-01,  4.1299e-01,  ..., -2.9467e-02,\n",
              "            4.0608e-01, -5.7824e-01],\n",
              "          [ 3.0138e-01,  2.7461e-02,  2.2496e-01,  ...,  6.0062e-01,\n",
              "            6.0881e-01, -6.9072e-01],\n",
              "          [ 1.1945e-02,  2.3894e-02, -3.7201e-02,  ..., -5.7900e-02,\n",
              "            5.6685e-02,  6.6155e-02],\n",
              "          ...,\n",
              "          [ 1.6242e-01, -4.8520e-01,  1.2625e+00,  ..., -6.7405e-01,\n",
              "            1.8762e-01, -8.1113e-01],\n",
              "          [ 1.1902e-04, -5.8588e-01,  1.2213e+00,  ..., -5.7744e-01,\n",
              "           -2.9251e-02, -7.7690e-01],\n",
              "          [ 1.7907e-01, -1.8316e-01,  9.8741e-01,  ..., -5.9047e-01,\n",
              "           -2.0539e-01, -8.5488e-01]],\n",
              " \n",
              "         [[-5.6672e-01, -1.2469e-01,  1.6195e-01,  ..., -1.6414e-01,\n",
              "            3.8705e-01, -6.6778e-01],\n",
              "          [-1.9347e-01,  6.0533e-01,  6.8250e-01,  ..., -8.4494e-01,\n",
              "           -9.2079e-01, -3.4532e-01],\n",
              "          [-9.3187e-01, -7.9712e-01,  1.2430e+00,  ..., -4.1945e-01,\n",
              "            3.3084e-01, -5.2985e-01],\n",
              "          ...,\n",
              "          [ 6.2115e-02, -4.8949e-01,  1.2419e+00,  ..., -6.6100e-01,\n",
              "            1.8439e-01, -7.6329e-01],\n",
              "          [-3.4036e-02, -4.2151e-01,  1.2901e+00,  ..., -6.5150e-01,\n",
              "            8.9077e-04, -8.2983e-01],\n",
              "          [-2.6392e-03, -3.6775e-02,  1.0670e+00,  ..., -7.2666e-01,\n",
              "           -1.1688e-01, -8.3958e-01]],\n",
              " \n",
              "         [[-4.6162e-01, -5.4700e-02,  3.8730e-01,  ..., -3.3702e-01,\n",
              "            2.7650e-01, -6.3966e-01],\n",
              "          [ 1.0889e+00,  7.0732e-01,  8.6630e-01,  ...,  1.8676e+00,\n",
              "            6.6572e-01, -4.8844e-01],\n",
              "          [-5.1695e-01, -9.8485e-01,  2.2662e-01,  ...,  1.5913e-01,\n",
              "            7.7425e-01, -2.5908e-01],\n",
              "          ...,\n",
              "          [-2.0541e-02, -4.5495e-01,  1.2572e+00,  ..., -7.1392e-01,\n",
              "            2.4989e-01, -7.5937e-01],\n",
              "          [-1.6381e-01, -4.7239e-01,  1.2784e+00,  ..., -6.7256e-01,\n",
              "            4.4134e-02, -7.5183e-01],\n",
              "          [-6.2362e-02, -9.8027e-02,  1.1903e+00,  ..., -7.2295e-01,\n",
              "           -7.2195e-02, -7.3661e-01]]]),\n",
              " tensor([[[ 8.2205e-02, -5.0600e-01, -7.3284e-02,  ...,  4.2533e-01,\n",
              "            3.4510e-01,  1.4422e-01],\n",
              "          [ 3.1744e-01, -1.7636e+00,  4.0025e-01,  ..., -1.6965e-01,\n",
              "            1.3164e+00,  6.5064e-02],\n",
              "          [ 4.8185e-01, -1.0005e+00,  1.1132e+00,  ...,  4.7811e-01,\n",
              "            9.5249e-01, -3.2231e-01],\n",
              "          ...,\n",
              "          [ 2.5830e-01, -3.7062e-01, -3.2582e-01,  ...,  5.7807e-01,\n",
              "            2.9687e-01, -5.0970e-03],\n",
              "          [-1.4774e+00, -5.8125e-01, -1.2645e+00,  ...,  1.0628e+00,\n",
              "           -4.6479e-01, -2.5965e-02],\n",
              "          [ 5.3093e-02,  1.7932e-02, -5.1457e-02,  ...,  1.2928e-02,\n",
              "           -3.2455e-02, -9.5527e-04]],\n",
              " \n",
              "         [[-1.2625e-01,  3.7238e-02,  4.8057e-01,  ..., -4.9462e-01,\n",
              "            5.3037e-01, -7.5094e-01],\n",
              "          [ 1.2178e+00,  1.1762e-01,  1.7850e-01,  ..., -2.0131e-02,\n",
              "            1.2388e-01, -3.7948e-01],\n",
              "          [-1.1670e+00, -1.0605e-01,  1.1178e+00,  ..., -2.6374e-01,\n",
              "            8.3794e-01, -1.4566e+00],\n",
              "          ...,\n",
              "          [ 1.7943e-01, -9.4601e-02,  1.3838e+00,  ..., -9.0854e-01,\n",
              "            3.2018e-01, -8.6190e-01],\n",
              "          [-1.2908e-01, -4.1516e-02,  1.4208e+00,  ..., -9.3276e-01,\n",
              "            1.9923e-01, -7.7585e-01],\n",
              "          [ 5.5931e-02,  4.4585e-01,  1.3658e+00,  ..., -1.0649e+00,\n",
              "            1.0150e-01, -8.8298e-01]],\n",
              " \n",
              "         [[-2.2569e-01, -8.3643e-02,  1.9321e-01,  ..., -3.1010e-01,\n",
              "            3.9705e-01, -1.0354e+00],\n",
              "          [-6.4555e-01,  7.0644e-01,  4.4446e-01,  ..., -1.0330e+00,\n",
              "           -9.7574e-01, -8.6309e-01],\n",
              "          [-9.3998e-01,  2.1268e-01,  4.1934e-01,  ..., -4.6421e-01,\n",
              "            2.3832e-01, -1.5856e+00],\n",
              "          ...,\n",
              "          [ 3.7860e-01, -3.2942e-01,  1.3219e+00,  ..., -7.6676e-01,\n",
              "            2.2438e-01, -1.0617e+00],\n",
              "          [ 1.7886e-01, -4.6528e-01,  1.3283e+00,  ..., -7.0916e-01,\n",
              "            3.1245e-02, -1.0007e+00],\n",
              "          [ 2.8775e-01, -1.7003e-01,  1.0830e+00,  ..., -7.2725e-01,\n",
              "           -8.7177e-02, -1.1088e+00]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-4.8161e-01,  5.5873e-02,  3.6396e-01,  ..., -2.5786e-01,\n",
              "            3.2075e-01, -6.4051e-01],\n",
              "          [ 5.0298e-01,  2.6332e-01, -2.9382e-01,  ...,  3.6661e-03,\n",
              "            5.4058e-01, -3.8752e-01],\n",
              "          [ 4.1571e-02,  2.7419e-02, -2.4019e-03,  ...,  9.4024e-03,\n",
              "           -1.0303e-02,  3.1395e-03],\n",
              "          ...,\n",
              "          [ 2.7304e-01, -3.8374e-01,  1.3604e+00,  ..., -7.5602e-01,\n",
              "            2.7978e-01, -9.8590e-01],\n",
              "          [ 1.0731e-01, -4.8459e-01,  1.3069e+00,  ..., -6.7001e-01,\n",
              "            9.4440e-02, -9.2723e-01],\n",
              "          [ 3.4480e-01, -9.3641e-02,  1.0376e+00,  ..., -6.4272e-01,\n",
              "           -7.2725e-02, -1.0559e+00]],\n",
              " \n",
              "         [[-3.0933e-01,  9.8086e-02,  1.6235e-01,  ..., -5.1087e-01,\n",
              "            3.8668e-01, -8.7518e-01],\n",
              "          [-5.9314e-01,  1.0374e-01,  2.2819e-01,  ..., -9.9425e-01,\n",
              "           -1.0113e+00, -8.3114e-01],\n",
              "          [-8.3047e-01, -4.1780e-01,  9.7392e-01,  ..., -7.4478e-01,\n",
              "            1.6980e-01, -1.0430e+00],\n",
              "          ...,\n",
              "          [ 1.7495e-01, -3.7263e-01,  1.3786e+00,  ..., -7.8877e-01,\n",
              "            2.6363e-01, -9.0854e-01],\n",
              "          [ 6.4009e-02, -2.5457e-01,  1.4439e+00,  ..., -7.9297e-01,\n",
              "            1.0723e-01, -9.6846e-01],\n",
              "          [ 1.1331e-01,  1.2347e-01,  1.2202e+00,  ..., -8.3938e-01,\n",
              "            2.0884e-02, -1.0291e+00]],\n",
              " \n",
              "         [[-9.8362e-02,  2.7157e-02,  4.1562e-01,  ..., -4.3173e-01,\n",
              "            3.2975e-01, -6.4370e-01],\n",
              "          [ 9.7120e-01,  9.0183e-01,  9.0270e-01,  ...,  1.2426e+00,\n",
              "            6.0044e-01, -5.2579e-01],\n",
              "          [-4.1550e-01, -1.0922e+00,  1.0393e-01,  ...,  5.1946e-01,\n",
              "            7.0412e-01,  5.7351e-02],\n",
              "          ...,\n",
              "          [ 1.0716e-02, -3.6232e-01,  1.3630e+00,  ..., -8.1569e-01,\n",
              "            2.6608e-01, -9.0369e-01],\n",
              "          [-1.3843e-01, -3.7121e-01,  1.3564e+00,  ..., -7.7006e-01,\n",
              "            8.6155e-02, -8.8344e-01],\n",
              "          [-6.0027e-03, -1.7894e-02,  1.2414e+00,  ..., -8.0860e-01,\n",
              "            1.2409e-02, -8.9623e-01]]]),\n",
              " tensor([[[-8.7071e-01, -1.7167e-01, -4.1474e-01,  ...,  5.0643e-01,\n",
              "            7.5320e-01,  6.1644e-01],\n",
              "          [ 1.6066e-01, -1.1465e+00, -1.2744e-01,  ...,  4.9624e-01,\n",
              "            1.8019e+00,  1.8012e-01],\n",
              "          [ 2.7691e-01, -6.7028e-01,  8.6098e-01,  ...,  6.8047e-01,\n",
              "            1.0129e+00, -2.4723e-01],\n",
              "          ...,\n",
              "          [ 3.0464e-01, -3.3218e-01, -1.8275e-01,  ...,  4.5554e-01,\n",
              "            3.0901e-01, -2.0985e-01],\n",
              "          [-1.2076e+00, -7.0128e-01, -1.1874e+00,  ...,  8.8510e-01,\n",
              "           -1.3617e-02,  1.5706e-01],\n",
              "          [ 2.6108e-01,  2.7389e-01, -4.1015e-01,  ...,  2.1230e-01,\n",
              "           -3.6857e-01, -8.9444e-02]],\n",
              " \n",
              "         [[-3.7418e-01,  1.9222e-01,  5.1624e-01,  ..., -4.2429e-01,\n",
              "            5.9706e-01, -6.2933e-01],\n",
              "          [ 5.2100e-01,  2.4934e-01, -5.4365e-04,  ...,  2.9755e-01,\n",
              "            7.2869e-02, -2.3605e-01],\n",
              "          [-7.1197e-01,  8.3246e-02,  8.6361e-01,  ..., -6.6959e-02,\n",
              "            2.8622e-01, -1.3424e+00],\n",
              "          ...,\n",
              "          [ 2.3591e-01, -1.2086e-01,  9.6129e-01,  ..., -5.9636e-01,\n",
              "            2.7862e-01, -8.5863e-01],\n",
              "          [ 6.4980e-02, -8.5833e-02,  9.6863e-01,  ..., -5.8316e-01,\n",
              "            2.1074e-01, -7.6211e-01],\n",
              "          [ 1.3112e-01,  1.3194e-01,  9.1263e-01,  ..., -6.2665e-01,\n",
              "            1.6988e-01, -8.7882e-01]],\n",
              " \n",
              "         [[-5.6518e-01,  1.0364e-01,  2.5837e-01,  ..., -3.8819e-01,\n",
              "            5.5210e-01, -9.5909e-01],\n",
              "          [-3.0060e-01,  3.8672e-01,  8.7838e-02,  ..., -4.5570e-02,\n",
              "            2.0756e-01, -6.9950e-01],\n",
              "          [-5.4967e-01, -1.2395e-01,  1.3440e-01,  ...,  3.1265e-02,\n",
              "            1.6978e-01, -1.7059e+00],\n",
              "          ...,\n",
              "          [ 4.0961e-01, -2.6808e-01,  9.5793e-01,  ..., -6.9141e-01,\n",
              "            7.9159e-02, -1.2496e+00],\n",
              "          [ 3.2734e-01, -3.2227e-01,  9.7360e-01,  ..., -6.8070e-01,\n",
              "           -2.7843e-02, -1.1723e+00],\n",
              "          [ 3.4083e-01, -2.0086e-01,  8.2181e-01,  ..., -6.7327e-01,\n",
              "           -5.5807e-02, -1.3299e+00]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-6.9182e-01,  2.0416e-01,  4.3648e-01,  ..., -3.0314e-01,\n",
              "            4.7310e-01, -8.5553e-01],\n",
              "          [ 1.7813e-01,  1.2169e-01, -4.2749e-01,  ...,  4.4657e-01,\n",
              "            7.6364e-01,  3.0947e-02],\n",
              "          [ 5.3415e-01, -7.8928e-02,  1.4875e-01,  ..., -1.1345e-01,\n",
              "           -5.8755e-01, -3.1055e-01],\n",
              "          ...,\n",
              "          [ 3.5249e-01, -2.7010e-01,  9.9985e-01,  ..., -6.8340e-01,\n",
              "            1.0818e-01, -1.2325e+00],\n",
              "          [ 2.8959e-01, -3.0842e-01,  9.7817e-01,  ..., -6.5475e-01,\n",
              "            1.4522e-02, -1.1639e+00],\n",
              "          [ 3.4913e-01, -1.4557e-01,  8.1888e-01,  ..., -6.2975e-01,\n",
              "           -2.8556e-02, -1.3328e+00]],\n",
              " \n",
              "         [[-5.9789e-01,  3.7393e-01,  3.6312e-01,  ..., -5.4766e-01,\n",
              "            4.4737e-01, -9.4693e-01],\n",
              "          [-1.6490e-01,  3.5445e-01, -3.2496e-01,  ..., -2.4498e-01,\n",
              "            9.9036e-02, -4.4764e-01],\n",
              "          [-5.3581e-01, -3.1959e-01,  4.6556e-01,  ..., -4.4887e-01,\n",
              "            1.7139e-02, -6.1641e-01],\n",
              "          ...,\n",
              "          [ 2.9763e-01, -2.3432e-01,  1.0222e+00,  ..., -7.0737e-01,\n",
              "            1.3708e-01, -1.1685e+00],\n",
              "          [ 2.6600e-01, -1.7855e-01,  1.0597e+00,  ..., -7.1025e-01,\n",
              "            7.2960e-02, -1.1607e+00],\n",
              "          [ 2.2185e-01,  3.0500e-03,  9.4328e-01,  ..., -7.1806e-01,\n",
              "            4.2196e-02, -1.2895e+00]],\n",
              " \n",
              "         [[-4.7710e-01,  3.0492e-01,  4.3348e-01,  ..., -3.6474e-01,\n",
              "            5.0915e-01, -5.5953e-01],\n",
              "          [ 4.0885e-01,  8.4721e-01,  9.2457e-01,  ...,  7.4495e-01,\n",
              "            6.0447e-01, -1.5275e-01],\n",
              "          [-4.6772e-01, -1.0561e+00,  4.2678e-02,  ...,  5.9716e-01,\n",
              "            4.6737e-01,  6.0252e-02],\n",
              "          ...,\n",
              "          [ 1.8646e-01, -2.0307e-01,  9.8380e-01,  ..., -5.7707e-01,\n",
              "            1.9232e-01, -9.1166e-01],\n",
              "          [ 1.3712e-01, -2.1207e-01,  9.7371e-01,  ..., -5.5709e-01,\n",
              "            1.0363e-01, -8.5783e-01],\n",
              "          [ 1.7508e-01, -3.6528e-02,  8.9330e-01,  ..., -5.5582e-01,\n",
              "            8.5925e-02, -9.3392e-01]]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YY-VXeiXHjO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "1347891b-d805-44b5-c07f-004dbc8cb5c5"
      },
      "source": [
        "#%cd /content/drive/My Drive\n",
        "#!pwd\n",
        "#import tensorflow as tf\n",
        "import torch\n",
        "#torch.save(outputs, '/content/drive/My Drive/outputs_tensor.pt')\n",
        "\n",
        "outputs = torch.load('/content/drive/My Drive/outputs_tensor.pt')\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-140061ea086f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#torch.save(outputs, '/content/drive/My Drive/outputs_tensor.pt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/outputs_tensor.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: unexpected EOF, expected 87049423 more bytes. The file might be corrupted."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzXlGzzMo8GI",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Understanding the Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB3-VFKLo-lz",
        "colab_type": "text"
      },
      "source": [
        "The full set of hidden states for this model, stored in the object hidden_states, is a little dizzying. This object has four dimensions, in the following order:\n",
        "\n",
        "1. The layer number (13 layers)\n",
        "2. The batch number (1 sentence)\n",
        "3. The word / token number (22 tokens in our sentence)\n",
        "4. The hidden unit / feature number (768 features)\n",
        "\n",
        "Wait, 13 layers? Doesn't BERT only have 12? It's 13 because the first element is the input embeddings, the rest is the outputs of each of BERT's 12 layers.\n",
        "\n",
        "That’s 219,648 unique values just to represent our one sentence!\n",
        "\n",
        "The second dimension, the batch size, is used when submitting multiple sentences to the model at once; here, though, we just have one example sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zvr6YWupwGp",
        "colab_type": "code",
        "outputId": "f0860ada-020c-431e-ea6f-25050c199000",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
        "layer_i = 0\n",
        "\n",
        "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
        "batch_i = 0\n",
        "\n",
        "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
        "token_i = 0\n",
        "\n",
        "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 677\n",
            "Number of tokens: 100\n",
            "Number of hidden units: 768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyUv6FApp1MQ",
        "colab_type": "text"
      },
      "source": [
        "Let's take a quick look at the range of values for a given layer and token.\n",
        "\n",
        "You'll find that the range is fairly similar for all layers and tokens, with the majority of values falling between [-2, 2], and a small smattering of values around -10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBrX44tHzDnv",
        "colab_type": "code",
        "outputId": "3caabfb2-4a8a-4f51-d809-35a29130031d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "input_ids[0:10]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[  101,  4012,  1033,  1024,  5958,  1010,  1051, 27421,  2121,  2539,\n",
              "           1010,  2184,  1024,  2871,  2572,  1024,  5143,  1010, 20475,  1025,\n",
              "           3608,  2075,  1010,  2728,  1025, 12075, 16001,  1010,  3660,  1025,\n",
              "           3016,  3238,  1010,  9802,  1025,  4562,  1010,  6285,  1025,  5041,\n",
              "           4974,  1010,  6902,  1025,  7641,  1010,  6384,  1025,  2011, 21716,\n",
              "           1010,  2198,  1025,  5252,  1010,  2728,  1025,  8902,  2140,  1010,\n",
              "           8057,  1025, 24119,  1010,  2198,  1025,  9574,  1010,  2577,  1025,\n",
              "          27831,  2140,  1010,  7010,  1025,  2585,  1010,  3487,  1025,  2139,\n",
              "          23736,  2638,  1010,  4656,  1025, 13109,  5162, 11528,  1010,  2984,\n",
              "           1025,  2413,  1010, 26568,  1025, 17297,  1010,  3960,  1025,   102]]),\n",
              " tensor([[  101,  1000,  2017,  2089,  3193,  1996, 17178,  2011, 29347, 18965,\n",
              "           2256,  4037,  2012,  7479,   102,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
              " tensor([[  101,  2065,  2017,  2052,  9544,  4374,  1037,  2524,  6100,  1997,\n",
              "           1996, 17178,  1999,  1996,  5653,  1010,  2738,  2084, 10523,  2009,\n",
              "           2006,  1996,  4037,  1010,  3531,  3967,  2149,   102,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
              " tensor([[  101,  2057,  2052,  2022,  2200,  5580, 20118,  7382, 13390,  2618,\n",
              "           2115,  8996,   102,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
              " tensor([[  101, 14089, 11721, 22036,  2121,  1010, 24531,  2078,  2213,  4748,\n",
              "          25300, 20528,  2099, 28952,  1013,  5786,  2475,  1011, 17273,  2575,\n",
              "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
              " tensor([[  101,  2004,  2017,  2113,  1010,  5658,  2573,  1006,  4372,  2860,\n",
              "           1007,  1998,  3795,  6143, 14768,  6129,  1006, 28177,  2015,  1007,\n",
              "           3728,  6472,  1037,  2048,  1011,  2095,  3820,  1010, 13557, 11338,\n",
              "           2072,  2088,  9006,  2052,  3710,  2004,  1005,  1055,  3078, 12108,\n",
              "          10802,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
              " tensor([[ 101, 1999, 2256, 3025, 4807, 1010, 2057, 5393, 2008, 2057, 2052, 3073,\n",
              "          2017, 2007, 2062, 6851, 2592, 2004, 2009, 2150, 2800,  102,    0,    0,\n",
              "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "             0,    0,    0,    0]]),\n",
              " tensor([[  101,  2927,  5958,  1010,  1051, 27421,  2121,  2656,  1010,  2012,\n",
              "           1037,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
              " tensor([[  101,  1056,  1010,  1996,  7709,  2005,  4214,  1005,  1055,  7570,\n",
              "           2271,  2078,  4822,  2248,  5269,  1010, 13343,  2710,  1010,  2478,\n",
              "           1996,  5385,  1011,  5989,  1011,  3042,  2193,  2097,  2689,   102,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
              " tensor([[ 101, 1996, 2047, 7709, 2003, 2004, 4076, 1024, 1015,  102,    0,    0,\n",
              "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "             0,    0,    0,    0]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQD2Pqki14Pg",
        "colab_type": "code",
        "outputId": "2d20e6ff-613d-4f76-e152-e6a37fff45d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hidden_states[12]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-8.7071e-01, -1.7167e-01, -4.1474e-01,  ...,  5.0643e-01,\n",
              "           7.5320e-01,  6.1644e-01],\n",
              "         [ 1.6066e-01, -1.1465e+00, -1.2744e-01,  ...,  4.9624e-01,\n",
              "           1.8019e+00,  1.8012e-01],\n",
              "         [ 2.7691e-01, -6.7028e-01,  8.6098e-01,  ...,  6.8047e-01,\n",
              "           1.0129e+00, -2.4723e-01],\n",
              "         ...,\n",
              "         [ 3.0464e-01, -3.3218e-01, -1.8275e-01,  ...,  4.5554e-01,\n",
              "           3.0901e-01, -2.0985e-01],\n",
              "         [-1.2076e+00, -7.0128e-01, -1.1874e+00,  ...,  8.8510e-01,\n",
              "          -1.3617e-02,  1.5706e-01],\n",
              "         [ 2.6108e-01,  2.7389e-01, -4.1015e-01,  ...,  2.1230e-01,\n",
              "          -3.6857e-01, -8.9444e-02]],\n",
              "\n",
              "        [[-3.7418e-01,  1.9222e-01,  5.1624e-01,  ..., -4.2429e-01,\n",
              "           5.9706e-01, -6.2933e-01],\n",
              "         [ 5.2100e-01,  2.4934e-01, -5.4365e-04,  ...,  2.9755e-01,\n",
              "           7.2869e-02, -2.3605e-01],\n",
              "         [-7.1197e-01,  8.3246e-02,  8.6361e-01,  ..., -6.6959e-02,\n",
              "           2.8622e-01, -1.3424e+00],\n",
              "         ...,\n",
              "         [ 2.3591e-01, -1.2086e-01,  9.6129e-01,  ..., -5.9636e-01,\n",
              "           2.7862e-01, -8.5863e-01],\n",
              "         [ 6.4980e-02, -8.5833e-02,  9.6863e-01,  ..., -5.8316e-01,\n",
              "           2.1074e-01, -7.6211e-01],\n",
              "         [ 1.3112e-01,  1.3194e-01,  9.1263e-01,  ..., -6.2665e-01,\n",
              "           1.6988e-01, -8.7882e-01]],\n",
              "\n",
              "        [[-5.6518e-01,  1.0364e-01,  2.5837e-01,  ..., -3.8819e-01,\n",
              "           5.5210e-01, -9.5909e-01],\n",
              "         [-3.0060e-01,  3.8672e-01,  8.7838e-02,  ..., -4.5570e-02,\n",
              "           2.0756e-01, -6.9950e-01],\n",
              "         [-5.4967e-01, -1.2395e-01,  1.3440e-01,  ...,  3.1265e-02,\n",
              "           1.6978e-01, -1.7059e+00],\n",
              "         ...,\n",
              "         [ 4.0961e-01, -2.6808e-01,  9.5793e-01,  ..., -6.9141e-01,\n",
              "           7.9159e-02, -1.2496e+00],\n",
              "         [ 3.2734e-01, -3.2227e-01,  9.7360e-01,  ..., -6.8070e-01,\n",
              "          -2.7843e-02, -1.1723e+00],\n",
              "         [ 3.4083e-01, -2.0086e-01,  8.2181e-01,  ..., -6.7327e-01,\n",
              "          -5.5807e-02, -1.3299e+00]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-6.9182e-01,  2.0416e-01,  4.3648e-01,  ..., -3.0314e-01,\n",
              "           4.7310e-01, -8.5553e-01],\n",
              "         [ 1.7813e-01,  1.2169e-01, -4.2749e-01,  ...,  4.4657e-01,\n",
              "           7.6364e-01,  3.0947e-02],\n",
              "         [ 5.3415e-01, -7.8928e-02,  1.4875e-01,  ..., -1.1345e-01,\n",
              "          -5.8755e-01, -3.1055e-01],\n",
              "         ...,\n",
              "         [ 3.5249e-01, -2.7010e-01,  9.9985e-01,  ..., -6.8340e-01,\n",
              "           1.0818e-01, -1.2325e+00],\n",
              "         [ 2.8959e-01, -3.0842e-01,  9.7817e-01,  ..., -6.5475e-01,\n",
              "           1.4522e-02, -1.1639e+00],\n",
              "         [ 3.4913e-01, -1.4557e-01,  8.1888e-01,  ..., -6.2975e-01,\n",
              "          -2.8556e-02, -1.3328e+00]],\n",
              "\n",
              "        [[-5.9789e-01,  3.7393e-01,  3.6312e-01,  ..., -5.4766e-01,\n",
              "           4.4737e-01, -9.4693e-01],\n",
              "         [-1.6490e-01,  3.5445e-01, -3.2496e-01,  ..., -2.4498e-01,\n",
              "           9.9036e-02, -4.4764e-01],\n",
              "         [-5.3581e-01, -3.1959e-01,  4.6556e-01,  ..., -4.4887e-01,\n",
              "           1.7139e-02, -6.1641e-01],\n",
              "         ...,\n",
              "         [ 2.9763e-01, -2.3432e-01,  1.0222e+00,  ..., -7.0737e-01,\n",
              "           1.3708e-01, -1.1685e+00],\n",
              "         [ 2.6600e-01, -1.7855e-01,  1.0597e+00,  ..., -7.1025e-01,\n",
              "           7.2960e-02, -1.1607e+00],\n",
              "         [ 2.2185e-01,  3.0500e-03,  9.4328e-01,  ..., -7.1806e-01,\n",
              "           4.2196e-02, -1.2895e+00]],\n",
              "\n",
              "        [[-4.7710e-01,  3.0492e-01,  4.3348e-01,  ..., -3.6474e-01,\n",
              "           5.0915e-01, -5.5953e-01],\n",
              "         [ 4.0885e-01,  8.4721e-01,  9.2457e-01,  ...,  7.4495e-01,\n",
              "           6.0447e-01, -1.5275e-01],\n",
              "         [-4.6772e-01, -1.0561e+00,  4.2678e-02,  ...,  5.9716e-01,\n",
              "           4.6737e-01,  6.0252e-02],\n",
              "         ...,\n",
              "         [ 1.8646e-01, -2.0307e-01,  9.8380e-01,  ..., -5.7707e-01,\n",
              "           1.9232e-01, -9.1166e-01],\n",
              "         [ 1.3712e-01, -2.1207e-01,  9.7371e-01,  ..., -5.5709e-01,\n",
              "           1.0363e-01, -8.5783e-01],\n",
              "         [ 1.7508e-01, -3.6528e-02,  8.9330e-01,  ..., -5.5582e-01,\n",
              "           8.5925e-02, -9.3392e-01]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbAUxYxD4Hpe",
        "colab_type": "code",
        "outputId": "896eb1d7-6003-438a-e258-9be74f9661c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "tokenizer.vocab.get(\"please\")  # 1996\n",
        "outputs.attention_weights"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-93caf6768183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"please\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 1996\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'attention_weights'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii8lXaACp4KU",
        "colab_type": "code",
        "outputId": "190406b5-cce2-4855-f03e-6757c44a869a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# For the 5th token in our sentence, select its feature values from layer 5.\n",
        "token_i = 3  #\"view\" # 6 and 10 are bank\n",
        "sentence_i = 1    \n",
        "#sentence_i = 10\n",
        "layer_i = 12\n",
        "\n",
        "vec = hidden_states[layer_i][sentence_i][token_i]\n",
        "\n",
        "print (token_tensor[token_i]) \n",
        "print (text_content[sentence_i]) \n",
        "\n",
        "#indexed_tokens = tokenizer.convert_tokens_to_ids(token_tensor)\n",
        "tokenized_text = tokenizer.tokenize(text_content[sentence_i])\n",
        "print (tokenized_text[token_i]) \n",
        "\n",
        "# Display the words with their indeces.\n",
        "#for tup in zip(text_content, indexed_tokens):\n",
        "#    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
        "\n",
        "# Plot the values as a histogram to show their distribution.\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.hist(vec, bins=200)\n",
        "plt.show()  # this visualizes the vector for token \"view\" in the last hidden layer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8ff25d68f2bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlayer_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hidden_states' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-7CsJ3ygZX8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "323e3fb5-63e9-4e80-8593-67d79cdd311a"
      },
      "source": [
        "#  Let's see if \"view\" has multiple tokens\n",
        "\n",
        "for i, token_str in enumerate(tokenized_text):\n",
        "  print (i, token_str)\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 \"\n",
            "1 you\n",
            "2 may\n",
            "3 view\n",
            "4 the\n",
            "5 newsletter\n",
            "6 by\n",
            "7 ae\n",
            "8 ##ssing\n",
            "9 our\n",
            "10 website\n",
            "11 at\n",
            "12 www\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNg4tbz8rhbL",
        "colab_type": "text"
      },
      "source": [
        "Grouping the values by layer makes sense for the model, but for our purposes we want it grouped by token.\n",
        "\n",
        "Current dimensions:\n",
        "\n",
        "[# layers, # batches, # tokens, # features]\n",
        "\n",
        "Desired dimensions:\n",
        "\n",
        "[# tokens, # layers, # features]\n",
        "\n",
        "Luckily, PyTorch includes the permute function for easily rearranging the dimensions of a tensor.\n",
        "\n",
        "However, the first dimension is currently a Python list!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYB-AkZlrgAd",
        "colab_type": "code",
        "outputId": "c89be9da-5c8a-485e-c1c3-24e8b1d22d78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# `hidden_states` is a Python list.\n",
        "print('      Type of hidden_states: ', type(hidden_states))\n",
        "\n",
        "# Each layer in the list is a torch tensor.\n",
        "print('Tensor shape for each layer: ', hidden_states[0].size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Type of hidden_states:  <class 'tuple'>\n",
            "Tensor shape for each layer:  torch.Size([1, 22, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-xgzS2LryXG",
        "colab_type": "text"
      },
      "source": [
        "Let's combine the layers to make this one whole big tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10EYBipsr0mT",
        "colab_type": "code",
        "outputId": "9f04b5f5-b0f5-4f06-a904-d6ec2da2c9e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Concatenate the tensors for all layers. We use `stack` here to\n",
        "# create a new dimension in the tensor.\n",
        "token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13, 1, 22, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJdLOE7wr4HN",
        "colab_type": "text"
      },
      "source": [
        "Let's get rid of the \"batches\" dimension since we don't need it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj0_LphRr6j9",
        "colab_type": "code",
        "outputId": "ebc990e6-2bab-44b7-d81d-f00b80902f38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Remove dimension 1, the \"batches\".\n",
        "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13, 22, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZFNQCQEr_q-",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can switch around the \"layers\" and \"tokens\" dimensions with permute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAiwKZI3sAqu",
        "colab_type": "code",
        "outputId": "eb765204-b99c-4285-ec09-ef00f29dc14d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Swap dimensions 0 and 1.\n",
        "token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([22, 13, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaRCIIqzsDIE",
        "colab_type": "text"
      },
      "source": [
        "##3.3. Creating word and sentence vectors from hidden states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55KOhfKIKG0r",
        "colab_type": "text"
      },
      "source": [
        "### Word Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9mspiRtsHQH",
        "colab_type": "text"
      },
      "source": [
        "Now, what do we do with these hidden states? We would like to get individual vectors for each of our tokens, or perhaps a single vector representation of the whole sentence, but for each token of our input we have 13 separate vectors each of length 768.\n",
        "\n",
        "In order to get the individual vectors we will need to combine some of the layer vectors...but which layer or combination of layers provides the best representation?\n",
        "\n",
        "Unfortunately, there's no single easy answer... Let's try a couple reasonable approaches, though. Afterwards, I'll point you to some helpful resources which look into this question further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIpgwR0KsQ1C",
        "colab_type": "text"
      },
      "source": [
        "Word Vectors\n",
        "To give you some examples, let's create word vectors two ways.\n",
        "\n",
        "First, let's **concatenate** the last four layers, giving us a single word vector per token. Each vector will have length 4 x 768 = 3,072."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAyPKoNgsQMC",
        "colab_type": "code",
        "outputId": "122c0938-1f4f-4ebd-db21-47e098fb64a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Stores the token vectors, with shape [22 x 3,072]\n",
        "token_vecs_cat = []\n",
        "\n",
        "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "    \n",
        "    # `token` is a [12 x 768] tensor\n",
        "\n",
        "    # Concatenate the vectors (that is, append them together) from the last \n",
        "    # four layers.\n",
        "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
        "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
        "    \n",
        "    # Use `cat_vec` to represent `token`.\n",
        "    token_vecs_cat.append(cat_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape is: 22 x 3072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMG9-694Jlyu",
        "colab_type": "text"
      },
      "source": [
        "As an alternative method, let's try creating the word vectors by **summing** together the last four layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx_BEglpJqxE",
        "colab_type": "code",
        "outputId": "dbe253d3-4e0d-470f-8efb-e96e7a353aba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Stores the token vectors, with shape [22 x 768]\n",
        "token_vecs_sum = []\n",
        "\n",
        "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "\n",
        "    # `token` is a [12 x 768] tensor\n",
        "\n",
        "    # Sum the vectors from the last four layers.\n",
        "    sum_vec = torch.sum(token[-4:], dim=0)\n",
        "    \n",
        "    # Use `sum_vec` to represent `token`.\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape is: 22 x 768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E4dtcQeJ-2z",
        "colab_type": "text"
      },
      "source": [
        "### Sentence Vectors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjF90A6xKLI0",
        "colab_type": "text"
      },
      "source": [
        "To get a single vector for our entire sentence we have multiple application-dependent strategies, but a simple approach is to average the second to last hiden layer of each token producing a single 768 length vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPqFFBr0KJyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# `hidden_states` has shape [13 x 1 x 22 x 768]\n",
        "\n",
        "# `token_vecs` is a tensor with shape [22 x 768]\n",
        "token_vecs = hidden_states[-2][0]\n",
        "\n",
        "# Calculate the average of all 22 token vectors.\n",
        "sentence_embedding = torch.mean(token_vecs, dim=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwZjtE3ZKPZB",
        "colab_type": "code",
        "outputId": "ad565b7f-ac94-487a-fb26-c23398ad3d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our final sentence embedding vector of shape: torch.Size([768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXTKJnb_KSgp",
        "colab_type": "text"
      },
      "source": [
        "##3.4. Confirming contextually dependent vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQN5w6zgKXiL",
        "colab_type": "text"
      },
      "source": [
        "To confirm that the value of these vectors are in fact contextually dependent, let's look at the different instances of the word \"bank\" in our example sentence:\n",
        "\n",
        "\"After stealing money from the **bank vault**, the **bank robber** was seen fishing on the **Mississippi river bank**.\"\n",
        "\n",
        "Let's find the index of those three instances of the word \"bank\" in the example sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL4OmJfSKh-w",
        "colab_type": "code",
        "outputId": "001a1e14-851b-472d-d487-66422867f628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "for i, token_str in enumerate(tokenized_text):\n",
        "  print (i, token_str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [CLS]\n",
            "1 after\n",
            "2 stealing\n",
            "3 money\n",
            "4 from\n",
            "5 the\n",
            "6 bank\n",
            "7 vault\n",
            "8 ,\n",
            "9 the\n",
            "10 bank\n",
            "11 robber\n",
            "12 was\n",
            "13 seen\n",
            "14 fishing\n",
            "15 on\n",
            "16 the\n",
            "17 mississippi\n",
            "18 river\n",
            "19 bank\n",
            "20 .\n",
            "21 [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ9I9Z8qKlbc",
        "colab_type": "text"
      },
      "source": [
        "They are at 6, 10, and 19.\n",
        "\n",
        "For this analysis, we'll use the word vectors that we created by summing the last four layers.\n",
        "\n",
        "We can try printing out their vectors to compare them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk0ZviYPKpb6",
        "colab_type": "code",
        "outputId": "c92f125a-fca6-4c06-cb65-5bf35790a8f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('First 5 vector values for each instance of \"bank\".')\n",
        "print('')\n",
        "print(\"bank vault   \", str(token_vecs_sum[6][:5]))\n",
        "print(\"bank robber  \", str(token_vecs_sum[10][:5]))\n",
        "print(\"river bank   \", str(token_vecs_sum[19][:5]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 5 vector values for each instance of \"bank\".\n",
            "\n",
            "bank vault    tensor([ 3.3596, -2.9805, -1.5421,  0.7065,  2.0031])\n",
            "bank robber   tensor([ 2.7359, -2.5577, -1.3094,  0.6797,  1.6633])\n",
            "river bank    tensor([ 1.5266, -0.8895, -0.5152, -0.9298,  2.8334])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw8qk5kvKoON",
        "colab_type": "text"
      },
      "source": [
        "We can see that the values differ, but let's calculate the cosine similarity between the vectors to make a more precise comparison.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BxO4s6RK0Ps",
        "colab_type": "code",
        "outputId": "81237668-13c7-4b08-ab84-fb14c71e6dcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Calculate the cosine similarity between the word bank \n",
        "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
        "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
        "\n",
        "# Calculate the cosine similarity between the word bank\n",
        "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
        "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
        "\n",
        "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
        "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vector similarity for  *similar*  meanings:  0.94\n",
            "Vector similarity for *different* meanings:  0.69\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}