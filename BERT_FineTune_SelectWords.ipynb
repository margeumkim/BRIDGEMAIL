{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_FineTune_SelectWords.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPRdUSU9tyF4A4dxRXfAFLZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ae13fbeca19046b7b9d48cc22c3a6873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0b03ad5a34df4c4bbe94cf203d2e4e5b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_807326e6f11f473581c49335aac7205f",
              "IPY_MODEL_50b2bfbfa9ec4c229cd279ed76238126"
            ]
          }
        },
        "0b03ad5a34df4c4bbe94cf203d2e4e5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "807326e6f11f473581c49335aac7205f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ffaf0af7bb6f498a92b1fbfcf825ef6e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d671aac291024366b124503ccfdb8b17"
          }
        },
        "50b2bfbfa9ec4c229cd279ed76238126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_39c5c48294ff4bb394f7083e864b1755",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "‚Äã",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 573kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fb2ff9ffca5847f8b8c285a38a2096fe"
          }
        },
        "ffaf0af7bb6f498a92b1fbfcf825ef6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d671aac291024366b124503ccfdb8b17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "39c5c48294ff4bb394f7083e864b1755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb2ff9ffca5847f8b8c285a38a2096fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "25ed4055940d4e6ba1818856297c7359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c42163d44b2e488ca077fd9e73a18898",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_145ad06f2acf43adbd4d5c6f31643a13",
              "IPY_MODEL_7abb0d17c758417b8869c5abd99f0a53"
            ]
          }
        },
        "c42163d44b2e488ca077fd9e73a18898": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "145ad06f2acf43adbd4d5c6f31643a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_af5184faa8914ba1b8999473abb30c49",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b0ef91ecf1ed4faab66fbd7917286531"
          }
        },
        "7abb0d17c758417b8869c5abd99f0a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_091463020a7c4129a38af0c994f50457",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "‚Äã",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 3.04kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_344a007514d34be6a91567e29aff1d51"
          }
        },
        "af5184faa8914ba1b8999473abb30c49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b0ef91ecf1ed4faab66fbd7917286531": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "091463020a7c4129a38af0c994f50457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "344a007514d34be6a91567e29aff1d51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a1cc13c3e9944f99e885f79ee341e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_02a60ab608394438ad636492bcf9ec2c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f1c20c7a214e46809d21dea0785c5ff6",
              "IPY_MODEL_d10a47bc0b5d473782f73d20b6fc111c"
            ]
          }
        },
        "02a60ab608394438ad636492bcf9ec2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1c20c7a214e46809d21dea0785c5ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_61bd7942ee5849b095590d4e094f0558",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93f92b7f4fa7450ebf541ad3d624d46c"
          }
        },
        "d10a47bc0b5d473782f73d20b6fc111c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2184913c65d34b71b88164023294f979",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "‚Äã",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:07&lt;00:00, 60.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99e8c78b7b0c4c1fbe5ebec06de86de0"
          }
        },
        "61bd7942ee5849b095590d4e094f0558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93f92b7f4fa7450ebf541ad3d624d46c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2184913c65d34b71b88164023294f979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99e8c78b7b0c4c1fbe5ebec06de86de0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/margeumkim/BRIDGEMAIL/blob/master/BERT_FineTune_SelectWords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM8wXsiM4jiH",
        "colab_type": "text"
      },
      "source": [
        "# BERT Fine-Tuning Tutorial with PyTorch\n",
        "Ref: Chris McCormick and Nick Ryan's notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r53A2j5p4rft",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Using Colab GPU for Training\n",
        "\n",
        "Google Colab offers free GPUs and TPUs! Since we'll be training a large neural network it's best to take advantage of this (in this case we'll attach a GPU), otherwise training will take a very long time.\n",
        "\n",
        "A GPU can be added by going to the menu and selecting:\n",
        "\n",
        "Edit ü°í Notebook Settings ü°í Hardware accelerator ü°í (GPU)\n",
        "\n",
        "Then run the following cell to confirm that the GPU is detected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHouifG_035d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37646b2c-983d-460e-f52f-f8bd3d1da381"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woonkBS05D-a",
        "colab_type": "text"
      },
      "source": [
        "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzsJxmUB5FT4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "38e7b7fc-1dae-47a7-ca0b-f7e76781928e"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAI2rmOB5KJ2",
        "colab_type": "text"
      },
      "source": [
        "## 1.2. Installing the Hugging Face Library\n",
        "\n",
        "Next, let's install the transformers package from Hugging Face which will give us a pytorch interface for working with BERT. (This library contains interfaces for other pretrained language models like OpenAI's GPT and GPT-2.) We've selected the pytorch interface because it strikes a nice balance between the high-level APIs (which are easy to use but don't provide insight into how things work) and tensorflow code (which contains lots of details but often sidetracks us into lessons about tensorflow, when the purpose here is BERT!).\n",
        "\n",
        "At the moment, the Hugging Face library seems to be the most widely accepted and powerful pytorch interface for working with BERT. In addition to supporting a variety of different pre-trained transformer models, the library also includes pre-built modifications of these models suited to your specific task. For example, in this tutorial we will use BertForSequenceClassification.\n",
        "\n",
        "The library also includes task-specific classes for token classification, question answering, next sentence prediciton, etc. Using these pre-built classes simplifies the process of modifying BERT for your purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD5w4uN75MW1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "ec028ca1-dfd3-4789-dd7f-3e8bdb1da53b"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\r\u001b[K     |‚ñå                               | 10kB 19.9MB/s eta 0:00:01\r\u001b[K     |‚ñà                               | 20kB 6.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñå                              | 30kB 7.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà                              | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñç                             | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà                             | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñç                            | 71kB 7.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñâ                            | 81kB 8.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñç                           | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 102kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 112kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 122kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 133kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 143kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 153kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 163kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 174kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 184kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 194kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 204kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 215kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 225kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 235kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 245kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 256kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 266kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 276kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 286kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 296kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 307kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 317kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 327kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 337kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 348kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 358kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 368kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 378kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 389kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 399kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 409kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 419kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 430kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 440kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 450kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 460kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 471kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 481kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 491kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 501kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 512kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 522kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 532kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 542kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 552kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 563kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 573kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 583kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 593kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 604kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 614kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 624kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 634kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 645kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 655kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 665kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 675kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1MB 31.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.8MB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 890kB 50.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=fcaf704fceba167069a89f6b69749cff7f4670f4adc5ee680326edb41100d0fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3WPNzYF531p",
        "colab_type": "text"
      },
      "source": [
        "The code in this notebook is actually a simplified version of the run_glue.py example script from huggingface.\n",
        "\n",
        "run_glue.py is a helpful utility which allows you to pick which GLUE benchmark task you want to run on, and which pre-trained model you want to use (you can see the list of possible models here). It also supports using either the CPU, a single GPU, or multiple GPUs. It even supports using 16-bit precision if you want further speed up.\n",
        "\n",
        "Unfortunately, all of this configurability comes at the cost of readability. In this Notebook, we've simplified the code greatly and added plenty of comments to make it clear what's going on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzkb8h4K6CYT",
        "colab_type": "text"
      },
      "source": [
        "# 2. Loading CoLA Dataset\n",
        "\n",
        "We'll use [The Corpus of Linguistic Acceptability (CoLA) ](https://nyu-mll.github.io/CoLA/) dataset for single sentence classification. It's a set of sentences labeled as grammatically correct or incorrect. It was first published in May of 2018, and is one of the tests included in the \"GLUE Benchmark\" on which models like BERT are competing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TliwVW-E6Fat",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. Download & Extract\n",
        "Import the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9smiD8qJ6Ht1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c909302f-1fc0-41bf-91ed-69054f62b9f1"
      },
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgLYtjlv98kY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0f59c166-6545-40f6-f995-8cb755f1a26c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Import the training data\n",
        "path = \"/content/drive/My Drive/data/train_set_6733.csv\"\n",
        "train_df = pd.read_csv(path)\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training messages: {:,}\\n'.format(train_df.shape[0]))\n",
        "\n",
        "\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training messages: 586\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6NAcHqCCp-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the dictionary data \n",
        "path_dict = \"/content/drive/My Drive/data/dict_topic_for_bert.csv\"\n",
        "dict_for_bert = pd.read_csv(path_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH5fajRd-Vz9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7a46e3c5-c09e-414a-b435-9bb5c62be186"
      },
      "source": [
        "train_df.iloc[10][\"content\"]\n",
        "# . or >  split --> each sentence == join with the primary category\n",
        "# . or >  split --> each sentence == join with the 3.1 \n",
        "# . or >  split --> each sentence == join with the 3.6\n",
        "# . or >  split --> each sentence == join with the 3.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Just a \"heads up\" ... Ken may get a call from Gov Gilmore regarding the Republican Governors\\' Association. Below are Sue Landwehr\\'s recommendations (with which I concur). Let me know if he calls and let us know if you need any additional information. ----- Forwarded by Steven J Kean/NA/Enron on 10/04/2000 09:20 AM ----- Richard Shapiro@ENRON 10/04/2000 07:17 AM To: Susan M Landwehr/HOU/EES@EES cc: Elizabeth Linnell/NA/Enron@Enron@EES, Steven J Kean/NA/Enron@Enron@EES Subject: Re: RGA request I agree w/ your recommendations. Susan M Landwehr@EES 10/03/2000 09:50 PM To: Richard Shapiro/NA/Enron@Enron cc: Elizabeth Linnell/NA/Enron@Enron, Steven J Kean/NA/Enron@Enron Subject: RGA request Rick--you may have seen a recent letter from Gov Jim Gilmore and the RGA requesting that we make an additional contribution in the next few weeks to the RGA for their efforts on the upcoming November elections. They list a fundraising goal of $1,660,000 (just a bit aggressive!) If we are not able to make an additional contribution, he asks that we renew our 2001 annual membership of $40,000 now instead of in January of 2001. If we do anything, my recommendation would be to cut a check now for our annual membership (the downside of doing that, of course, is that they will hit us a number of times in 2001 for incremental contributions to specific events/causes). I bring this to your attention for two reasons-one is that if I know Governor Gilmore, he will probably make a call directly to Ken Lay and you and Steve may want to have an answer available if Mr. Lay gets the call. Secondly, what are your thoughts on my response to RGA? My preference would be to not commit to any additional dollars, with a fall back that if Gilmore does call Lay and he wants to respond positively, we then commit to renewing our membership early. I think you are pretty busy over the next few days--please respond as your schedule allows. THanks.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7aX9gfN_2BL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "e4b8bcc6-5ffa-454a-9599-e0c7954f35a7"
      },
      "source": [
        "train_df['any_3_1'].describe()  #23%"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    586.000000\n",
              "mean       0.238908\n",
              "std        0.426781\n",
              "min        0.000000\n",
              "25%        0.000000\n",
              "50%        0.000000\n",
              "75%        0.000000\n",
              "max        1.000000\n",
              "Name: any_3_1, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9Jhj-Ph-8SB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "bb22d58a-5ecc-441f-a35d-b077f8f620e5"
      },
      "source": [
        "train_randselect = []\n",
        "train_t31 = []\n",
        "train_t36 = []\n",
        "train_t32 = []\n",
        "\n",
        "for index, row in train_df.iterrows():\n",
        "    if type(row['content']) ==  str:\n",
        "      for j in range(len(row['content'].split('.'))):\n",
        "        train_randselect.append([row['primary_cat'], row['content'].split('.')[j]])\n",
        "        train_t31.append([row['any_3_1'], row['content'].split('.')[j]])\n",
        "        train_t36.append([row['any_3_6'], row['content'].split('.')[j]])\n",
        "        train_t32.append([row['any_3_2'], row['content'].split('.')[j]])\n",
        "    else:\n",
        "      print (index)\n",
        "      pass\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "54\n",
            "83\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-43da4bd0b9c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m  \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrain_randselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'primary_cat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtrain_t31\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'any_3_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_t36\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'any_3_6'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4401\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4403\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4404\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4405\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_convert_scalar_indexer\u001b[0;34m(self, key, kind)\u001b[0m\n\u001b[1;32m   2858\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positional\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2860\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCMultiIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2862\u001b[0m             \u001b[0;31m# we can raise here if we are definitive that this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI-bRv4wAay_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_t36_df = pd.DataFrame(train_t36, columns = ['Label', 'Sentence'])   # 34% yes  /// 32866 sentences\n",
        "train_t31_df = pd.DataFrame(train_t31, columns = ['Label', 'Sentence'])   # 19% yes\n",
        "train_t32_df = pd.DataFrame(train_t32, columns = ['Label', 'Sentence'])   # 14% yes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-jdSbzTCQCv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "d7ee69a8-e156-4f2d-fd33-6f05a53d961b"
      },
      "source": [
        "term_list = dict_for_bert['Term']\n",
        "term_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0              lower\n",
              "1             member\n",
              "2              taken\n",
              "3        spokeswoman\n",
              "4             summer\n",
              "            ...     \n",
              "1494           light\n",
              "1495        research\n",
              "1496            love\n",
              "1497    circumstance\n",
              "1498           phone\n",
              "Name: Term, Length: 1499, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfkWqPvsDIrj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "be07a476-b05a-4dfe-f6a3-be3e391a1ec1"
      },
      "source": [
        "train_t36_bool_list = []\n",
        "\n",
        "for index, row in train_t36_df.iterrows():\n",
        "    if type(row['Sentence']) == str:\n",
        "        my_bool = any(item in list(row['Sentence'].split(' ')) for item in list(dict_for_bert['Term'])) \n",
        "        #print (my_bool)\n",
        "        train_t36_bool_list.append(my_bool)\n",
        "\n",
        "train_t36_df['term_bool'] = train_t36_bool_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c3005a83e7a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_t36_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mmy_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_for_bert\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Term'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m#print (my_bool)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_t36_bool_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_bool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-c3005a83e7a9>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_t36_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mmy_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_for_bert\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Term'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m#print (my_bool)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_t36_bool_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_bool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4403\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4404\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4405\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4406\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw_zVvetR6cP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2c27df1a-2a84-4a5b-d7c8-425030fcf350"
      },
      "source": [
        "train_t36_df['term_bool'].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     32866\n",
              "unique        2\n",
              "top        True\n",
              "freq      25048\n",
              "Name: term_bool, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MOAjd5RqUiI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "131482be-c2e3-4da1-eef1-843ad534d315"
      },
      "source": [
        "train_t36_df_red = train_t36_df[train_t36_df['term_bool'] == True]\n",
        "len(train_t36_df_red)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25048"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir-MAnLLrnCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_t36_df_red.to_csv('train_t36_ready_complete.csv', index=True)\n",
        "!cp train_t36_ready_complete.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdS8KSc9q5Rm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_t36_df_1000 = train_t36_df_red.sample(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBXkUr0erRGp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "295e3c39-99ec-4f53-8bdc-a7c37d53f0f2"
      },
      "source": [
        "train_t36_df_1000['Label'].describe()  # 33%"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1000.000000\n",
              "mean        0.333000\n",
              "std         0.471522\n",
              "min         0.000000\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         1.000000\n",
              "max         1.000000\n",
              "Name: Label, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_x9jY0AGU8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_t31_bool_list = []\n",
        "\n",
        "for index, row in train_t31_df.iterrows():\n",
        "    if type(row['Sentence']) == str:\n",
        "        my_bool = any(item in list(row['Sentence'].split(' ')) for item in list(dict_for_bert['Term'])) \n",
        "        #print (my_bool)\n",
        "        train_t31_bool_list.append(my_bool)\n",
        "\n",
        "train_t31_df['term_bool'] = train_t31_bool_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31qQch1iGl8g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "159284ba-4fd6-4cce-fa41-b7c81a9780b3"
      },
      "source": [
        "train_t31_df['term_bool'].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     32866\n",
              "unique        2\n",
              "top        True\n",
              "freq      25048\n",
              "Name: term_bool, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HAMUj8kGpG-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04c3901b-cb7c-44ec-a5c2-3d53566cc2aa"
      },
      "source": [
        "train_t31_df_red = train_t31_df[train_t31_df['term_bool'] == True]\n",
        "len(train_t31_df_red)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25048"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5-Zxl5WGuTf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "5deba001-f735-4146-bf6f-d44324766c30"
      },
      "source": [
        "train_t31_df_red.to_csv('train_t31_ready_complete.csv', index=True)\n",
        "!cp train_t31_ready_complete.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fbf291f2f059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_t31_df_red\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_t31_ready_complete.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cp train_t31_ready_complete.csv \"drive/My Drive/\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_t31_df_red' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHkcFr3fnBlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the training data\n",
        "import pandas as pd\n",
        "\n",
        "path_dict = \"/content/drive/My Drive/train_t31_ready_complete.csv\"\n",
        "train_t31_df_red = pd.read_csv(path_dict)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsRILvsjG1KF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_t31_df_3000 = train_t31_df_red.sample(3000)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB4c65ySG5Ip",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d1f15791-0f8c-4f97-d08b-45114248b623"
      },
      "source": [
        "train_t31_df_3000['Label'].describe()  # 18%"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    3000.000000\n",
              "mean        0.191667\n",
              "std         0.393678\n",
              "min         0.000000\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         0.000000\n",
              "max         1.000000\n",
              "Name: Label, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecetV1Qnupw1",
        "colab_type": "text"
      },
      "source": [
        "### Pick the subset that you want to train your model with!\n",
        "Let's extract the sentences and labels of our training set as numpy ndarrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXPNnmt9uo6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = train_t31_df_3000.Sentence.values\n",
        "labels = train_t31_df_3000.Label.values"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfKDxT0dwl1m",
        "colab_type": "text"
      },
      "source": [
        "# 3. Tokenization & Input Formatting\n",
        "\n",
        "In this section, we'll transform our dataset into the format that BERT can be trained on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS196pOXwrBW",
        "colab_type": "text"
      },
      "source": [
        "3.1. BERT Tokenizer\n",
        "To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n",
        "\n",
        "The tokenization must be performed by the tokenizer included with BERT--the below cell will download this for us. We'll be using the \"uncased\" version here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlzPw5_pwoYe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "ae13fbeca19046b7b9d48cc22c3a6873",
            "0b03ad5a34df4c4bbe94cf203d2e4e5b",
            "807326e6f11f473581c49335aac7205f",
            "50b2bfbfa9ec4c229cd279ed76238126",
            "ffaf0af7bb6f498a92b1fbfcf825ef6e",
            "d671aac291024366b124503ccfdb8b17",
            "39c5c48294ff4bb394f7083e864b1755",
            "fb2ff9ffca5847f8b8c285a38a2096fe"
          ]
        },
        "outputId": "5775dc1b-998a-4ef8-abab-d10e33fe5db3"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae13fbeca19046b7b9d48cc22c3a6873",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti‚Ä¶"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GStBu-N_xLXc",
        "colab_type": "text"
      },
      "source": [
        "Let's apply the tokenizer to one sentence just to see the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6SzNAl6xKbq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c65c6a93-7d25-4a7e-b9be-f0a84647923d"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:   Copyright 2001 Los Angeles Times=20 Wednesday, June 20, 2001=20 The FERC's Action Is Good, Bad,Ugly=20 By PETER NAVARRO ?????The Federal Energy Regulatory Commission's new wholesale price caps wi= ll=20 save the Western states literally tens of billions of dollars in electricit= y=20 bills\n",
            "Tokenized:  ['copyright', '2001', 'los', 'angeles', 'times', '=', '20', 'wednesday', ',', 'june', '20', ',', '2001', '=', '20', 'the', 'fe', '##rc', \"'\", 's', 'action', 'is', 'good', ',', 'bad', ',', 'ugly', '=', '20', 'by', 'peter', 'navarro', '?', '?', '?', '?', '?', 'the', 'federal', 'energy', 'regulatory', 'commission', \"'\", 's', 'new', 'wholesale', 'price', 'caps', 'wi', '=', 'll', '=', '20', 'save', 'the', 'western', 'states', 'literally', 'tens', 'of', 'billions', 'of', 'dollars', 'in', 'electric', '##it', '=', 'y', '=', '20', 'bills']\n",
            "Token IDs:  [9385, 2541, 3050, 3349, 2335, 1027, 2322, 9317, 1010, 2238, 2322, 1010, 2541, 1027, 2322, 1996, 10768, 11890, 1005, 1055, 2895, 2003, 2204, 1010, 2919, 1010, 9200, 1027, 2322, 2011, 2848, 23524, 1029, 1029, 1029, 1029, 1029, 1996, 2976, 2943, 10738, 3222, 1005, 1055, 2047, 17264, 3976, 9700, 15536, 1027, 2222, 1027, 2322, 3828, 1996, 2530, 2163, 6719, 15295, 1997, 25501, 1997, 6363, 1999, 3751, 4183, 1027, 1061, 1027, 2322, 8236]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpJ-2wKTxRMn",
        "colab_type": "text"
      },
      "source": [
        "When we actually convert all of our sentences, we'll use the tokenize.encode function to handle both steps, rather than calling tokenize and convert_tokens_to_ids separately.\n",
        "\n",
        "Before we can do that, though, we need to talk about some of BERT's formatting requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glULV4vtxUE2",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Tokenize Dataset\n",
        "\n",
        "he transformers library provides a helpful encode function which will handle most of the parsing and data prep steps for us.\n",
        "\n",
        "Before we are ready to encode our text, though, we need to decide on a maximum sentence length for padding / truncating to.\n",
        "\n",
        "The below cell will perform one tokenization pass of the dataset in order to measure the maximum sentence length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c29x1UHPxSqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "54073825-9e53-471a-c48e-da7199dd32b5"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1372 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  1374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdJVqatXxgSm",
        "colab_type": "text"
      },
      "source": [
        "Just in case there are some longer test sentences, I'll set the maximum length to 64.\n",
        "\n",
        "Now we're ready to perform the real tokenization.\n",
        "\n",
        "The tokenizer.encode_plus function combines multiple steps for us:\n",
        "\n",
        "Split the sentence into tokens.\n",
        "Add the special [CLS] and [SEP] tokens.\n",
        "Map the tokens to their IDs.\n",
        "Pad or truncate all sentences to the same length.\n",
        "Create the attention masks which explicitly differentiate real tokens from [PAD] tokens.\n",
        "The first four features are in tokenizer.encode, but I'm using tokenizer.encode_plus to get the fifth item (attention masks). Documentation is here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Quy3x0Rbxj-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "bcd29ee1-4a31-4ca1-a587-38322e04fd10"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 300,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:   Copyright 2001 Los Angeles Times=20 Wednesday, June 20, 2001=20 The FERC's Action Is Good, Bad,Ugly=20 By PETER NAVARRO ?????The Federal Energy Regulatory Commission's new wholesale price caps wi= ll=20 save the Western states literally tens of billions of dollars in electricit= y=20 bills\n",
            "Token IDs: tensor([  101,  9385,  2541,  3050,  3349,  2335,  1027,  2322,  9317,  1010,\n",
            "         2238,  2322,  1010,  2541,  1027,  2322,  1996, 10768, 11890,  1005,\n",
            "         1055,  2895,  2003,  2204,  1010,  2919,  1010,  9200,  1027,  2322,\n",
            "         2011,  2848, 23524,  1029,  1029,  1029,  1029,  1029,  1996,  2976,\n",
            "         2943, 10738,  3222,  1005,  1055,  2047, 17264,  3976,  9700, 15536,\n",
            "         1027,  2222,  1027,  2322,  3828,  1996,  2530,  2163,  6719, 15295,\n",
            "         1997, 25501,  1997,  6363,  1999,  3751,  4183,  1027,  1061,  1027,\n",
            "         2322,  8236,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no8JMkZmxsbO",
        "colab_type": "text"
      },
      "source": [
        "## 3.4. Training & Validation Split \n",
        "Divide up our training set to use 90% for training and 10% for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrpHRZXxx_qP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "40382313-0dc6-4037-db30-859676ade9fe"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2,700 training samples\n",
            "  300 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz5Gq3qjyQd6",
        "colab_type": "text"
      },
      "source": [
        "We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hX0cNECyOJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT_50hJy0Lsh",
        "colab_type": "text"
      },
      "source": [
        "# 4. Train Our Classification Model\n",
        "\n",
        "Now that our input data is properly formatted, it's time to fine tune the BERT model.\n",
        "\n",
        "## 4.1. BertForSequenceClassification\n",
        "\n",
        "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n",
        "\n",
        "Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
        "\n",
        "Here is the current list of classes provided for fine-tuning:\n",
        "\n",
        "BertModel\n",
        "BertForPreTraining\n",
        "BertForMaskedLM\n",
        "BertForNextSentencePrediction\n",
        "BertForSequenceClassification - The one we'll use.\n",
        "BertForTokenClassification\n",
        "BertForQuestionAnswering\n",
        "The documentation for these can be found under here.\n",
        "\n",
        "We'll be using BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
        "\n",
        "OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\").\n",
        "\n",
        "The documentation for from_pretrained can be found here, with the additional parameters defined here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdDG9FCy0HOY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "25ed4055940d4e6ba1818856297c7359",
            "c42163d44b2e488ca077fd9e73a18898",
            "145ad06f2acf43adbd4d5c6f31643a13",
            "7abb0d17c758417b8869c5abd99f0a53",
            "af5184faa8914ba1b8999473abb30c49",
            "b0ef91ecf1ed4faab66fbd7917286531",
            "091463020a7c4129a38af0c994f50457",
            "344a007514d34be6a91567e29aff1d51",
            "4a1cc13c3e9944f99e885f79ee341e7e",
            "02a60ab608394438ad636492bcf9ec2c",
            "f1c20c7a214e46809d21dea0785c5ff6",
            "d10a47bc0b5d473782f73d20b6fc111c",
            "61bd7942ee5849b095590d4e094f0558",
            "93f92b7f4fa7450ebf541ad3d624d46c",
            "2184913c65d34b71b88164023294f979",
            "99e8c78b7b0c4c1fbe5ebec06de86de0"
          ]
        },
        "outputId": "5339d568-bb3d-489d-acc1-7c0ee2a40bea"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = True, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25ed4055940d4e6ba1818856297c7359",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_‚Ä¶"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a1cc13c3e9944f99e885f79ee341e7e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri‚Ä¶"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1BfBmUj0o3D",
        "colab_type": "text"
      },
      "source": [
        "## 4.2. Optimizer & Learning Rate Scheduler\n",
        "Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n",
        "\n",
        "For the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the BERT paper):\n",
        "\n",
        "* Batch size: 16, 32\n",
        "* Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "* Number of epochs: 2, 3, 4\n",
        "\n",
        "\n",
        "We chose:\n",
        "\n",
        "* Batch size: 32 (set when creating our DataLoaders)\n",
        "* Learning rate: 2e-5\n",
        "* Epochs: 4 (we'll see that this is probably too many...)\n",
        "\n",
        "The epsilon parameter eps = 1e-8 is \"a very small number to prevent any division by zero in the implementation\" (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
        "\n",
        "You can find the creation of the AdamW optimizer in run_glue.py [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3ZjV21e0qmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niw2lEMN0wSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 4  # I am doing 4\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC-rTcO608hF",
        "colab_type": "text"
      },
      "source": [
        "## 4.3. Training Loop\n",
        "Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase.\n",
        "\n",
        "> Thank you to Stas Bekman for contributing the insights and code for using validation loss to detect over-fitting!\n",
        "\n",
        "Training:\n",
        "\n",
        "* Unpack our data inputs and labels\n",
        "* Load data onto the GPU for acceleration\n",
        "* Clear out the gradients calculated in the previous pass.\n",
        "** In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\n",
        "* Forward pass (feed input data through the network)\n",
        "* Backward pass (backpropagation)\n",
        "* Tell the network to update parameters with optimizer.step()\n",
        "* Track variables for monitoring progress\n",
        "\n",
        "Evalution:\n",
        "\n",
        "* Unpack our data inputs and labels\n",
        "* Load data onto the GPU for acceleration\n",
        "* Forward pass (feed input data through the network)\n",
        "* Compute loss on our validation data and track variables for monitoring progress\n",
        "\n",
        "Pytorch hides all of the detailed calculations from us, but we've commented the code to point out which of the above steps are happening on each line.\n",
        "\n",
        "> PyTorch also has some beginner tutorials which you may also find helpful.\n",
        "\n",
        "Define a helper function for calculating accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R-HK50W07Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txiOuYKO1FAA",
        "colab_type": "text"
      },
      "source": [
        "Helper function for formatting elapsed times as hh:mm:ss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kigK7lE1Cy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82v6Q2a31HmZ",
        "colab_type": "text"
      },
      "source": [
        "We're ready to kick off the training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUvFKyyD1Jv4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "b1e1d424-bb3b-421f-97c4-7e6767ad949e"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     85.    Elapsed: 0:01:10.\n",
            "  Batch    80  of     85.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.51\n",
            "  Training epcoh took: 0:02:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 0.44\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     85.    Elapsed: 0:01:08.\n",
            "  Batch    80  of     85.    Elapsed: 0:02:17.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epcoh took: 0:02:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 0.44\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     85.    Elapsed: 0:01:08.\n",
            "  Batch    80  of     85.    Elapsed: 0:02:17.\n",
            "\n",
            "  Average training loss: 0.48\n",
            "  Training epcoh took: 0:02:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 0.44\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     85.    Elapsed: 0:01:08.\n",
            "  Batch    80  of     85.    Elapsed: 0:02:16.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epcoh took: 0:02:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 0.44\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:10:02 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO6W46aMpggW",
        "colab_type": "text"
      },
      "source": [
        "Let's view the summary of the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvNzF8dHpfkM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8fdc8a81-c4a1-46ba-c4e8-c29d3cb7798e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0:02:26</td>\n",
              "      <td>0:00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.49</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0:02:24</td>\n",
              "      <td>0:00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.48</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0:02:24</td>\n",
              "      <td>0:00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.46</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0:02:24</td>\n",
              "      <td>0:00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.51         0.44           0.84       0:02:26         0:00:06\n",
              "2               0.49         0.44           0.84       0:02:24         0:00:06\n",
              "3               0.48         0.44           0.84       0:02:24         0:00:06\n",
              "4               0.46         0.44           0.84       0:02:24         0:00:06"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbCDwRoyx1ML",
        "colab_type": "text"
      },
      "source": [
        "Notice that, while the the training loss is going down with each epoch, the validation loss is not changing much.\n",
        "\n",
        "Validation Loss is a more precise measure than accuracy, because with accuracy we don't care about the exact output value, but just which side of a threshold it falls on.\n",
        "\n",
        "If we are predicting the correct answer, but with less confidence, then validation loss will catch this, while accuracy will not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21YIwkF5qlgO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "b7701450-775b-4568-9fe5-384471384a06"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mSJSelsqtOx",
        "colab_type": "text"
      },
      "source": [
        "# 5. Performance On Test Set\n",
        "Now we'll load the holdout dataset and prepare inputs just as we did with the training set. \n",
        "\n",
        "## 5.1. Data Preparation\n",
        "We'll need to apply all of the same steps that we did for the training data to prepare our test data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGi2FEoMquVK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5beaa992-7abe-4fd3-e78d-9b2e15c9c213"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Import the test data\n",
        "path_test = \"/content/drive/My Drive/data/test_set_6733.csv\"\n",
        "test_df = pd.read_csv(path_test)\n",
        "\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test messages: {:,}\\n'.format(test_df.shape[0]))\n",
        "\n",
        "\n",
        "# Import the dictionary data \n",
        "#path_dict = \"/content/drive/My Drive/data/dict_topic_for_bert.csv\"\n",
        "#dict_for_bert = pd.read_csv(path_dict)\n",
        "\n",
        "\n",
        "# Describe the label info\n",
        "test_df['any_3_1'].describe()  #23%\n",
        "\n",
        "\n",
        "# Split messages into sentences and tag the labels\n",
        "test_t31 = []\n",
        "\n",
        "for index, row in test_df.iterrows():\n",
        "    if type(row['content']) ==  str:\n",
        "      for j in range(len(row['content'].split('.'))):\n",
        "        test_t31.append([row['any_3_1'], row['content'].split('.')[j]])\n",
        "    else:\n",
        "      print (index)\n",
        "      pass\n",
        "\n",
        "test_df = pd.DataFrame(test_t31, columns = ['Label', 'Sentence'])   \n",
        "\n",
        "\n",
        "# Go through the sentences, and check whether that sentence has any of the key words; keep only those sentences with keywords\n",
        "term_list = dict_for_bert['Term']\n",
        "\n",
        "test_t31_bool_list = []\n",
        "\n",
        "for index, row in test_df.iterrows():\n",
        "    if type(row['Sentence']) == str:\n",
        "        my_bool = any(item in list(row['Sentence'].split(' ')) for item in list(dict_for_bert['Term'])) \n",
        "        #print (my_bool)\n",
        "        test_t31_bool_list.append(my_bool)\n",
        "\n",
        "test_df['term_bool'] = test_t31_bool_list\n",
        "test_df['term_bool'].describe()\n",
        "\n",
        "test_df_red = test_df[test_df['term_bool'] == True]\n",
        "print(len(test_df_red))\n",
        "\n",
        "\n",
        "# Save the file for later use\n",
        "test_df_red.to_csv('test_t31_ready_complete.csv', index=True)\n",
        "!cp test_t31_ready_complete.csv \"drive/My Drive/\"\n",
        "\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test messages: 289\n",
            "\n",
            "115\n",
            "16398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azqvZWXb2b4t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6d50080c-28a0-4cf7-b922-3f9885eb8e7e"
      },
      "source": [
        "test_t31_df_3000.head()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>term_bool</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7037</th>\n",
              "      <td>0</td>\n",
              "      <td>The longer the delay in selling the bonds, t=...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19677</th>\n",
              "      <td>0</td>\n",
              "      <td>??\"It was usually Edison that was more combat...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3041</th>\n",
              "      <td>0</td>\n",
              "      <td>A couple walking a dog made the grisly discov...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12474</th>\n",
              "      <td>0</td>\n",
              "      <td>Since the broilers remained hot, th= e=20 coo...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7864</th>\n",
              "      <td>1</td>\n",
              "      <td>=20 \"It's too great of a risk,\" she said</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Label                                           Sentence  term_bool\n",
              "7037       0   The longer the delay in selling the bonds, t=...       True\n",
              "19677      0   ??\"It was usually Edison that was more combat...       True\n",
              "3041       0   A couple walking a dog made the grisly discov...       True\n",
              "12474      0   Since the broilers remained hot, th= e=20 coo...       True\n",
              "7864       1           =20 \"It's too great of a risk,\" she said       True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmd-pn1YypKe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "07e039a8-0798-40fb-8c31-89a17f33250c"
      },
      "source": [
        "# Randomly sample 3000 sentences \n",
        "test_t31_df_3000 = test_df_red.sample(3000)\n",
        "print(test_t31_df_3000['Label'].describe())  # 18%\n",
        "\n",
        "\n",
        "# Get the lists of sentences and their labels.\n",
        "sentences = test_t31_df_3000.Sentence.values\n",
        "labels = test_t31_df_3000.Label.values\n",
        "  \n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 300,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count    3000.00\n",
            "mean        0.17\n",
            "std         0.38\n",
            "min         0.00\n",
            "25%         0.00\n",
            "50%         0.00\n",
            "75%         0.00\n",
            "max         1.00\n",
            "Name: Label, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF8pVjhSvRqf",
        "colab_type": "text"
      },
      "source": [
        "## 5.2. Evaluate on Test Set\n",
        "\n",
        "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHoG9qouvVM5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d464e479-a0f1-452d-9b7a-a3e0df3bca97"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 3,000 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6kbHz41w13c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53695970-4e79-4357-d6d4-0d3c28c0c447"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (test_t31_df_3000.Label.sum(), len(test_t31_df_3000.Label), (test_t31_df_3000.Label.sum() / len(test_t31_df_3000.Label) * 100.0)))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 518 of 3000 (17.27%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHGq5zZjyhIA",
        "colab_type": "text"
      },
      "source": [
        "Accuracy on the CoLA benchmark is measured using the \"[Matthews correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)\" (MCC).\n",
        "\n",
        "We use MCC here because the classes are imbalanced:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vw_rPjLxFfx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "de33e392-47d4-474e-f664-7870f481de67"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpfOKu9_yqwZ",
        "colab_type": "text"
      },
      "source": [
        "The final score will be based on the entire test set, but let's take a look at the scores on the individual batches to get a sense of the variability in the metric between batches.\n",
        "\n",
        "Each batch has 32 sentences in it, except the last batch which has only (516 % 32) = 4 test sentences in it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeALqwDOyyRY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "a21d7072-28f0-48fd-dd88-106a17488384"
      },
      "source": [
        "# Create a barplot showing the MCC score for each batch of test samples.\n",
        "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
        "\n",
        "plt.title('MCC Score per Batch')\n",
        "plt.ylabel('MCC Score (-1 to +1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA9dxS0hzClT",
        "colab_type": "text"
      },
      "source": [
        "Now we'll combine the results for all of the batches and calculate our final MCC score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0O0jMfP8J7-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0b01c435-7767-4b59-8573-4927e88ba0f1"
      },
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total MCC: %.3f' % mcc)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total MCC: 0.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwO9rs_E8HGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "52d3e47b-38e8-4ec2-dcff-b59a2871db00"
      },
      "source": [
        "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "print('Classification accuracy using BERT Fine Tuning: {0:0.2%}'.format(matthews_corrcoef(flat_true_labels, flat_predictions)))\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification accuracy using BERT Fine Tuning: 0.00%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYp0wWAT8IkB",
        "colab_type": "text"
      },
      "source": [
        "## A1. Saving & Loading Fine-Tuned Model\n",
        "This first cell (taken from ```run_glue.py``` [here](https://github.com/huggingface/transformers/blob/35ff345fc9df9e777b27903f11fa213e4052595b/examples/run_glue.py#L495)) writes the model and tokenizer out to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amRhuTAC4_h4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4bcb496a-5069-4509-efab-93ff59c412bb"
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "#torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/model_save/vocab.txt',\n",
              " '/content/drive/My Drive/model_save/special_tokens_map.json',\n",
              " '/content/drive/My Drive/model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpQD0PSr5mMw",
        "colab_type": "text"
      },
      "source": [
        "Let's check out the file sizes, out of curiosity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGETuYv_5phf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d75bb4a3-ccf5-4060-a453-62ee07c85f55"
      },
      "source": [
        "!ls -l --block-size=K '/content/drive/My Drive/model_save/'"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 427947K\n",
            "-rw------- 1 root root      1K Jun 22 02:44 config.json\n",
            "-rw------- 1 root root 427719K Jun 22 02:44 pytorch_model.bin\n",
            "-rw------- 1 root root      1K Jun 22 02:44 special_tokens_map.json\n",
            "-rw------- 1 root root      1K Jun 22 02:44 tokenizer_config.json\n",
            "-rw------- 1 root root    227K Jun 22 02:44 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-F7iz2r56KH",
        "colab_type": "text"
      },
      "source": [
        "The largest file is the model weights, at around 428 megabytes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKOr7LUE6FtP",
        "colab_type": "text"
      },
      "source": [
        "# 6. Load the fine-tuned model to drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvNNSG2d5-51",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aed3f637-2c0a-4f7a-a9c5-869ed6541050"
      },
      "source": [
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "print (output_dir)\n",
        "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiZzU7ih8JiZ",
        "colab_type": "text"
      },
      "source": [
        "## Import a user's message"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfKAJwxn8w-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Import the training data\n",
        "path_to_emails_dt = \"/content/drive/My Drive/data/emails.csv\"\n",
        "all_emails_df = pd.read_csv(path_to_emails_dt)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHpaOJ3U9Jj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Helper functions\n",
        "def get_text_from_email(msg):\n",
        "    '''To get the content from email objects'''\n",
        "    parts = []\n",
        "    for part in msg.walk():\n",
        "        if part.get_content_type() == 'text/plain':\n",
        "            parts.append( part.get_payload() )\n",
        "    return ''.join(parts)\n",
        "\n",
        "def split_email_addresses(line):\n",
        "    '''To separate multiple email addresses'''\n",
        "    if line:\n",
        "        addrs = line.split(',')\n",
        "        addrs = frozenset(map(lambda x: x.strip(), addrs))\n",
        "    else:\n",
        "        addrs = None\n",
        "    return addrs"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXcVch149_Oe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "61558da5-f28a-4316-fa84-8c7b4bf58f35"
      },
      "source": [
        "import email \n",
        "\n",
        "# Parse the emails into a list email objects\n",
        "messages = list(map(email.message_from_string, all_emails_df['message']))\n",
        "all_emails_df.drop('message', axis=1, inplace=True)\n",
        "# Get fields from parsed email objects\n",
        "keys = messages[0].keys()\n",
        "for key in keys:\n",
        "    all_emails_df[key] = [doc[key] for doc in messages]\n",
        "# Parse content from emails\n",
        "all_emails_df['content'] = list(map(get_text_from_email, messages))\n",
        "# Split multiple email addresses\n",
        "all_emails_df['From'] = all_emails_df['From'].map(split_email_addresses)\n",
        "all_emails_df['To'] = all_emails_df['To'].map(split_email_addresses)\n",
        "\n",
        "# Extract the root of 'file' as 'user'\n",
        "all_emails_df['user'] = all_emails_df['file'].map(lambda x:x.split('/')[0])\n",
        "del messages\n",
        "\n",
        "#all_emails_df.head()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>Message-ID</th>\n",
              "      <th>Date</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>Subject</th>\n",
              "      <th>Mime-Version</th>\n",
              "      <th>Content-Type</th>\n",
              "      <th>Content-Transfer-Encoding</th>\n",
              "      <th>X-From</th>\n",
              "      <th>X-To</th>\n",
              "      <th>X-cc</th>\n",
              "      <th>X-bcc</th>\n",
              "      <th>X-Folder</th>\n",
              "      <th>X-Origin</th>\n",
              "      <th>X-FileName</th>\n",
              "      <th>content</th>\n",
              "      <th>user</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>allen-p/_sent_mail/1.</td>\n",
              "      <td>&lt;18782981.1075855378110.JavaMail.evans@thyme&gt;</td>\n",
              "      <td>Mon, 14 May 2001 16:39:00 -0700 (PDT)</td>\n",
              "      <td>(phillip.allen@enron.com)</td>\n",
              "      <td>(tim.belden@enron.com)</td>\n",
              "      <td></td>\n",
              "      <td>1.0</td>\n",
              "      <td>text/plain; charset=us-ascii</td>\n",
              "      <td>7bit</td>\n",
              "      <td>Phillip K Allen</td>\n",
              "      <td>Tim Belden &lt;Tim Belden/Enron@EnronXGate&gt;</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>\\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...</td>\n",
              "      <td>Allen-P</td>\n",
              "      <td>pallen (Non-Privileged).pst</td>\n",
              "      <td>Here is our forecast\\n\\n</td>\n",
              "      <td>allen-p</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>allen-p/_sent_mail/10.</td>\n",
              "      <td>&lt;15464986.1075855378456.JavaMail.evans@thyme&gt;</td>\n",
              "      <td>Fri, 4 May 2001 13:51:00 -0700 (PDT)</td>\n",
              "      <td>(phillip.allen@enron.com)</td>\n",
              "      <td>(john.lavorato@enron.com)</td>\n",
              "      <td>Re:</td>\n",
              "      <td>1.0</td>\n",
              "      <td>text/plain; charset=us-ascii</td>\n",
              "      <td>7bit</td>\n",
              "      <td>Phillip K Allen</td>\n",
              "      <td>John J Lavorato &lt;John J Lavorato/ENRON@enronXg...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>\\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...</td>\n",
              "      <td>Allen-P</td>\n",
              "      <td>pallen (Non-Privileged).pst</td>\n",
              "      <td>Traveling to have a business meeting takes the...</td>\n",
              "      <td>allen-p</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>allen-p/_sent_mail/100.</td>\n",
              "      <td>&lt;24216240.1075855687451.JavaMail.evans@thyme&gt;</td>\n",
              "      <td>Wed, 18 Oct 2000 03:00:00 -0700 (PDT)</td>\n",
              "      <td>(phillip.allen@enron.com)</td>\n",
              "      <td>(leah.arsdall@enron.com)</td>\n",
              "      <td>Re: test</td>\n",
              "      <td>1.0</td>\n",
              "      <td>text/plain; charset=us-ascii</td>\n",
              "      <td>7bit</td>\n",
              "      <td>Phillip K Allen</td>\n",
              "      <td>Leah Van Arsdall</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>\\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail</td>\n",
              "      <td>Allen-P</td>\n",
              "      <td>pallen.nsf</td>\n",
              "      <td>test successful.  way to go!!!</td>\n",
              "      <td>allen-p</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>allen-p/_sent_mail/1000.</td>\n",
              "      <td>&lt;13505866.1075863688222.JavaMail.evans@thyme&gt;</td>\n",
              "      <td>Mon, 23 Oct 2000 06:13:00 -0700 (PDT)</td>\n",
              "      <td>(phillip.allen@enron.com)</td>\n",
              "      <td>(randall.gay@enron.com)</td>\n",
              "      <td></td>\n",
              "      <td>1.0</td>\n",
              "      <td>text/plain; charset=us-ascii</td>\n",
              "      <td>7bit</td>\n",
              "      <td>Phillip K Allen</td>\n",
              "      <td>Randall L Gay</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>\\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail</td>\n",
              "      <td>Allen-P</td>\n",
              "      <td>pallen.nsf</td>\n",
              "      <td>Randy,\\n\\n Can you send me a schedule of the s...</td>\n",
              "      <td>allen-p</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>allen-p/_sent_mail/1001.</td>\n",
              "      <td>&lt;30922949.1075863688243.JavaMail.evans@thyme&gt;</td>\n",
              "      <td>Thu, 31 Aug 2000 05:07:00 -0700 (PDT)</td>\n",
              "      <td>(phillip.allen@enron.com)</td>\n",
              "      <td>(greg.piper@enron.com)</td>\n",
              "      <td>Re: Hello</td>\n",
              "      <td>1.0</td>\n",
              "      <td>text/plain; charset=us-ascii</td>\n",
              "      <td>7bit</td>\n",
              "      <td>Phillip K Allen</td>\n",
              "      <td>Greg Piper</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>\\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail</td>\n",
              "      <td>Allen-P</td>\n",
              "      <td>pallen.nsf</td>\n",
              "      <td>Let's shoot for Tuesday at 11:45.</td>\n",
              "      <td>allen-p</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       file  ...     user\n",
              "0     allen-p/_sent_mail/1.  ...  allen-p\n",
              "1    allen-p/_sent_mail/10.  ...  allen-p\n",
              "2   allen-p/_sent_mail/100.  ...  allen-p\n",
              "3  allen-p/_sent_mail/1000.  ...  allen-p\n",
              "4  allen-p/_sent_mail/1001.  ...  allen-p\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_udvCwI_RGV",
        "colab_type": "text"
      },
      "source": [
        "#### Select kate-s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed3-z_FC_X_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "user_email_df = all_emails_df[all_emails_df['user']== 'symes-k']"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCGJTe0QBkvf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "36f5d99d-3fce-44ad-83b6-bcb25e188860"
      },
      "source": [
        "print('Number of messages in this account: {:,}\\n'.format(len(user_email_df)))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of messages in this account: 10,827\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcDNCSQTCHDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# message --> sentesces; check keywords in sentences \n",
        "\n",
        "sentences_list = []\n",
        "\n",
        "for index, row in user_email_df.iterrows():\n",
        "    if type(row['content']) ==  str:\n",
        "      sentence_list = row['content'].split('.')\n",
        "      for j in range(len(sentence_list)):\n",
        "        if type(sentence_list[j]) == str:\n",
        "          if any(item in list(sentence_list[j].split(' ')) for item in list(dict_for_bert['Term'])):\n",
        "            sentences_list.append(sentence_list[j])\n",
        "    else:\n",
        "      print (index)\n",
        "      pass\n"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVamBIlGFy-g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a191347c-4b43-4791-c12e-a612a1b66660"
      },
      "source": [
        "print('Number of eligible sentences in this account: {:,}\\n'.format(len(sentences_list)))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of eligible sentences in this account: 87,042\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6Tq8WdHGEfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "# Import the dictionary data \n",
        "#path_dict = \"/content/drive/My Drive/data/dict_topic_for_bert.csv\"\n",
        "#dict_for_bert = pd.read_csv(path_dict)\n",
        "\n",
        "# Randomly sample 10000 sentences \n",
        "from random import sample \n",
        "sentences = sample(sentences_list, 10000)\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZhrXXMcJvQA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "eb23af5e-8a68-4ac3-97b9-f7cfc25200f0"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 300,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZWaslOZKf1j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf4a6c87-65c2-48d9-87ef-7bc122f0f31a"
      },
      "source": [
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G32SwDpEKqhx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58f92d88-08a3-4282-8e91-58b5cf53467c"
      },
      "source": [
        "# Run the text through BERT, and collect all of the hidden states produced\n",
        "# from all 12 layers. \n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, grab hidden states\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "      hidden_states = outputs[0]\n",
        "\n",
        "print('    DONE.')\n"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nt3tkQDNVuY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "fced2abf-f27a-4b96-eaaa-8138391e04aa"
      },
      "source": [
        "hidden_states"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6897, -0.5112],\n",
              "        [ 0.4758, -0.4587],\n",
              "        [ 0.8842, -0.7432],\n",
              "        [ 0.4402, -0.4614],\n",
              "        [ 0.6914, -0.5895],\n",
              "        [ 0.4027, -0.6871],\n",
              "        [ 0.7188, -0.4990],\n",
              "        [ 0.6533, -0.7773],\n",
              "        [ 0.8631, -0.7833],\n",
              "        [ 0.6461, -0.5056],\n",
              "        [ 0.4788, -0.6270],\n",
              "        [ 0.7530, -0.8769],\n",
              "        [ 0.7376, -0.4931],\n",
              "        [ 0.8011, -0.7978],\n",
              "        [ 0.7727, -0.6083],\n",
              "        [ 0.8982, -0.6668]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    }
  ]
}